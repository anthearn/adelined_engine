#!/usr/bin/env python3
"""
Adelined data CLI.

Usage example:
    python3 adl fetch youtube --channels @lexfridman,@hubermanlab --output cache/youtube.json
"""

import argparse
import json
import csv
import os
import re
import sqlite3
import sys
import threading
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import requests
from bs4 import BeautifulSoup
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

# OpenAI confirmation (one per command run)
OPENAI_CONFIRMED = False

LOCAL_OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "phi3.5")
LOCAL_OLLAMA_EMBED_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "bge-large:335m-en-v1.5-fp16")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
LLM_PROVIDER_META_ENRICH = "LLM_PROVIDER_META_ENRICH"
LLM_PROVIDER_EMBED = "LLM_PROVIDER_EMBED"
PODSCAN_API_BASE = "https://podscan.fm/api/v1"


def resolve_llm_provider(env_var: str, default: str) -> str:
    raw = os.getenv(env_var)
    if not raw:
        return default
    val = raw.strip().lower()
    if val in ("local", "openai"):
        return val
    print(
        f"âš ï¸ {env_var} must be 'local' or 'openai'; got '{raw}'.",
        file=sys.stderr,
    )
    raise ValueError(f"Invalid {env_var} value: {raw}")

APPLE_TOPICS = [
    {"apple_id": "1301", "parent": None, "name": "Arts"},
    {"apple_id": "1302", "parent": None, "name": "Business"},
    {"apple_id": "1303", "parent": None, "name": "Comedy"},
    {"apple_id": "1304", "parent": None, "name": "Education"},
    {"apple_id": "1483", "parent": None, "name": "Fiction"},
    {"apple_id": "1511", "parent": None, "name": "Government"},
    {"apple_id": "1487", "parent": None, "name": "History"},
    {"apple_id": "1512", "parent": None, "name": "Health & Fitness"},
    {"apple_id": "1305", "parent": None, "name": "Kids & Family"},
    {"apple_id": "1502", "parent": None, "name": "Leisure"},
    {"apple_id": "1310", "parent": None, "name": "Music"},
    {"apple_id": "1489", "parent": None, "name": "News"},
    {"apple_id": "1314", "parent": None, "name": "Religion & Spirituality"},
    {"apple_id": "1315", "parent": None, "name": "Science"},
    {"apple_id": "1324", "parent": None, "name": "Society & Culture"},
    {"apple_id": "1545", "parent": None, "name": "Sports"},
    {"apple_id": "1318", "parent": None, "name": "Technology"},
    {"apple_id": "1488", "parent": None, "name": "True Crime"},
    {"apple_id": "1309", "parent": None, "name": "TV & Film"},
    # Subcategories (partial but high-value)
    {"apple_id": "1401", "parent": "1301", "name": "Design"},
    {"apple_id": "1402", "parent": "1301", "name": "Fashion & Beauty"},
    {"apple_id": "1404", "parent": "1301", "name": "Food"},
    {"apple_id": "1405", "parent": "1301", "name": "Books"},
    {"apple_id": "1412", "parent": "1301", "name": "Performing Arts"},
    {"apple_id": "1410", "parent": "1301", "name": "Visual Arts"},
    {"apple_id": "1413", "parent": "1302", "name": "Careers"},
    {"apple_id": "1414", "parent": "1302", "name": "Entrepreneurship"},
    {"apple_id": "1415", "parent": "1302", "name": "Investing"},
    {"apple_id": "1416", "parent": "1302", "name": "Management"},
    {"apple_id": "1471", "parent": "1302", "name": "Marketing"},
    {"apple_id": "1472", "parent": "1302", "name": "Non-Profit"},
    {"apple_id": "1476", "parent": "1304", "name": "Courses"},
    {"apple_id": "1480", "parent": "1304", "name": "Self-Improvement"},
    {"apple_id": "1468", "parent": "1304", "name": "Language Learning"},
    {"apple_id": "1469", "parent": "1304", "name": "How To"},
    {"apple_id": "1481", "parent": "1304", "name": "K-12"},
    {"apple_id": "1482", "parent": "1304", "name": "Higher Education"},
    {"apple_id": "1436", "parent": "1512", "name": "Mental Health"},
    {"apple_id": "1438", "parent": "1512", "name": "Nutrition"},
    {"apple_id": "1441", "parent": "1512", "name": "Fitness"},
    {"apple_id": "1440", "parent": "1512", "name": "Sexuality"},
    {"apple_id": "1444", "parent": "1512", "name": "Alternative Health"},
    {"apple_id": "1459", "parent": "1502", "name": "Animation & Manga"},
    {"apple_id": "1456", "parent": "1502", "name": "Hobbies"},
    {"apple_id": "1463", "parent": "1502", "name": "Home & Garden"},
    {"apple_id": "1464", "parent": "1502", "name": "Video Games"},
    {"apple_id": "1460", "parent": "1502", "name": "Automotive"},
    {"apple_id": "1458", "parent": "1502", "name": "Crafts"},
    {"apple_id": "1465", "parent": "1314", "name": "Buddhism"},
    {"apple_id": "1466", "parent": "1314", "name": "Christianity"},
    {"apple_id": "1467", "parent": "1314", "name": "Islam"},
    {"apple_id": "1470", "parent": "1314", "name": "Judaism"},
    {"apple_id": "1473", "parent": "1314", "name": "Spirituality"},
    {"apple_id": "1474", "parent": "1314", "name": "Hinduism"},
    {"apple_id": "1475", "parent": "1314", "name": "Religion"},
    {"apple_id": "1533", "parent": "1314", "name": "Other"},
    {"apple_id": "1477", "parent": "1315", "name": "Natural Sciences"},
    {"apple_id": "1478", "parent": "1315", "name": "Social Sciences"},
    {"apple_id": "1479", "parent": "1315", "name": "Mathematics"},
    {"apple_id": "1484", "parent": "1324", "name": "Documentary"},
    {"apple_id": "1485", "parent": "1324", "name": "Personal Journals"},
    {"apple_id": "1486", "parent": "1324", "name": "Philosophy"},
    {"apple_id": "1325", "parent": "1324", "name": "Places & Travel"},
    {"apple_id": "1547", "parent": "1545", "name": "Football"},
    {"apple_id": "1548", "parent": "1545", "name": "Basketball"},
    {"apple_id": "1549", "parent": "1545", "name": "Baseball"},
    {"apple_id": "1550", "parent": "1545", "name": "Hockey"},
    {"apple_id": "1551", "parent": "1545", "name": "Soccer"},
    {"apple_id": "1552", "parent": "1545", "name": "Outdoor"},
    {"apple_id": "1553", "parent": "1545", "name": "Rugby"},
    {"apple_id": "1554", "parent": "1545", "name": "Cricket"},
    {"apple_id": "1555", "parent": "1545", "name": "Swimming"},
    {"apple_id": "1556", "parent": "1545", "name": "Tennis"},
    {"apple_id": "1557", "parent": "1545", "name": "Wrestling"},
    {"apple_id": "1558", "parent": "1545", "name": "Sports News"},
    {"apple_id": "1559", "parent": "1545", "name": "Fantasy Sports"},
    {"apple_id": "1560", "parent": "1545", "name": "Running"},
    {"apple_id": "1561", "parent": "1545", "name": "Golf"},
    {"apple_id": "1562", "parent": "1545", "name": "Volleyball"},
    {"apple_id": "1563", "parent": "1545", "name": "Extreme Sports"},
    {"apple_id": "1564", "parent": "1545", "name": "Swimming & Diving"},
    {"apple_id": "1565", "parent": "1545", "name": "Racing"},
    {"apple_id": "1566", "parent": "1545", "name": "Chess"},
]
PODCASTINDEX_COLUMNS: Sequence[str] = [
    "id",
    "url",
    "title",
    "lastUpdate",
    "link",
    "lastHttpStatus",
    "dead",
    "contentType",
    "itunesId",
    "originalUrl",
    "itunesAuthor",
    "itunesOwnerName",
    "explicit",
    "imageUrl",
    "itunesType",
    "generator",
    "newestItemPubdate",
    "language",
    "oldestItemPubdate",
    "episodeCount",
    "popularityScore",
    "priority",
    "createdOn",
    "updateFrequency",
    "chash",
    "host",
    "newestEnclosureUrl",
    "podcastGuid",
    "description",
    "category1",
    "category2",
    "category3",
    "category4",
    "category5",
    "category6",
    "category7",
    "category8",
    "category9",
    "category10",
    "newestEnclosureDuration",
]


@dataclass
class YouTubeChannelRecord:
    channel_url: str
    title: str
    description: str
    about: str
    subscribers: Optional[str]
    scraped_at: str


# --- LLM / Embedding abstraction wrappers ---
class LLMClient:
    def __init__(self, provider: str = "openai", model: str = "gpt-4o-mini"):
        self.provider = provider
        self.model = model
        self.api_key = os.getenv("OPENAI_API_KEY")

    def normalize_entity(self, entity_type: str, name: str, fields_text: str) -> Optional[Dict]:
        if self.provider != "openai" or not self.api_key:
            return None
        client = OpenAI(api_key=self.api_key)
        schema = (
            "{"
            "\"summary\":\"...\","
            "\"vertical\":\"...\","
            "\"audience\":\"...\","
            "\"pain_points\":\"...\","
            "\"use_cases\":\"...\","
            "\"themes\":\"...\","
            "\"tone\":\"...\","
            "\"geo\":\"...\","
            "\"topics\":[{\"apple_category_name\":\"...\",\"apple_subcategory_name\":\"...\",\"apple_category_id\":\"...\",\"apple_subcategory_id\":\"...\"}]"
            "}"
        )
        prompt = (
            "You are a classification and summarisation assistant.\n"
            "Normalize noisy marketing/technical text about an entity into a simple schema, using plain language a non-expert would understand.\n"
            "Return JSON ONLY with keys: summary, vertical, audience, pain_points, use_cases, themes, tone, geo, topics (array as specified).\n"
            "Be concise, high-signal, no fluff. Use common industry labels.\n\n"
            f"ENTITY_TYPE: {entity_type}\n"
            f"NAME: {name}\n"
            f"RAW FIELDS:\n{fields_text}\n\n"
            "Output JSON in this structure:\n"
            + schema
        )
        try:
            resp = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"},
            )
            return json.loads(resp.choices[0].message.content)
        except Exception as exc:
            print(f"âš ï¸ Normalization failed: {exc}", file=sys.stderr)
            return None

    def explain_match(self, brand_profile: Dict, creator_profile: Dict, scores: Dict[str, float]) -> Optional[str]:
        if self.provider != "openai" or not self.api_key:
            return None
        if not ensure_openai_confirmation("match explanation", allow_fallback=False):
            return None
        client = OpenAI(api_key=self.api_key)
        prompt = (
            "Given the normalized profiles of a brand and a creator plus per-dimension similarity scores, provide a short rationale (2 sentences) "
            "on commercial fit. Keep it factual and concise.\n\n"
            f"BRAND: {json.dumps(brand_profile)}\nCREATOR: {json.dumps(creator_profile)}\nSCORES: {json.dumps(scores)}"
        )
        try:
            resp = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            return resp.choices[0].message.content
        except Exception:
            return None


class EmbeddingClient:
    def __init__(self, provider: str = "openai", model: str = "text-embedding-3-large"):
        self.provider = provider
        self.model = model
        self.api_key = os.getenv("OPENAI_API_KEY")

    def embed(self, texts: List[str]) -> List[Optional[List[float]]]:
        if self.provider != "openai" or not self.api_key:
            return [None for _ in texts]
        client = OpenAI(api_key=self.api_key)
        try:
            resp = client.embeddings.create(model=self.model, input=texts)
            return [item.embedding for item in resp.data]
        except Exception as exc:
            print(f"âš ï¸ Embedding batch failed: {exc}", file=sys.stderr)
            return [None for _ in texts]


def ensure_openai_confirmation(action: str, allow_fallback: bool = False) -> bool:
    """
    Returns True if OpenAI can be used (confirmed), False if fallback should occur.
    """
    global OPENAI_CONFIRMED
    if OPENAI_CONFIRMED:
        return True
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        print("âŒ OPENAI_API_KEY not set.", file=sys.stderr)
        return False
    prompt = (
        f"âš ï¸ You are about to use the OpenAI API for: {action}. This may incur cost.\n"
        'Type YES to continue, anything else to abort: '
    )
    resp = input(prompt).strip()
    if resp == "YES":
        OPENAI_CONFIRMED = True
        return True
    if allow_fallback:
        print("â†©ï¸ OpenAI not confirmed; falling back to local provider.")
        return False
    print("Aborted; OpenAI not confirmed.")
    return False


def ensure_ollama_running() -> bool:
    try:
        res = requests.get(f"{OLLAMA_HOST}/api/version", timeout=5)
        res.raise_for_status()
        return True
    except Exception:
        print('âŒ Ollama server not running â€” start with "ollama serve"', file=sys.stderr)
        return False


@dataclass
class SpotifyShowRecord:
    show_url: str
    title: str
    author: str
    description: str
    thumbnail: Optional[str]
    scraped_at: str


class YouTubeChannelFetcher:
    def __init__(
        self,
        channels: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.channels = channels
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch(self) -> List[YouTubeChannelRecord]:
        results: List[YouTubeChannelRecord] = []
        for raw in self.channels:
            url = self._normalise_channel_url(raw)
            about_url = url.rstrip("/") + "/about"
            html = self._get(about_url)
            if not html:
                continue
            record = self._parse_about_page(url, html)
            if record:
                results.append(record)
            if self.delay:
                time.sleep(self.delay)
        return results

    def _normalise_channel_url(self, identifier: str) -> str:
        ident = identifier.strip()
        if ident.startswith("http://") or ident.startswith("https://"):
            return ident
        if ident.startswith("@"):
            return f"https://www.youtube.com/{ident}"
        if ident.startswith("UC"):
            return f"https://www.youtube.com/channel/{ident}"
        return f"https://www.youtube.com/@{ident}"

    def _get(self, url: str) -> Optional[str]:
        try:
            res = self.session.get(url, timeout=20)
            res.raise_for_status()
            return res.text
        except Exception as exc:
            print(f"âŒ Failed to fetch {url}: {exc}", file=sys.stderr)
            return None

    def _parse_about_page(self, channel_url: str, html: str) -> Optional[YouTubeChannelRecord]:
        soup = BeautifulSoup(html, "html.parser")
        title = (soup.find("meta", {"property": "og:title"}) or {}).get("content", "")
        description = (soup.find("meta", {"property": "og:description"}) or {}).get(
            "content", ""
        )

        about_text = ""
        about_el = soup.select_one("#description-container") or soup.select_one(
            "#description"
        )
        if about_el:
            about_text = about_el.get_text(separator=" ", strip=True)

        subscriber_text = None
        sub_el = soup.select_one("#subscriber-count")
        if sub_el:
            subscriber_text = sub_el.get_text(strip=True)

        scraped_at = datetime.utcnow().isoformat()
        if not title and not description and not about_text:
            return None
        return YouTubeChannelRecord(
            channel_url=channel_url,
            title=title,
            description=description,
            about=about_text,
            subscribers=subscriber_text,
            scraped_at=scraped_at,
        )


class SpotifyShowFetcher:
    def __init__(
        self,
        shows: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.shows = shows
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch(self) -> List[SpotifyShowRecord]:
        results: List[SpotifyShowRecord] = []
        for raw in self.shows:
            url = self._normalise_show_url(raw)
            html = self._get(url)
            if not html:
                continue
            oembed = self._get_oembed(url)
            record = self._parse_show_page(url, html, oembed)
            if record:
                results.append(record)
            if self.delay:
                time.sleep(self.delay)
        return results

    def _normalise_show_url(self, identifier: str) -> str:
        ident = identifier.strip()
        if ident.startswith("http://") or ident.startswith("https://"):
            return ident
        return f"https://open.spotify.com/show/{ident}"

    def _get(self, url: str) -> Optional[str]:
        try:
            res = self.session.get(url, timeout=20)
            res.raise_for_status()
            return res.text
        except Exception as exc:
            print(f"âŒ Failed to fetch {url}: {exc}", file=sys.stderr)
            return None

    def _get_oembed(self, url: str) -> Dict:
        try:
            res = self.session.get(
                "https://open.spotify.com/oembed",
                params={"url": url},
                timeout=15,
            )
            res.raise_for_status()
            return res.json()
        except Exception:
            return {}

    def _parse_show_page(self, show_url: str, html: str, oembed: Dict) -> Optional[SpotifyShowRecord]:
        soup = BeautifulSoup(html, "html.parser")
        title = oembed.get("title") or (soup.find("meta", {"property": "og:title"}) or {}).get(
            "content", ""
        )
        description = (
            (soup.find("meta", {"property": "og:description"}) or {}).get("content", "")
        )
        author = oembed.get("author_name", "")
        thumbnail = oembed.get("thumbnail_url") or (soup.find("meta", {"property": "og:image"}) or {}).get(
            "content"
        )

        scraped_at = datetime.utcnow().isoformat()
        if not title and not description:
            return None
        return SpotifyShowRecord(
            show_url=show_url,
            title=title or "",
            author=author or "",
            description=description or "",
            thumbnail=thumbnail,
            scraped_at=scraped_at,
        )


def ensure_podcastindex_tables(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS podcastindex_feeds (
            id INTEGER PRIMARY KEY,
            url TEXT,
            title TEXT,
            lastUpdate INTEGER,
            link TEXT,
            lastHttpStatus INTEGER,
            dead INTEGER,
            contentType TEXT,
            itunesId INTEGER,
            originalUrl TEXT,
            itunesAuthor TEXT,
            itunesOwnerName TEXT,
            explicit INTEGER,
            imageUrl TEXT,
            itunesType TEXT,
            generator TEXT,
            newestItemPubdate INTEGER,
            language TEXT,
            oldestItemPubdate INTEGER,
            episodeCount INTEGER,
            popularityScore INTEGER,
            priority INTEGER,
            createdOn INTEGER,
            updateFrequency INTEGER,
            chash TEXT,
            host TEXT,
            newestEnclosureUrl TEXT,
            podcastGuid TEXT,
            description TEXT,
            category1 TEXT,
            category2 TEXT,
            category3 TEXT,
            category4 TEXT,
            category5 TEXT,
            category6 TEXT,
            category7 TEXT,
            category8 TEXT,
            category9 TEXT,
            category10 TEXT,
            newestEnclosureDuration INTEGER
        )
        """
    )
    conn.execute("CREATE INDEX IF NOT EXISTS idx_podcastindex_feeds_itunesId ON podcastindex_feeds(itunesId)")


def ensure_creators_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creators (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            podcastindex_id INTEGER,
            podcastguid TEXT,
            itunesid INTEGER,
            language TEXT,
            normalized_summary TEXT,
            normalized_vertical TEXT,
            normalized_audience TEXT,
            normalized_pain_points TEXT,
            normalized_use_cases TEXT,
            normalized_themes TEXT,
            normalized_tone TEXT,
            normalized_geo TEXT,
            normalized_topics TEXT,
            title TEXT,
            description TEXT,
            feed_url TEXT,
            web_link TEXT,
            categories TEXT,
            newestItemPubdate INTEGER,
            oldestItemPubdate INTEGER,
            episodeCount INTEGER,
            updateFrequency INTEGER,
            vertical TEXT,
            audience_profile TEXT,
            industry_keywords TEXT,
            business_functions TEXT,
            pain_points TEXT,
            geo_focus TEXT,
            apple_topics TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_creators_pid ON creators(podcastindex_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_creators_itunesid ON creators(itunesid)")
    for col, coltype in [
        ("normalized_summary", "TEXT"),
        ("normalized_vertical", "TEXT"),
        ("normalized_audience", "TEXT"),
        ("normalized_pain_points", "TEXT"),
        ("normalized_use_cases", "TEXT"),
        ("normalized_themes", "TEXT"),
        ("normalized_tone", "TEXT"),
        ("normalized_geo", "TEXT"),
        ("normalized_topics", "TEXT"),
    ]:
        ensure_column(conn, "creators", col, coltype)


def ensure_creator_enrichment_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_enrichment (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            podcastindex_id INTEGER,
            uk_score REAL,
            source_spotify TEXT,
            source_apple TEXT,
            source_youtube TEXT,
            source_social_twitter TEXT,
            source_social_instagram TEXT,
            source_social_website TEXT,
            source_social_patreon TEXT,
            cleaned_text_block TEXT,
            raw_text_sources TEXT,
            normalized_text TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    ensure_column(conn, "creator_enrichment", "normalized_text", "TEXT")


def ensure_creator_country_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_country (
            podcastindex_id INTEGER PRIMARY KEY,
            itunesid INTEGER,
            country TEXT
        )
        """
    )


def ensure_podscan_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS podscan (
            id TEXT PRIMARY KEY,
            guid TEXT,
            podcastindex_id INTEGER,
            itunesid INTEGER,
            email TEXT,
            website TEXT,
            full_json TEXT,
            region TEXT
        )
        """
    )


def ensure_column(conn: sqlite3.Connection, table: str, column: str, coltype: str) -> None:
    existing = {row[1] for row in conn.execute(f"PRAGMA table_info({table})")}
    if column not in existing:
        conn.execute(f"ALTER TABLE {table} ADD COLUMN {column} {coltype}")


def ensure_brands_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS brands (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            brand_name TEXT,
            website_url TEXT UNIQUE,
            source_website_raw TEXT,
            source_linkedin_raw TEXT,
            source_social_raw TEXT,
            extracted_description TEXT,
            extracted_product_category TEXT,
            extracted_target_audience TEXT,
            extracted_tone TEXT,
            extracted_key_themes TEXT,
            extracted_goals TEXT,
            brand_vertical TEXT,
            brand_audience_profile TEXT,
            brand_use_cases TEXT,
            brand_pain_points TEXT,
            brand_geo_focus TEXT,
            brand_keywords TEXT,
            brand_summary TEXT,
            brand_vertical_override TEXT,
            flag_inconsistent_positioning INTEGER,
            recommended_vertical TEXT,
            recommended_confidence REAL,
            candidate_verticals TEXT,
            normalized_summary TEXT,
            normalized_vertical TEXT,
            normalized_audience TEXT,
            normalized_pain_points TEXT,
            normalized_use_cases TEXT,
            normalized_themes TEXT,
            normalized_tone TEXT,
            normalized_geo TEXT,
            normalized_topics TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    # Ensure new columns on existing DBs
    for col, coltype in [
        ("brand_vertical", "TEXT"),
        ("brand_audience_profile", "TEXT"),
        ("brand_use_cases", "TEXT"),
        ("brand_pain_points", "TEXT"),
        ("brand_geo_focus", "TEXT"),
        ("brand_keywords", "TEXT"),
        ("brand_summary", "TEXT"),
        ("brand_vertical_override", "TEXT"),
        ("flag_inconsistent_positioning", "INTEGER"),
        ("recommended_vertical", "TEXT"),
        ("recommended_confidence", "REAL"),
        ("candidate_verticals", "TEXT"),
        ("normalized_summary", "TEXT"),
        ("normalized_vertical", "TEXT"),
        ("normalized_audience", "TEXT"),
        ("normalized_pain_points", "TEXT"),
        ("normalized_use_cases", "TEXT"),
        ("normalized_themes", "TEXT"),
        ("normalized_tone", "TEXT"),
        ("normalized_geo", "TEXT"),
        ("normalized_topics", "TEXT"),
    ]:
        ensure_column(conn, "brands", col, coltype)


def ensure_creator_vectors_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_vectors (
            podcastindex_id INTEGER PRIMARY KEY,
            embedding TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def ensure_brand_vectors_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS brand_vectors (
            adlid INTEGER PRIMARY KEY,
            embedding TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def ensure_entity_embeddings_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS entity_embeddings (
            embedding_id INTEGER PRIMARY KEY AUTOINCREMENT,
            entity_type TEXT CHECK(entity_type IN ('creator', 'brand')),
            entity_id INTEGER,
            vector_type TEXT,
            embedding_vector TEXT,
            confidence REAL,
            source_text TEXT,
            source_normalized_id INTEGER,
            model_name TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_entity_embeddings ON entity_embeddings(entity_type, entity_id)"
    )
    ensure_column(conn, "entity_embeddings", "source_normalized_id", "INTEGER")
    ensure_column(conn, "entity_embeddings", "vector_type", "TEXT")
    ensure_column(conn, "entity_embeddings", "model_name", "TEXT")


def ensure_entity_normalized_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS entity_normalized (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            entity_type TEXT NOT NULL,
            entity_id INTEGER NOT NULL,
            version TEXT DEFAULT 'v1',
            normalized_block TEXT NOT NULL,
            norm_fields TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute(
        """
        CREATE UNIQUE INDEX IF NOT EXISTS idx_entity_norm_unique
        ON entity_normalized(entity_type, entity_id, version)
        """
    )


def ensure_apple_categories_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS apple_podcast_categories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            apple_id TEXT,
            parent_apple_id TEXT,
            name TEXT,
            full_path TEXT
        )
        """
    )
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_apple_cat_parent ON apple_podcast_categories(parent_apple_id)"
    )

def ensure_creator_topics_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_topics (
            podcastindex_id INTEGER,
            apple_category_id TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY(podcastindex_id, apple_category_id)
        )
        """
    )
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_creator_topics_pid ON creator_topics(podcastindex_id)"
    )

def ensure_brand_topics_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS brand_topics (
            adlid INTEGER,
            apple_category_id TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY(adlid, apple_category_id)
        )
        """
    )
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_brand_topics_adlid ON brand_topics(adlid)"
    )


def ensure_normalization_lexicon_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS normalization_lexicon (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            raw_label TEXT,
            normalized_label TEXT,
            label_type TEXT,
            notes TEXT
        )
        """
    )


def configure_sqlite(conn: sqlite3.Connection) -> None:
    # Reduce lock contention and waits.
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA busy_timeout=5000;")


def ensure_apple_discovery_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS apple_discovery (
            trackId INTEGER PRIMARY KEY,
            collectionName TEXT,
            artistName TEXT,
            feedUrl TEXT,
            artworkUrl TEXT,
            primaryGenreName TEXT,
            genres TEXT,
            country TEXT,
            trackCount INTEGER,
            releaseDate TEXT,
            description TEXT,
            raw_json TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def clear_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    response = input(
        f"This will DELETE all rows from creator_enrichment in {db_path}. Type YES to confirm: "
    ).strip()
    if response != "YES":
        print("Aborted; no changes made.")
        return 0
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        conn.execute("DELETE FROM creator_enrichment")
        conn.commit()
        print("âœ… Cleared creator_enrichment.")
        return 0
    finally:
        conn.close()


def show_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        def load_json(val):
            try:
                return json.loads(val) if val else {}
            except Exception:
                return {}

        def auto_normalize_creator(podcastindex_id: int, *, use_podscan: bool) -> bool:
            exists = conn.execute(
                "SELECT 1 FROM entity_normalized WHERE entity_type='creator' AND entity_id=? AND version='v1'",
                (podcastindex_id,),
            ).fetchone()
            if exists:
                return False
            name = f"podcast {podcastindex_id}"
            fields_parts: List[str] = []
            if use_podscan:
                row = conn.execute(
                    """
                    SELECT c.title, ps.full_json
                    FROM creators c
                    JOIN podscan ps ON ps.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id=?
                      AND ps.full_json IS NOT NULL
                      AND TRIM(ps.full_json) <> ''
                    """,
                    (podcastindex_id,),
                ).fetchone()
                if not row:
                    return False
                name = row["title"] or name
                try:
                    pod = json.loads(row["full_json"])
                except Exception:
                    print(f"âš ï¸ Invalid Podscan JSON for creator {podcastindex_id}; skipping normalization.")
                    return False
                desc = pod.get("podcast_description") or ""
                if pod.get("podcast_name"):
                    name = pod.get("podcast_name") or name
                if not desc:
                    return False
                fields_parts.append(f"DESCRIPTION: {desc}")
            else:
                row = conn.execute(
                    """
                    SELECT c.*, ce.cleaned_text_block, ce.raw_text_sources
                    FROM creators c
                    LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id=?
                    """,
                    (podcastindex_id,),
                ).fetchone()
                if not row:
                    return False
                name = row["title"] or name
                for key in ["title", "description", "categories", "cleaned_text_block"]:
                    if row[key]:
                        fields_parts.append(f"{key.upper()}: {row[key]}")
                try:
                    raw_sources = json.loads(row["raw_text_sources"]) if row["raw_text_sources"] else {}
                except Exception:
                    raw_sources = {}
                for k, v in (raw_sources or {}).items():
                    if v:
                        fields_parts.append(f"{k.upper()}: {v}")
            raw_blob = "\n".join(fields_parts)
            if not raw_blob:
                return False
            if len(raw_blob) > 12000:
                raw_blob = raw_blob[:12000]
            simplified = simplify_entity_text("creator", name, raw_blob, norm_provider, ollama_model) or raw_blob
            fields_text = f"SIMPLIFIED:\n{simplified}\n\nRAW:\n{raw_blob}"
            debug_tag = f"creator:{podcastindex_id}"
            norm = normalize_entity_to_json(
                "creator",
                name,
                fields_text,
                categories_hint,
                provider=norm_provider,
                model=ollama_model,
                debug_tag=debug_tag,
            )
            if not norm or not norm.get("summary"):
                return False
            for key in ["vertical", "audience", "themes", "use_cases", "pain_points", "tone", "geo"]:
                norm[key] = map_label_from_lexicon(conn, norm.get(key))
            topics_val = norm.get("topics")
            topics_str = map_topics_to_apple(conn, topics_val)
            block_parts = [
                f"SUMMARY:\n{norm.get('summary','')}",
                f"VERTICAL:\n{norm.get('vertical','')}",
                f"AUDIENCE:\n{norm.get('audience','')}",
                f"PAIN_POINTS:\n{norm.get('pain_points','')}",
                f"USE_CASES:\n{norm.get('use_cases','')}",
                f"THEMES:\n{norm.get('themes','')}",
                f"TONE:\n{norm.get('tone','')}",
                f"GEO:\n{norm.get('geo','')}",
                f"TOPICS:\n{topics_str}",
            ]
            normalized_block = "\n".join(block_parts)
            conn.execute(
                """
                INSERT OR REPLACE INTO entity_normalized (entity_type, entity_id, version, normalized_block, norm_fields)
                VALUES ('creator', ?, 'v1', ?, ?)
                """,
                (podcastindex_id, normalized_block, json.dumps(norm)),
            )
            def to_str(val):
                if val is None:
                    return None
                if isinstance(val, list):
                    return ", ".join(str(x) for x in val if x)
                return str(val)
            conn.execute(
                """
                UPDATE creators SET
                    normalized_summary=?, normalized_vertical=?, normalized_audience=?, normalized_pain_points=?,
                    normalized_use_cases=?, normalized_themes=?, normalized_tone=?, normalized_geo=?, normalized_topics=?
                WHERE podcastindex_id=?
                """,
                (
                    to_str(norm.get("summary")),
                    to_str(norm.get("vertical")),
                    to_str(norm.get("audience")),
                    to_str(norm.get("pain_points")),
                    to_str(norm.get("use_cases")),
                    to_str(norm.get("themes")),
                    to_str(norm.get("tone")),
                    to_str(norm.get("geo")),
                    to_str(topics_str),
                    podcastindex_id,
                ),
            )
            return True

        def count_nonempty(d: Dict) -> int:
            if not isinstance(d, dict):
                return 0
            return sum(1 for v in d.values() if v not in (None, "", [], {}))

        if not args.source:
            cur = conn.execute(
                """
                SELECT ce.podcastindex_id, ce.source_spotify, ce.source_apple, ce.source_youtube,
                       ce.source_social_twitter, ce.source_social_instagram, ce.source_social_website, ce.source_social_patreon,
                       pf.title as name
                FROM creator_enrichment ce
                LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
                LIMIT ?
                """,
                (args.limit,),
            )
            rows = cur.fetchall()
            if not rows:
                print("No enrichment rows found.")
                return 0
            for row in rows:
                spotify_j = load_json(row["source_spotify"])
                apple_j = load_json(row["source_apple"])
                youtube_j = load_json(row["source_youtube"])
                socials_all = [
                    load_json(row["source_social_twitter"]),
                    load_json(row["source_social_instagram"]),
                    load_json(row["source_social_website"]),
                    load_json(row["source_social_patreon"]),
                ]
                social_count = sum(count_nonempty(s) for s in socials_all)
                print(f"name: {row['name'] or 'N/A'}")
                print(f"spotify: {count_nonempty(spotify_j)} data")
                print(f"apple: {count_nonempty(apple_j)} data")
                print(f"youtube: {count_nonempty(youtube_j)} data")
                print(f"social: {social_count} data")
                print("")
            return 0

        if args.source == "social":
            cur = conn.execute(
                """
                SELECT ce.podcastindex_id, ce.source_social_twitter, ce.source_social_instagram,
                       ce.source_social_website, ce.source_social_patreon, pf.title as name
                FROM creator_enrichment ce
                LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
                LIMIT ?
                """,
                (args.limit,),
            )
            rows = cur.fetchall()
            if not rows:
                print("No rows with social data.")
                return 0
            for row in rows:
                print(f"{row['name'] or row['podcastindex_id']}:")
                for label, col in [
                    ("twitter", "source_social_twitter"),
                    ("instagram", "source_social_instagram"),
                    ("website", "source_social_website"),
                    ("patreon", "source_social_patreon"),
                ]:
                    data = load_json(row[col])
                    if not data or not isinstance(data, dict):
                        continue
                    if count_nonempty(data) == 0:
                        continue
                    print(f"  {label}: {data}")
                print("")
            return 0

        col = {
            "spotify": "source_spotify",
            "apple": "source_apple",
            "youtube": "source_youtube",
            "twitter": "source_social_twitter",
            "instagram": "source_social_instagram",
            "website": "source_social_website",
            "patreon": "source_social_patreon",
        }[args.source]
        cur = conn.execute(
            f"""
            SELECT ce.podcastindex_id, ce.{col}, pf.title as name
            FROM creator_enrichment ce
            LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
            WHERE ce.{col} IS NOT NULL
            LIMIT ?
            """,
            (args.limit,),
        )
        rows = cur.fetchall()
        if not rows:
            print(f"No rows with {args.source} data.")
            return 0
        for row in rows:
            data = load_json(row[col])
            if not isinstance(data, dict):
                print(f"{row['name'] or row['podcastindex_id']}: (invalid JSON)")
                continue
            print(f"{row['name'] or row['podcastindex_id']}:")
            for k, v in data.items():
                if v in (None, "", [], {}):
                    continue
                print(f"  {k}: {v}")
            print("")
        return 0
    finally:
        conn.close()


def summary_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        total_creators = conn.execute("SELECT COUNT(*) FROM creators").fetchone()[0]
        total_enriched = conn.execute("SELECT COUNT(*) FROM creator_enrichment").fetchone()[0]

        def has_payload(val) -> bool:
            if isinstance(val, dict):
                if not val:
                    return False
                status = val.get("status")
                if status in ("error", "not_found", "missing_credentials"):
                    return False
                for v in val.values():
                    if has_payload(v):
                        return True
                return False
            if isinstance(val, list):
                return any(has_payload(v) for v in val)
            return val not in (None, "", [], {})

        def count_valid(col: str) -> int:
            rows = conn.execute(
                f"SELECT {col} FROM creator_enrichment WHERE {col} IS NOT NULL"
            ).fetchall()
            count = 0
            for r in rows:
                try:
                    d = json.loads(r[col]) if r[col] else {}
                except Exception:
                    d = {}
                if has_payload(d):
                    count += 1
            return count

        def count_any_social() -> int:
            rows = conn.execute(
                """
                SELECT source_social_twitter, source_social_instagram, source_social_website, source_social_patreon
                FROM creator_enrichment
                """
            ).fetchall()
            count = 0
            for r in rows:
                blobs = [r["source_social_twitter"], r["source_social_instagram"], r["source_social_website"], r["source_social_patreon"]]
                payload = False
                for blob in blobs:
                    try:
                        d = json.loads(blob) if blob else {}
                    except Exception:
                        d = {}
                    if has_payload(d):
                        payload = True
                        break
                if payload:
                    count += 1
            return count

        spotify_ok = count_valid("source_spotify")
        apple_ok = count_valid("source_apple")
        youtube_ok = count_valid("source_youtube")
        social_ok = count_any_social()

        print(f"Total creators: {total_creators}")
        print(f"Enriched: {total_enriched} / {total_creators}")
        print(f"    spotify: {spotify_ok} / {total_enriched}")
        print(f"    apple: {apple_ok} / {total_enriched}")
        print(f"    youtube: {youtube_ok} / {total_enriched}")
        print(f"    socials: {social_ok} / {total_enriched}")
        return 0
    finally:
        conn.close()


def ingest_podcastindex_feeds(args: argparse.Namespace) -> int:
    source_db = Path(args.source_db)
    target_db = Path(args.target_db)

    batch_size = args.batch_size
    refresh_feeds = bool(getattr(args, "refresh_feeds", False))
    if refresh_feeds and not source_db.exists():
        print(f"âŒ Source DB not found: {source_db}", file=sys.stderr)
        return 1

    if refresh_feeds:
        print(f"ðŸ”„ Refreshing feeds from {source_db} -> {target_db} (batch size {batch_size})")
    else:
        print(f"ðŸ”„ Populating creators from existing feeds in {target_db} (batch size {batch_size})")

    tgt = sqlite3.connect(target_db, timeout=10)
    tgt.row_factory = sqlite3.Row
    configure_sqlite(tgt)

    try:
        ensure_podcastindex_tables(tgt)
        ensure_creators_table(tgt)
        ensure_creator_enrichment_table(tgt)
        ensure_column(tgt, "creator_enrichment", "source_social_twitter", "TEXT")
        ensure_column(tgt, "creator_enrichment", "source_social_instagram", "TEXT")
        ensure_column(tgt, "creator_enrichment", "source_social_website", "TEXT")
        ensure_column(tgt, "creator_enrichment", "source_social_patreon", "TEXT")
        # New creator metadata columns
        for col in [
            ("vertical", "TEXT"),
            ("audience_profile", "TEXT"),
            ("industry_keywords", "TEXT"),
            ("business_functions", "TEXT"),
            ("pain_points", "TEXT"),
            ("geo_focus", "TEXT"),
            ("title", "TEXT"),
            ("description", "TEXT"),
            ("feed_url", "TEXT"),
            ("web_link", "TEXT"),
            ("categories", "TEXT"),
            ("newestItemPubdate", "INTEGER"),
            ("oldestItemPubdate", "INTEGER"),
            ("episodeCount", "INTEGER"),
            ("updateFrequency", "INTEGER"),
        ]:
            ensure_column(tgt, "creators", col[0], col[1])

        inserted_feeds = 0
        inserted_creators = 0

        if refresh_feeds:
            src = sqlite3.connect(source_db, timeout=10)
            src.row_factory = sqlite3.Row
            configure_sqlite(src)
            try:
                existing_ids = {row[0] for row in tgt.execute("SELECT id FROM podcastindex_feeds")}
                placeholders = ",".join("?" for _ in PODCASTINDEX_COLUMNS)
                feed_sql = f"""
                    INSERT OR REPLACE INTO podcastindex_feeds ({','.join(PODCASTINDEX_COLUMNS)})
                    VALUES ({placeholders})
                """
                total_processed = 0
                src_cursor = src.execute("SELECT * FROM podcasts")
                while True:
                    rows = src_cursor.fetchmany(batch_size)
                    if not rows:
                        break
                    feed_rows = []
                    for row in rows:
                        podcastindex_id = row["id"]
                        feed_rows.append(tuple(row[col] for col in PODCASTINDEX_COLUMNS))
                        if podcastindex_id not in existing_ids:
                            inserted_feeds += 1
                            existing_ids.add(podcastindex_id)
                    if feed_rows:
                        tgt.executemany(feed_sql, feed_rows)
                        tgt.commit()
                    total_processed += len(rows)
                    if total_processed % (batch_size * 5) == 0:
                        print(
                            f"Feeds pass: processed {total_processed:,} rows | "
                            f"inserted/updated feeds: {inserted_feeds:,}"
                        )
                print(f"âœ… Feeds refresh complete. Total feeds inserted/updated: {inserted_feeds:,}")
            finally:
                src.close()

        # Step 2: filter feeds -> creators (always runs)
        existing_creators_by_pid = {
            row[0] for row in tgt.execute("SELECT podcastindex_id FROM creators")
        }
        three_years_ago = int(time.time()) - 3 * 365 * 24 * 3600
        filtered = tgt.execute(
            """
            SELECT *
            FROM podcastindex_feeds
            WHERE language LIKE '%en%'
              AND (lastHttpStatus IS NULL OR lastHttpStatus = 200)
              AND LENGTH(description) >= 50
              AND newestItemPubdate IS NOT NULL
              AND newestItemPubdate >= ?
              AND episodeCount IS NOT NULL AND episodeCount >= 3
            ORDER BY newestItemPubdate DESC
            """,
            (three_years_ago,),
        )
        creator_sql = """
            INSERT OR IGNORE INTO creators (
                podcastindex_id, podcastguid, itunesid, language,
                title, description, feed_url, web_link, categories,
                newestItemPubdate, oldestItemPubdate, episodeCount, updateFrequency
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        processed_filtered = 0
        while True:
            rows = filtered.fetchmany(batch_size)
            if not rows:
                break
            creator_rows = []
            for row in rows:
                pid = row["id"]
                if pid in existing_creators_by_pid:
                    continue
                categories = []
                for key in ("category1", "category2", "category3", "category4", "category5", "category6", "category7", "category8", "category9", "category10"):
                    if row[key]:
                        categories.append(row[key])
                categories_text = ", ".join(categories) if categories else None
                creator_rows.append(
                    (
                        pid,
                        row["podcastGuid"],
                        row["itunesId"],
                        row["language"],
                        row["title"],
                        row["description"],
                        row["url"],
                        row["link"],
                        categories_text,
                        row["newestItemPubdate"],
                        row["oldestItemPubdate"],
                        row["episodeCount"],
                        row["updateFrequency"],
                    )
                )
                existing_creators_by_pid.add(pid)
                inserted_creators += 1
            if creator_rows:
                tgt.executemany(creator_sql, creator_rows)
                tgt.commit()
            processed_filtered += len(rows)
            if processed_filtered % (batch_size * 5) == 0:
                print(
                    f"Creators pass: processed {processed_filtered:,} filtered rows | "
                    f"inserted creators: {inserted_creators:,}"
                )

        print(
            f"âœ… Done. Feeds inserted/updated: {inserted_feeds:,}; "
            f"creators inserted: {inserted_creators:,}"
        )
        return 0
    finally:
        tgt.close()
        tgt.close()


def parse_comma_list(raw: Optional[str], *, default: Sequence[str]) -> List[str]:
    if not raw:
        return list(default)
    if raw.lower() == "all":
        return list(default)
    return [chunk.strip() for chunk in raw.split(",") if chunk.strip()]


def load_list_inputs(raw: Optional[str], path: Optional[Path]) -> List[str]:
    items: List[str] = []
    if raw:
        items.extend(parse_comma_list(raw, default=[]))
    if path and path.exists():
        items.extend([line.strip() for line in path.read_text().splitlines() if line.strip()])
    # Deduplicate while preserving order
    seen = set()
    deduped = []
    for item in items:
        if item not in seen:
            seen.add(item)
            deduped.append(item)
    return deduped


def discover_apple(args: argparse.Namespace) -> int:
    terms = load_list_inputs(args.terms, args.terms_file)
    if not terms:
        print("âŒ No search terms provided.", file=sys.stderr)
        return 1

    db_path = Path(args.adl_db)
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_apple_discovery_table(conn)

    seen_ids = {
        row[0] for row in conn.execute("SELECT trackId FROM apple_discovery")
    }

    inserted = 0
    total_terms = len(terms)
    try:
        for idx, term in enumerate(terms, start=1):
            print(f"ðŸ”Ž Term {idx}/{total_terms}: '{term}'")
            offset = 0
            pages = 0
            while pages < args.max_pages:
                backoff = 5
                params = {
                    "term": term,
                    "country": args.country,
                    "media": "podcast",
                    "limit": args.limit_per_term,
                    "offset": offset,
                }
                while True:
                    res = requests.get("https://itunes.apple.com/search", params=params, timeout=15)
                    if res.status_code == 429:
                        print(f"  429 rate limit hit; sleeping {backoff}s...")
                        time.sleep(backoff)
                        backoff = min(backoff * 2, 60)
                        continue
                    res.raise_for_status()
                    break
                payload = res.json()
                results = payload.get("results", [])
                if not results:
                    print(f"  No more results at page {pages}.")
                    break
                rows = []
                for r in results:
                    tid = r.get("trackId")
                    if not tid or tid in seen_ids:
                        continue
                    seen_ids.add(tid)
                    rows.append(
                        (
                            tid,
                            r.get("collectionName"),
                            r.get("artistName"),
                            r.get("feedUrl"),
                            r.get("artworkUrl100"),
                            r.get("primaryGenreName"),
                            json.dumps(r.get("genres")),
                            r.get("country"),
                            r.get("trackCount"),
                            r.get("releaseDate"),
                            r.get("description") or r.get("longDescription"),
                            json.dumps(r),
                        )
                    )
                if rows:
                    conn.executemany(
                        """
                        INSERT INTO apple_discovery (
                            trackId, collectionName, artistName, feedUrl, artworkUrl,
                            primaryGenreName, genres, country, trackCount, releaseDate,
                            description, raw_json
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        rows,
                    )
                    conn.commit()
                    inserted += len(rows)
                    print(f"  Page {pages+1}: inserted {len(rows)} new (total {inserted})")
                offset += args.limit_per_term
                pages += 1
                time.sleep(0.5)
        print(f"âœ… Apple discovery inserted {inserted} new rows into apple_discovery.")
        return 0
    finally:
        conn.close()


def ingest_apple_discovery(args: argparse.Namespace) -> int:
    source_db = Path(args.source_db)
    target_db = Path(args.target_db)

    if not target_db.exists():
        print(f"âŒ Target DB not found: {target_db}", file=sys.stderr)
        return 1

    tgt = sqlite3.connect(target_db, timeout=10)
    tgt.row_factory = sqlite3.Row
    configure_sqlite(tgt)
    try:
        ensure_creators_table(tgt)
        ensure_apple_discovery_table(tgt)

        existing_itunes = {
            row[0] for row in tgt.execute("SELECT itunesid FROM creators WHERE itunesid IS NOT NULL")
        }

        rows = tgt.execute(
            "SELECT trackId, collectionName, country FROM apple_discovery"
        ).fetchall()
        to_insert = []
        for r in rows:
            tid = r["trackId"]
            if tid in existing_itunes:
                continue
            # Use negative trackId to avoid clashing with PodcastIndex IDs
            pid = -int(tid)
            language = "en-gb" if (r["country"] or "").lower() == "gb" else "en"
            to_insert.append((pid, None, tid, language))

        if not to_insert:
            print("No new Apple discovery rows to merge.")
            return 0

        tgt.executemany(
            """
            INSERT OR IGNORE INTO creators (podcastindex_id, podcastguid, itunesid, language)
            VALUES (?, ?, ?, ?)
            """,
            to_insert,
        )
        tgt.commit()
        print(f"âœ… Merged {len(to_insert)} Apple discovery rows into creators (pid = -trackId).")
        return 0
    finally:
        tgt.close()


def fetch_apple_metadata(itunes_id: Optional[int], title: str) -> Dict:
    if not title and not itunes_id:
        return {}
    try:
        params = {"entity": "podcast"}
        if itunes_id:
            params["id"] = itunes_id
            url = "https://itunes.apple.com/lookup"
        else:
            params["term"] = title
            params["limit"] = 3
            url = "https://itunes.apple.com/search"
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        results = data.get("results", [])
        if not results:
            return {}
        best = results[0]
        desc = (
            best.get("description")
            or best.get("longDescription")
            or best.get("collectionDescription")
            or ""
        )
        if desc and desc.lower() in ("notexplicit", "explicit"):
            desc = ""
        return {
            "title": best.get("collectionName") or best.get("trackName"),
            "description": desc,
            "genres": best.get("genres"),
            "publisher": best.get("artistName"),
            "artistUrl": best.get("artistViewUrl"),
            "collectionUrl": best.get("collectionViewUrl"),
            "feedUrl": best.get("feedUrl"),
            "country": best.get("country"),
            "status": "ok",
        }
    except Exception as exc:
        print(f"âš ï¸ Apple lookup failed: {exc}", file=sys.stderr)
        return {}


def fetch_podscan_country(
    itunes_id: Optional[int],
) -> Tuple[Optional[str], Optional[str], Optional[Dict]]:
    if not itunes_id:
        return None, None, None
    key = os.getenv("PODSCAN_API_KEY")
    if not key:
        print("âŒ PODSCAN_API_KEY not set; cannot lookup country.", file=sys.stderr)
        return None, None, None
    headers = {
        "Authorization": f"Bearer {key}",
        "Accept": "application/json",
        "User-Agent": "adelined/1.0",
    }
    res = requests.get(
        f"{PODSCAN_API_BASE}/podcasts/search/by/itunesid",
        headers=headers,
        params={"itunes_id": itunes_id},
        timeout=15,
    )
    if res.status_code == 404:
        return "404", None, None
    res.raise_for_status()
    data = res.json()
    podcast = data.get("podcast") or {}
    region = podcast.get("region")
    reach = data.get("reach") or (podcast.get("reach") or {})
    social_links = reach.get("social_links") or podcast.get("social_links") or []
    email = reach.get("email")
    if not email:
        for link in social_links:
            item = link or {}
            link_email = item.get("email")
            if link_email:
                email = link_email
                break
            url = item.get("url") or ""
            if url.startswith("mailto:"):
                email = url.replace("mailto:", "", 1)
                break
            if item.get("platform") == "email" and url:
                email = url
                break
    return (str(region).upper() if region else None), email, data


def extract_podscan_contact(podcast: Dict) -> Tuple[Optional[str], Optional[str]]:
    reach = podcast.get("reach") or {}
    social_links = reach.get("social_links") or []
    email = reach.get("email")
    website = reach.get("website") or podcast.get("podcast_url")
    if not email or not website:
        for link in social_links:
            item = link or {}
            if not email:
                link_email = item.get("email")
                if link_email:
                    email = link_email
            if not website:
                website = item.get("website")
                if not website:
                    url = item.get("url") or ""
                    if item.get("platform") == "website" and url:
                        website = url
            if email and website:
                break
    return email, website


def fetch_spotify_metadata(show_url: Optional[str], title: str) -> Dict:
    client_id = os.getenv("SPOTIFY_CLIENT_ID")
    client_secret = os.getenv("SPOTIFY_CLIENT_SECRET")
    if not client_id or not client_secret:
        return {"status": "missing_credentials"}

    try:
        token_res = requests.post(
            "https://accounts.spotify.com/api/token",
            data={"grant_type": "client_credentials"},
            auth=(client_id, client_secret),
            timeout=15,
        )
        token_res.raise_for_status()
        access_token = token_res.json().get("access_token")
        headers = {"Authorization": f"Bearer {access_token}"}

        show_id = None
        if show_url and "open.spotify.com/show/" in show_url:
            show_id = show_url.rstrip("/").split("/")[-1]
        else:
            if not title:
                return {"status": "not_found"}
            search_res = requests.get(
                "https://api.spotify.com/v1/search",
                headers=headers,
                params={"q": title, "type": "show", "market": "GB", "limit": 3},
                timeout=15,
            )
            search_res.raise_for_status()
            items = search_res.json().get("shows", {}).get("items", [])
            if items:
                # pick best by title similarity
                from difflib import SequenceMatcher

                scored = []
                for it in items:
                    score = SequenceMatcher(None, title.lower(), (it.get("name") or "").lower()).ratio()
                    scored.append((score, it))
                scored.sort(reverse=True, key=lambda x: x[0])
                top_score, top_item = scored[0]
                if top_score >= 0.8:
                    show_id = top_item["id"]

        if not show_id:
            return {"status": "not_found"}

        show_res = requests.get(
            f"https://api.spotify.com/v1/shows/{show_id}",
            headers=headers,
            params={"market": "GB"},
            timeout=15,
        )
        show_res.raise_for_status()
        show_data = show_res.json()

        # Verify title match to avoid mismatches
        if title:
            from difflib import SequenceMatcher

            if SequenceMatcher(None, title.lower(), (show_data.get("name") or "").lower()).ratio() < 0.8:
                return {"status": "not_found"}

        ep_res = requests.get(
            f"https://api.spotify.com/v1/shows/{show_id}/episodes",
            headers=headers,
            params={"market": "GB", "limit": 10},
            timeout=15,
        )
        ep_res.raise_for_status()
        eps = ep_res.json().get("items", []) or []
        ep_samples = [
            {"name": ep.get("name"), "description": ep.get("description")}
            for ep in eps[:10]
        ]

        return {
            "show": {
                "id": show_id,
                "name": show_data.get("name"),
                "description": show_data.get("description"),
                "publisher": show_data.get("publisher"),
                "languages": show_data.get("languages"),
                "total_episodes": show_data.get("total_episodes"),
            },
            "episodes_sample": ep_samples,
            "status": "ok",
        }
    except requests.HTTPError as http_err:
        status_code = http_err.response.status_code if http_err.response else None
        detail = ""
        try:
            detail = http_err.response.json()
        except Exception:
            detail = http_err.response.text if hasattr(http_err, "response") else ""
        print(f"âš ï¸ Spotify fetch failed: {http_err} detail={detail}", file=sys.stderr)
        return {"status": "error", "error": str(http_err), "detail": detail, "http_status": status_code}
    except Exception as exc:
        print(f"âš ï¸ Spotify fetch failed: {exc}", file=sys.stderr)
        return {"status": "error", "error": str(exc)}


def fetch_youtube_metadata(title: str, *, mode: str = "lean") -> Dict:
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        return {"status": "missing_credentials"}

    try:
        # Search channel
        search_res = requests.get(
            "https://www.googleapis.com/youtube/v3/search",
            params={
                "part": "snippet",
                "q": title,
                "type": "channel",
                "regionCode": "GB",
                "maxResults": 8,
                "key": api_key,
            },
            timeout=15,
        )
        search_res.raise_for_status()
        items = search_res.json().get("items", [])
        picked_channel = None

        for ch in items:
            channel_id = ch["snippet"]["channelId"]
            # Fetch channel details to get full description
            chan_res = requests.get(
                "https://www.googleapis.com/youtube/v3/channels",
                params={
                    "part": "snippet",
                    "id": channel_id,
                    "key": api_key,
                },
                timeout=15,
            )
            if chan_res.status_code != 200:
                continue
            chan_items = chan_res.json().get("items", [])
            if not chan_items:
                continue
            chan_snip = chan_items[0].get("snippet", {}) or {}
            channel_title = chan_snip.get("title", "")
            channel_description = chan_snip.get("description", "")

            lower_title = channel_title.lower()
            if "topic" in lower_title or lower_title.endswith(" - topic"):
                # Reject auto-generated topic channels
                continue
            if not channel_description or len(channel_description.strip()) < 20:
                # Reject channels with no meaningful about text
                continue

            picked_channel = {
                "id": channel_id,
                "title": channel_title,
                "about": channel_description,
            }
            break

        if not picked_channel:
            return {"status": "not_found"}

        channel_id = picked_channel["id"]
        channel_title = picked_channel["title"]
        channel_description = picked_channel["about"]

        # Fetch top videos by viewCount
        video_limit = 1 if mode == "lean" else 3
        videos_res = requests.get(
            "https://www.googleapis.com/youtube/v3/search",
            params={
                "part": "snippet",
                "channelId": channel_id,
                "order": "viewCount",
                "maxResults": video_limit,
                "type": "video",
                "key": api_key,
            },
            timeout=15,
        )
        videos_res.raise_for_status()
        video_items = videos_res.json().get("items", []) or []
        videos_sample = []
        comments_sample: List[str] = []

        for vid in video_items:
            vid_id = vid["id"]["videoId"]
            vid_title = vid["snippet"]["title"]
            vid_desc = vid["snippet"].get("description", "")
            videos_sample.append({"id": vid_id, "title": vid_title, "description": vid_desc})

            # Comments
            if mode != "lean":
                comments_res = requests.get(
                    "https://www.googleapis.com/youtube/v3/commentThreads",
                    params={
                        "part": "snippet",
                        "videoId": vid_id,
                        "maxResults": 50,
                        "order": "relevance",
                        "textFormat": "plainText",
                        "key": api_key,
                    },
                    timeout=15,
                )
                if comments_res.status_code == 200:
                    threads = comments_res.json().get("items", []) or []
                    for th in threads:
                        top = th.get("snippet", {}).get("topLevelComment", {}).get("snippet", {})
                        txt = top.get("textDisplay")
                        if txt:
                            comments_sample.append(txt)
                        if len(comments_sample) >= 50:
                            break
                if len(comments_sample) >= 50:
                    break

        return {
            "channel": picked_channel,
            "videos_sample": videos_sample,
            "comments_sample": comments_sample[:50],
            "status": "ok",
        }
    except requests.HTTPError as http_err:
        detail = ""
        status_code = http_err.response.status_code if http_err.response else None
        try:
            detail = http_err.response.json()
        except Exception:
            detail = http_err.response.text if hasattr(http_err, "response") else ""
        print(f"âš ï¸ YouTube fetch failed: {http_err} detail={detail}", file=sys.stderr)
        return {"status": "error", "error": str(http_err), "detail": detail, "http_status": status_code}
    except Exception as exc:
        print(f"âš ï¸ YouTube fetch failed: {exc}", file=sys.stderr)
        return {"status": "error", "error": str(exc)}


def fetch_social_bios(title: str) -> Dict:
    return {}


def build_text_block(
    base: Dict,
    apple: Dict,
    spotify: Dict,
    youtube: Dict,
    social: Dict,
    extracted: Optional[Dict] = None,
) -> Tuple[str, Dict]:
    # Compose raw sources
    raw_sources = {
        "podcastindex_description": base.get("pi_description"),
        "spotify_description": (spotify.get("show", {}) if spotify else {}).get("description"),
        "apple_description": apple.get("description"),
        "youtube_about": (youtube.get("channel", {}) if youtube else {}).get("about"),
        "youtube_descriptions": [v.get("description") for v in (youtube.get("videos_sample") or [])] if youtube else [],
        "youtube_comments": youtube.get("comments_sample") if youtube else [],
        "social_bios": {
            "twitter": (social.get("twitter") or {}) if social else {},
            "instagram": (social.get("instagram") or {}) if social else {},
            "website_about": (social.get("website") or {}).get("about") if social else None,
        },
        "extracted_metadata": extracted or {},
    }

    def val(key: str) -> str:
        if not extracted:
            return ""
        v = extracted.get(key)
        return v if v else ""

    text_block = f"""NAME:
{apple.get('title') or spotify.get('title') or base.get('pi_title') or ''}

SUMMARY:
{val('summary')}

VERTICAL:
{val('vertical')}

AUDIENCE:
{val('audience_profile')}

FUNCTIONS:
{val('business_functions')}

USE_CASES:

KEYWORDS:
{val('industry_keywords')}

PAIN_POINTS:
{val('pain_points')}

GEO:
{val('geo_focus')}
"""
    cleaned = clean_text_block(text_block)
    # Cap length to avoid runaway blobs
    if len(cleaned) > 4000:
        cleaned = cleaned[:4000]
    return cleaned, raw_sources


def enrich_creators(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    from_podscan = bool(getattr(args, "from_podscan", False))

    have_spotify = bool(os.getenv("SPOTIFY_CLIENT_ID") and os.getenv("SPOTIFY_CLIENT_SECRET"))
    have_youtube = bool(os.getenv("YOUTUBE_API_KEY"))
    if not from_podscan:
        if not have_spotify:
            print("âŒ SPOTIFY_CLIENT_ID/SECRET not set; enrichment requires Spotify API.", file=sys.stderr)
            return 1
        if not have_youtube:
            print("âŒ YOUTUBE_API_KEY not set; enrichment requires YouTube API.", file=sys.stderr)
            return 1
    if from_podscan and args.sources:
        print("âš ï¸ --sources ignored when --from-podscan is set.")

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        ensure_podcastindex_tables(conn)
        ensure_creators_table(conn)
        ensure_creator_enrichment_table(conn)
        if from_podscan:
            ensure_podscan_table(conn)
        ensure_column(conn, "creator_enrichment", "source_social_twitter", "TEXT")
        ensure_column(conn, "creator_enrichment", "source_social_instagram", "TEXT")
        ensure_column(conn, "creator_enrichment", "source_social_website", "TEXT")
        ensure_column(conn, "creator_enrichment", "source_social_patreon", "TEXT")
        for col in [
            ("vertical", "TEXT"),
            ("audience_profile", "TEXT"),
            ("industry_keywords", "TEXT"),
            ("business_functions", "TEXT"),
            ("pain_points", "TEXT"),
            ("geo_focus", "TEXT"),
        ]:
            ensure_column(conn, "creators", col[0], col[1])

        requested_sources = None
        if args.sources and not from_podscan:
            requested_sources = {s.strip().lower() for s in args.sources.split(",") if s.strip()}
            print(f"ðŸ”Ž Enriching only requested sources: {', '.join(sorted(requested_sources))}")
            print("   Will prioritize rows with missing/error/not_found for those sources.")
        disabled_sources = set()
        auto_norm = bool(getattr(args, "auto_norm", False))
        norm_provider = "local"
        ollama_model = LOCAL_OLLAMA_MODEL
        categories_hint = ""
        if auto_norm:
            ensure_entity_normalized_table(conn)
            ensure_normalization_lexicon_table(conn)
            ensure_apple_categories_table(conn)
            cat_rows = conn.execute(
                "SELECT full_path FROM apple_podcast_categories LIMIT 30"
            ).fetchall()
            categories_hint = "\n".join([r["full_path"] for r in cat_rows]) if cat_rows else ""

        cursor = conn.cursor()
        pidrange = args.pidrange
        podscan_select = ", ps.full_json as podscan_json" if from_podscan else ""
        podscan_join = (
            "JOIN podscan ps ON ps.podcastindex_id = c.podcastindex_id "
            "AND ps.full_json IS NOT NULL AND TRIM(ps.full_json) <> ''"
            if from_podscan
            else ""
        )

        if args.only_id:
            cursor.execute(
                f"""
                SELECT c.podcastindex_id, c.itunesid, c.language,
                       p.url as feed_url, p.link as web_link, p.title as pi_title, p.description as pi_description,
                       p.category1, p.category2, p.category3, p.category4, p.category5,
                       p.category6, p.category7, p.category8, p.category9, p.category10,
                       p.podcastGuid,
                       ce.podcastindex_id as ce_pid,
                       ce.source_spotify, ce.source_apple, ce.source_youtube,
                       ce.source_social_twitter, ce.source_social_instagram, ce.source_social_website, ce.source_social_patreon
                       {podscan_select}
                FROM creators c
                LEFT JOIN podcastindex_feeds p ON c.podcastindex_id = p.id
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                {podscan_join}
                WHERE c.podcastindex_id = ?
                """,
                (args.only_id,),
            )
        else:
            range_clause = ""
            params = []
            if pidrange:
                range_clause = "WHERE c.podcastindex_id BETWEEN ? AND ?"
                params.extend([pidrange[0], pidrange[1]])
            query = f"""
                SELECT c.podcastindex_id, c.itunesid, c.language,
                       p.url as feed_url, p.link as web_link, p.title as pi_title, p.description as pi_description,
                       p.category1, p.category2, p.category3, p.category4, p.category5,
                       p.category6, p.category7, p.category8, p.category9, p.category10,
                       p.podcastGuid,
                       ce.podcastindex_id as ce_pid,
                       ce.source_spotify, ce.source_apple, ce.source_youtube,
                       ce.source_social_twitter, ce.source_social_instagram, ce.source_social_website, ce.source_social_patreon
                       {podscan_select}
                FROM creators c
                LEFT JOIN podcastindex_feeds p ON c.podcastindex_id = p.id
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                {podscan_join}
                {range_clause}
                ORDER BY
                    p.newestItemPubdate DESC NULLS LAST,
                    p.episodeCount DESC NULLS LAST,
                    COALESCE(p.updateFrequency, 999999) ASC
                LIMIT ? OFFSET ?
            """
            base_params = list(params)

        target_limit = args.limit if args.limit and args.limit > 0 else None
        batch_size = max(target_limit or 100, 100)

        def load_json(val):
            try:
                return json.loads(val) if val else {}
            except Exception:
                return {}

        def auto_normalize_creator(podcastindex_id: int, *, use_podscan: bool) -> bool:
            exists = conn.execute(
                "SELECT 1 FROM entity_normalized WHERE entity_type='creator' AND entity_id=? AND version='v1'",
                (podcastindex_id,),
            ).fetchone()
            if exists:
                return False
            name = f"podcast {podcastindex_id}"
            fields_parts: List[str] = []
            if use_podscan:
                row = conn.execute(
                    """
                    SELECT c.title, ps.full_json
                    FROM creators c
                    JOIN podscan ps ON ps.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id=?
                      AND ps.full_json IS NOT NULL
                      AND TRIM(ps.full_json) <> ''
                    """,
                    (podcastindex_id,),
                ).fetchone()
                if not row:
                    return False
                name = row["title"] or name
                try:
                    pod = json.loads(row["full_json"])
                except Exception:
                    print(f"âš ï¸ Invalid Podscan JSON for creator {podcastindex_id}; skipping normalization.")
                    return False
                desc = pod.get("podcast_description") or ""
                if pod.get("podcast_name"):
                    name = pod.get("podcast_name") or name
                if not desc:
                    return False
                fields_parts.append(f"DESCRIPTION: {desc}")
            else:
                row = conn.execute(
                    """
                    SELECT c.*, ce.cleaned_text_block, ce.raw_text_sources
                    FROM creators c
                    LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id=?
                    """,
                    (podcastindex_id,),
                ).fetchone()
                if not row:
                    return False
                name = row["title"] or name
                for key in ["title", "description", "categories", "cleaned_text_block"]:
                    if row[key]:
                        fields_parts.append(f"{key.upper()}: {row[key]}")
                try:
                    raw_sources = json.loads(row["raw_text_sources"]) if row["raw_text_sources"] else {}
                except Exception:
                    raw_sources = {}
                for k, v in (raw_sources or {}).items():
                    if v:
                        fields_parts.append(f"{k.upper()}: {v}")
            raw_blob = "\n".join(fields_parts)
            if not raw_blob:
                return False
            if len(raw_blob) > 12000:
                raw_blob = raw_blob[:12000]
            simplified = simplify_entity_text("creator", name, raw_blob, norm_provider, ollama_model) or raw_blob
            fields_text = f"SIMPLIFIED:\n{simplified}\n\nRAW:\n{raw_blob}"
            debug_tag = f"creator:{podcastindex_id}"
            norm = normalize_entity_to_json(
                "creator",
                name,
                fields_text,
                categories_hint,
                provider=norm_provider,
                model=ollama_model,
                debug_tag=debug_tag,
            )
            if not norm or not norm.get("summary"):
                return False
            for key in ["vertical", "audience", "themes", "use_cases", "pain_points", "tone", "geo"]:
                norm[key] = map_label_from_lexicon(conn, norm.get(key))
            topics_val = norm.get("topics")
            topics_str = map_topics_to_apple(conn, topics_val)
            block_parts = [
                f"SUMMARY:\n{norm.get('summary','')}",
                f"VERTICAL:\n{norm.get('vertical','')}",
                f"AUDIENCE:\n{norm.get('audience','')}",
                f"PAIN_POINTS:\n{norm.get('pain_points','')}",
                f"USE_CASES:\n{norm.get('use_cases','')}",
                f"THEMES:\n{norm.get('themes','')}",
                f"TONE:\n{norm.get('tone','')}",
                f"GEO:\n{norm.get('geo','')}",
                f"TOPICS:\n{topics_str}",
            ]
            normalized_block = "\n".join(block_parts)
            conn.execute(
                """
                INSERT OR REPLACE INTO entity_normalized (entity_type, entity_id, version, normalized_block, norm_fields)
                VALUES ('creator', ?, 'v1', ?, ?)
                """,
                (podcastindex_id, normalized_block, json.dumps(norm)),
            )
            def to_str(val):
                if val is None:
                    return None
                if isinstance(val, list):
                    return ", ".join(str(x) for x in val if x)
                return str(val)
            conn.execute(
                """
                UPDATE creators SET
                    normalized_summary=?, normalized_vertical=?, normalized_audience=?, normalized_pain_points=?,
                    normalized_use_cases=?, normalized_themes=?, normalized_tone=?, normalized_geo=?, normalized_topics=?
                WHERE podcastindex_id=?
                """,
                (
                    to_str(norm.get("summary")),
                    to_str(norm.get("vertical")),
                    to_str(norm.get("audience")),
                    to_str(norm.get("pain_points")),
                    to_str(norm.get("use_cases")),
                    to_str(norm.get("themes")),
                    to_str(norm.get("tone")),
                    to_str(norm.get("geo")),
                    to_str(topics_str),
                    podcastindex_id,
                ),
            )
            return True

        def needs_source_refresh(source_name: str, existing: Dict) -> bool:
            if requested_sources and source_name not in requested_sources:
                return False
            status = existing.get("status")
            if not existing or status in (None, "", "not_found", "error", "missing_credentials"):
                return True
            return False

        def select_candidates(raw_rows: List[sqlite3.Row]) -> List[sqlite3.Row]:
            rows = []
            for r in raw_rows:
                existing_spotify = load_json(r["source_spotify"])
                existing_apple = load_json(r["source_apple"])
                existing_youtube = load_json(r["source_youtube"])
                existing_social = {
                    "twitter": load_json(r["source_social_twitter"]),
                    "instagram": load_json(r["source_social_instagram"]),
                    "website": load_json(r["source_social_website"]),
                    "patreon": load_json(r["source_social_patreon"]),
                }
                ce_missing = (
                    r["source_spotify"] is None
                    and r["source_apple"] is None
                    and r["source_youtube"] is None
                    and r["source_social_twitter"] is None
                    and r["source_social_instagram"] is None
                    and r["source_social_website"] is None
                    and r["source_social_patreon"] is None
                )

                all_social_keys = {"twitter", "instagram", "website", "patreon"}
                if requested_sources:
                    if "social" in requested_sources:
                        requested_social_keys = all_social_keys
                    else:
                        requested_social_keys = {s for s in requested_sources if s in all_social_keys}
                else:
                    requested_social_keys = all_social_keys

                def social_needs_refresh() -> bool:
                    for key in requested_social_keys:
                        val = existing_social.get(key)
                        if not val:
                            return True
                        if isinstance(val, dict) and len(val.keys()) == 0:
                            return True
                    return False

                if from_podscan:
                    if r["ce_pid"] is None:
                        rows.append(r)
                    continue
                if not requested_sources:
                    refresh_needed = ce_missing or any(
                        needs_source_refresh(src_name, existing)
                        for src_name, existing in [
                            ("spotify", existing_spotify),
                            ("apple", existing_apple),
                            ("youtube", existing_youtube),
                        ]
                    )
                    if not refresh_needed:
                        refresh_needed = social_needs_refresh()
                    if refresh_needed:
                        rows.append(r)
                else:
                    refresh_needed = False
                    for src_name, existing in [
                        ("spotify", existing_spotify),
                        ("apple", existing_apple),
                        ("youtube", existing_youtube),
                    ]:
                        if src_name in requested_sources and needs_source_refresh(src_name, existing):
                            refresh_needed = True
                            break
                    if not refresh_needed and requested_social_keys:
                        refresh_needed = social_needs_refresh()
                    if refresh_needed or ce_missing:
                        rows.append(r)
            return rows

        if target_limit:
            print(
                "ðŸ” Will process up to {} creators (including missing/error/not_found for requested sources).".format(
                    target_limit
                )
            )
        else:
            print("ðŸ” Will process creators (including missing/error/not_found for requested sources).")

        enriched_rows = 0
        attempted_rows = 0
        had_candidates = False
        total_label = str(target_limit) if target_limit else "?"
        stats = {
            "spotify_ok": 0,
            "spotify_missing": 0,
            "spotify_not_found": 0,
            "youtube_ok": 0,
            "youtube_missing": 0,
            "youtube_not_found": 0,
            "apple_ok": 0,
            "apple_not_found": 0,
        }
        def process_candidates(candidate_rows: List[sqlite3.Row]) -> bool:
            nonlocal enriched_rows, attempted_rows
            for row in candidate_rows:
                if target_limit and enriched_rows >= target_limit:
                    return True
                podcastindex_id = row["podcastindex_id"]
                itunes_id = row["itunesid"]
                language = (row["language"] or "").lower()
                if language and "en" not in language:
                    continue
                attempted_rows += 1
                base = dict(row)
                # Use feed URL to guess spotify URL if it matches open.spotify.com/show/...
                spotify_url = base.get("feed_url") if base.get("feed_url", "").startswith("https://open.spotify.com/show/") else None

                title_for_lookup = base.get("pi_title") or ""

                existing_spotify = load_json(row["source_spotify"])
                existing_apple = load_json(row["source_apple"])
                existing_youtube = load_json(row["source_youtube"])
                existing_social = {
                    "twitter": load_json(row["source_social_twitter"]),
                    "instagram": load_json(row["source_social_instagram"]),
                    "website": load_json(row["source_social_website"]),
                    "patreon": load_json(row["source_social_patreon"]),
                }
                if from_podscan:
                    podscan_json = row["podscan_json"]
                    if not podscan_json:
                        continue
                    try:
                        pod = json.loads(podscan_json)
                    except Exception:
                        print(f"âš ï¸ Invalid Podscan JSON for creator {podcastindex_id}; skipping.")
                        continue
                    pod_name = (
                        pod.get("podcast_name")
                        or pod.get("podcast_title")
                        or pod.get("title")
                        or pod.get("name")
                        or ""
                    )
                    pod_desc = (
                        pod.get("podcast_description")
                        or pod.get("description")
                        or pod.get("summary")
                        or ""
                    )
                    combined_raw = "\n\n".join([v for v in [pod_name, pod_desc] if v]).strip()
                    if not combined_raw:
                        continue
                    base["pi_title"] = pod_name or base.get("pi_title") or ""
                    base["pi_description"] = pod_desc or base.get("pi_description") or ""
                    cleaned_text, raw_sources = build_text_block(
                        base=base,
                        apple={},
                        spotify={},
                        youtube={},
                        social={},
                        extracted=extract_creator_metadata(combined_raw),
                    )
                    raw_sources["podscan_name"] = pod_name
                    raw_sources["podscan_description"] = pod_desc
                    uk_score = compute_uk_score(base, {}, {}, {}, {})
                    conn.execute(
                        """
                        INSERT OR REPLACE INTO creator_enrichment (
                            podcastindex_id, uk_score, source_spotify, source_apple, source_youtube,
                            source_social_twitter, source_social_instagram, source_social_website, source_social_patreon,
                            cleaned_text_block, raw_text_sources, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                        """,
                        (
                            podcastindex_id,
                            uk_score,
                            None,
                            None,
                            None,
                            None,
                            None,
                            None,
                            None,
                            cleaned_text,
                            json.dumps(raw_sources),
                        ),
                    )
                    extracted = raw_sources.get("extracted_metadata") or {}
                    if extracted:
                        conn.execute(
                            """
                            UPDATE creators
                            SET vertical = ?, audience_profile = ?, industry_keywords = ?, business_functions = ?, pain_points = ?, geo_focus = ?
                            WHERE podcastindex_id = ?
                            """,
                            (
                                extracted.get("vertical"),
                                extracted.get("audience_profile"),
                                extracted.get("industry_keywords"),
                                extracted.get("business_functions"),
                                extracted.get("pain_points"),
                                extracted.get("geo_focus"),
                                podcastindex_id,
                            ),
                        )
                    if auto_norm:
                        auto_normalize_creator(podcastindex_id, use_podscan=True)
                    enriched_rows += 1
                    if enriched_rows % 10 == 0:
                        conn.commit()
                        print(
                            f"Enriched {enriched_rows}/{total_label} (last podcastindex_id={podcastindex_id})"
                        )
                    continue

                fetch_spotify_flag = ((not requested_sources) or ("spotify" in requested_sources)) and ("spotify" not in disabled_sources)
                fetch_apple_flag = ((not requested_sources) or ("apple" in requested_sources)) and ("apple" not in disabled_sources)
                fetch_youtube_flag = ((not requested_sources) or ("youtube" in requested_sources)) and ("youtube" not in disabled_sources)
                social_requested = (not requested_sources) or ("social" in requested_sources) or any(
                    s in requested_sources for s in ["twitter", "instagram", "website", "patreon"]
                )
                fetch_social_flag = social_requested and ("social" not in disabled_sources)

                spotify_fetched = fetch_spotify_flag
                youtube_fetched = fetch_youtube_flag
                if attempted_rows % 5 == 1 or attempted_rows == 1:
                    print(f"[{attempted_rows}/{total_label}] pid={podcastindex_id} fetching sourcesâ€¦", end="\r")
                spotify_meta = existing_spotify if not fetch_spotify_flag else fetch_spotify_metadata(spotify_url, title_for_lookup)
                apple_meta = existing_apple if not fetch_apple_flag else fetch_apple_metadata(itunes_id, title_for_lookup)
                youtube_meta = existing_youtube if not fetch_youtube_flag else fetch_youtube_metadata(title_for_lookup, mode="lean")
                social_meta = existing_social if not fetch_social_flag else fetch_social_bios_from_sources(base, apple_meta, spotify_meta, youtube_meta)

                # Stop on 403 errors for any source
                def is_403(meta: Dict) -> bool:
                    if not isinstance(meta, dict):
                        return False
                    if meta.get("http_status") == 403:
                        return True
                    detail = meta.get("detail")
                    if isinstance(detail, str) and "quotaExceeded" in detail:
                        return True
                    if isinstance(detail, dict) and meta.get("detail", {}).get("error", {}).get("errors"):
                        # Inspect nested error reasons for quotaExceeded
                        for err in meta["detail"]["error"]["errors"]:
                            if err.get("reason") == "quotaExceeded":
                                return True
                    return False

                if spotify_fetched and is_403(spotify_meta):
                    if requested_sources and "spotify" in requested_sources:
                        print(f"STOP: Spotify returned 403 for podcastindex_id={podcastindex_id}. Detail: {spotify_meta.get('detail')}")
                        print("\033[31mEnrichment halted due to Spotify 403 (likely quota or key restriction).\033[0m")
                        conn.rollback()
                        return 1
                    else:
                        print(f"âš ï¸ Spotify 403 encountered for podcastindex_id={podcastindex_id}. Skipping further Spotify calls this run.")
                        print("\033[31mSkipping Spotify due to 403 (likely quota or key restriction).\033[0m")
                        disabled_sources.add("spotify")
                        spotify_meta = {"status": "error", "http_status": 403, "detail": spotify_meta.get("detail")}
                if youtube_fetched and is_403(youtube_meta):
                    if requested_sources and "youtube" in requested_sources:
                        print(f"STOP: YouTube returned 403 for podcastindex_id={podcastindex_id}. Detail: {youtube_meta.get('detail')}")
                        print("\033[31mEnrichment halted due to YouTube 403 (likely quota or API key restriction).\033[0m")
                        conn.rollback()
                        return 1
                    else:
                        print(f"âš ï¸ YouTube 403 encountered for podcastindex_id={podcastindex_id}. Skipping further YouTube calls this run.")
                        print("\033[31mSkipping YouTube due to 403 (likely quota or key restriction).\033[0m")
                        disabled_sources.add("youtube")
                        youtube_meta = {"status": "error", "http_status": 403, "detail": youtube_meta.get("detail")}

                # Stats
                apple_status = apple_meta.get("status") or ("ok" if apple_meta else "not_found")
                spotify_status = spotify_meta.get("status") or ("ok" if spotify_meta else "not_found")
                youtube_status = youtube_meta.get("status") or ("ok" if youtube_meta else "not_found")
                if apple_status == "ok":
                    stats["apple_ok"] += 1
                else:
                    stats["apple_not_found"] += 1
                if spotify_status == "ok":
                    stats["spotify_ok"] += 1
                elif spotify_status == "missing_credentials":
                    stats["spotify_missing"] += 1
                else:
                    stats["spotify_not_found"] += 1
                if youtube_status == "ok":
                    stats["youtube_ok"] += 1
                elif youtube_status == "missing_credentials":
                    stats["youtube_missing"] += 1
                else:
                    stats["youtube_not_found"] += 1

                # Build combined raw text for metadata extraction
                raw_pieces = []
                for val in [
                    base.get("pi_description"),
                    apple_meta.get("description"),
                    (spotify_meta.get("show", {}) if spotify_meta else {}).get("description"),
                    (youtube_meta.get("channel", {}) if youtube_meta else {}).get("about"),
                    (social_meta.get("website") or {}).get("about") if social_meta else None,
                ]:
                    if val:
                        raw_pieces.append(val)
                # Episode titles for extra signal
                if spotify_meta.get("episodes_sample"):
                    ep_titles = [ep.get("name") or "" for ep in spotify_meta["episodes_sample"] if ep]
                    raw_pieces.append("; ".join(ep_titles[:30]))
                combined_raw = "\n\n".join(raw_pieces) if raw_pieces else ""

                if attempted_rows % 5 == 1 or attempted_rows == 1:
                    print(f"[{attempted_rows}/{total_label}] pid={podcastindex_id} running GPT extractâ€¦", end="\r")
                cleaned_text, raw_sources = build_text_block(
                    base=base,
                    apple=apple_meta,
                    spotify=spotify_meta,
                    youtube=youtube_meta,
                    social=social_meta,
                    extracted=extract_creator_metadata(combined_raw) if combined_raw else {},
                )

                uk_score = compute_uk_score(base, apple_meta, spotify_meta, youtube_meta, social_meta)

                conn.execute(
                    """
                    INSERT OR REPLACE INTO creator_enrichment (
                        podcastindex_id, uk_score, source_spotify, source_apple, source_youtube,
                        source_social_twitter, source_social_instagram, source_social_website, source_social_patreon,
                        cleaned_text_block, raw_text_sources, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                    """,
                    (
                        podcastindex_id,
                        uk_score,
                        json.dumps(spotify_meta),
                        json.dumps(apple_meta),
                        json.dumps(youtube_meta),
                        json.dumps(social_meta.get("twitter")) if social_meta.get("twitter") else None,
                        json.dumps(social_meta.get("instagram")) if social_meta.get("instagram") else None,
                        json.dumps(social_meta.get("website")) if social_meta.get("website") else None,
                        json.dumps(social_meta.get("patreon")) if social_meta.get("patreon") else None,
                        cleaned_text,
                        json.dumps(raw_sources),
                    ),
                )
                # Persist structured creator metadata onto creators table
                extracted = raw_sources.get("extracted_metadata") or {}
                if extracted:
                    conn.execute(
                        """
                        UPDATE creators
                        SET vertical = ?, audience_profile = ?, industry_keywords = ?, business_functions = ?, pain_points = ?, geo_focus = ?
                        WHERE podcastindex_id = ?
                        """,
                        (
                            extracted.get("vertical"),
                            extracted.get("audience_profile"),
                            extracted.get("industry_keywords"),
                            extracted.get("business_functions"),
                            extracted.get("pain_points"),
                            extracted.get("geo_focus"),
                            podcastindex_id,
                        ),
                    )
                if auto_norm:
                    auto_normalize_creator(podcastindex_id, use_podscan=False)
                enriched_rows += 1
                if enriched_rows % 10 == 0:
                    conn.commit()
                    print(
                        f"Enriched {enriched_rows}/{total_label} (last podcastindex_id={podcastindex_id})"
                    )
            return False

        if args.only_id:
            raw_rows = cursor.fetchall()
            if not raw_rows:
                print("No creators to enrich.")
                return 0
            rows = select_candidates(raw_rows)
            if not rows:
                print("No creators to enrich for the requested sources.")
                return 0
            had_candidates = True
            if process_candidates(rows):
                pass
        else:
            current_offset = args.offset
            while True:
                batch_params = base_params + [batch_size, current_offset]
                cursor.execute(query, batch_params)
                raw_rows = cursor.fetchall()
                if not raw_rows:
                    break
                rows = select_candidates(raw_rows)
                if rows:
                    had_candidates = True
                if process_candidates(rows):
                    break
                current_offset += batch_size

        if not had_candidates:
            print("No creators to enrich for the requested sources.")
            return 0

        conn.commit()
        if from_podscan:
            print(f"âœ… Enriched {enriched_rows} creators from Podscan.")
        else:
            print(
                f"âœ… Enriched {enriched_rows} creators. "
                f"Apple ok: {stats['apple_ok']}, not_found/missing: {stats['apple_not_found']}; "
                f"Spotify ok: {stats['spotify_ok']}, missing creds: {stats['spotify_missing']}, other: {stats['spotify_not_found']}; "
                f"YouTube ok: {stats['youtube_ok']}, missing creds: {stats['youtube_missing']}, other: {stats['youtube_not_found']}"
            )
        return 0
    finally:
        conn.close()


def clean_text_block(text: str) -> str:
    """
    Apply basic cleaning rules:
    - strip URLs
    - remove hashtags/handles
    - remove boilerplate like 'like and subscribe'
    - remove timestamps like [00:00], 00:00:
    - collapse repeated whitespace
    """
    if not text:
        return ""
    # Remove URLs
    text = re.sub(r"https?://\\S+|www\\.\\S+", " ", text)
    # Remove hashtags/handles
    text = re.sub(r"[@#][A-Za-z0-9_]+", " ", text)
    # Remove timestamps like [00:00] or 00:00:
    text = re.sub(r"\\[?\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\]?\\:?", " ", text)
    # Remove common boilerplate phrases
    boilerplate_patterns = [
        r"like and subscribe",
        r"donâ€™t forget to subscribe",
        r"don't forget to subscribe",
        r"check out our sponsor",
        r"leave a comment",
        r"support us on patreon",
    ]
    for pat in boilerplate_patterns:
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)
    # Collapse repeated text: naive approach by splitting sentences and deduping consecutive repeats
    parts = text.split(".")
    deduped_parts = []
    prev = None
    for part in parts:
        chunk = part.strip()
        if not chunk:
            continue
        if chunk == prev:
            continue
        deduped_parts.append(chunk)
        prev = chunk
    text = ". ".join(deduped_parts)
    # Normalize whitespace
    text = re.sub(r"\\s+", " ", text).strip()
    return text


UK_KEYWORDS = [
    "united kingdom",
    "uk",
    "british",
    "england",
    "scotland",
    "wales",
    "northern ireland",
    "london",
    "manchester",
    "birmingham",
    "leeds",
    "glasgow",
    "edinburgh",
    "bristol",
    "liverpool",
    "cardiff",
    "belfast",
]


def compute_uk_score(base: Dict, apple: Dict, spotify: Dict, youtube: Dict, social: Dict) -> float:
    score = 0.0
    haystack = " ".join(
        [
            str(base.get("pi_description") or ""),
            str(base.get("pi_title") or ""),
            str(apple.get("description") or ""),
            str((spotify.get("show", {}) if spotify else {}).get("description") or ""),
            str(youtube.get("channel", {}).get("about", "") if youtube else ""),
            str(social.get("website", {}).get("about", "") if social else ""),
        ]
    ).lower()

    # PodcastIndex location not present in schema, so skip; use text heuristics.
    if any(word in haystack for word in ("uk podcast", "british", "london", "uk-based", "britain")):
        score += 0.4

    # Cities / keywords
    for kw in UK_KEYWORDS:
        if kw in haystack:
            score += 0.1
            break

    # Domain heuristic
    feed_url = base.get("feed_url") or ""
    if ".co.uk" in feed_url or feed_url.endswith(".uk"):
        score += 0.2

    # Cap 0..1
    score = max(0.0, min(1.0, score))
    return score


def clean_bio_text(text: str, max_len: int = 500) -> Optional[str]:
    if not text:
        return None
    # Remove URLs, hashtags, handles
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)
    text = re.sub(r"[@#][A-Za-z0-9_]+", " ", text)
    # Remove boilerplate phrases
    boiler = [
        "follow me",
        "link in bio",
        "business enquiries",
        "business inquiries",
        "subscribe",
        "shop",
        "http",
        "https",
    ]
    for b in boiler:
        text = re.sub(b, " ", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) < 10:
        return None
    if len(text) > max_len:
        text = text[:max_len]
    return text


def extract_social_candidates(base: Dict, apple: Dict, spotify: Dict, youtube: Dict) -> Dict[str, List[str]]:
    texts = [
        base.get("pi_description") or "",
        apple.get("description") or "",
        spotify.get("show", {}).get("description") or "",
        (youtube.get("channel", {}) if youtube else {}).get("about") or "",
    ]
    urls = []
    for t in texts:
        urls.extend(re.findall(r"https?://\S+", t))
    # Base links
    if base.get("feed_url"):
        # avoid feed URLs for social scraping
        if not any(tag in base["feed_url"].lower() for tag in ["feedburner", ".xml", "rss"]):
            urls.append(base["feed_url"])
    if base.get("web_link"):
        urls.append(base["web_link"])
    # Apple URLs
    if apple.get("artistUrl"):
        urls.append(apple["artistUrl"])
    if apple.get("collectionUrl"):
        urls.append(apple["collectionUrl"])
    if apple.get("feedUrl"):
        urls.append(apple["feedUrl"])

    candidates = {"twitter": [], "instagram": [], "website": [], "patreon": []}
    for u in urls:
        lu = u.lower()
        if "twitter.com" in lu or "x.com" in lu:
            candidates["twitter"].append(u)
        elif "instagram.com" in lu:
            candidates["instagram"].append(u)
        elif "patreon.com" in lu:
            candidates["patreon"].append(u)
        elif "linktr.ee" in lu or "linktree" in lu:
            candidates["website"].append(u)
        else:
            candidates["website"].append(u)
    return candidates


def fetch_twitter_bio(url: str) -> Optional[Dict]:
    backoff = 2
    for _ in range(3):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                time.sleep(backoff)
                backoff *= 2
                continue
            parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
            soup = BeautifulSoup(res.text, parser)
            meta = soup.find("meta", attrs={"name": "description"})
            if meta and meta.get("content"):
                bio = clean_bio_text(meta["content"])
                if bio:
                    handle = url.split("/")[-1]
                    return {"handle": f"@{handle}", "bio": bio}
            return None
        except Exception:
            time.sleep(backoff)
            backoff *= 2
            continue
    return None


def fetch_instagram_bio(url: str) -> Optional[Dict]:
    backoff = 2
    for _ in range(3):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                time.sleep(backoff)
                backoff *= 2
                continue
            parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
            soup = BeautifulSoup(res.text, parser)
            meta = soup.find("meta", property="og:description")
            if meta and meta.get("content"):
                desc = meta["content"]
                desc = desc.split("Followers")[0]
                bio = clean_bio_text(desc)
                if bio:
                    handle = url.rstrip("/").split("/")[-1]
                    return {"handle": f"@{handle}", "bio": bio}
            return None
        except Exception:
            time.sleep(backoff)
            backoff *= 2
            continue
    return None


def fetch_website_about(url: str) -> Optional[Dict]:
    try:
        targets = [url]
        if url.endswith("/"):
            base = url.rstrip("/")
        else:
            base = url
        targets.append(base + "/about")
        targets.append(base + "/about-us")
        for t in targets:
            try:
                res = requests.get(t, timeout=10)
                if res.status_code != 200:
                    continue
                parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
                soup = BeautifulSoup(res.text, parser)
                paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
                text = " ".join(paragraphs)
                bio = clean_bio_text(text, max_len=500)
                if bio:
                    return {"url": t, "about": bio}
            except Exception:
                continue
    except Exception:
        return None
    return None


def fetch_social_bios_from_sources(base: Dict, apple: Dict, spotify: Dict, youtube: Dict) -> Dict:
    cands = extract_social_candidates(base, apple, spotify, youtube)
    result = {}
    if cands["twitter"]:
        tw = fetch_twitter_bio(cands["twitter"][0])
        if tw:
            result["twitter"] = tw
    if cands["instagram"]:
        ig = fetch_instagram_bio(cands["instagram"][0])
        if ig:
            result["instagram"] = ig
    if cands["patreon"]:
        pt = fetch_website_about(cands["patreon"][0])
        if pt:
            result["patreon"] = pt
    if cands["website"]:
        ws = fetch_website_about(cands["website"][0])
        if ws:
            result["website"] = ws
    return result


def scrape_website_text(url: str) -> Tuple[str, List[str]]:
    try:
        res = requests.get(url, timeout=15)
        res.raise_for_status()
        parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
        try:
            soup = BeautifulSoup(res.text, parser)
        except Exception as exc:
            print(f"âš ï¸ Parser issue for {url} (parser={parser}): {exc}", file=sys.stderr)
            # Fallback to html parser if xml failed
            soup = BeautifulSoup(res.text, "html.parser")
        for tag in soup(["script", "style", "nav", "footer"]):
            tag.decompose()
        text = soup.get_text(" ", strip=True)
        links = [a.get("href") for a in soup.find_all("a") if a.get("href")]
        # Limit size
        if len(text) > 10000:
            text = text[:10000]
        return text, links
    except Exception as exc:
        print(f"âš ï¸ Website scrape failed for {url}: {exc}", file=sys.stderr)
        return "", []


def scrape_linkedin_about(url: str) -> str:
    try:
        res = requests.get(url, timeout=15)
        if res.status_code != 200:
            return ""
        parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
        soup = BeautifulSoup(res.text, parser)
        meta = soup.find("meta", attrs={"name": "description"})
        if meta and meta.get("content"):
            return meta["content"][:2000]
        return soup.get_text(" ", strip=True)[:2000]
    except Exception:
        return ""


def find_social_links(links: List[str]) -> Dict[str, List[str]]:
    cands = {"twitter": [], "instagram": [], "linkedin": [], "website": []}
    for link in links:
        if not link:
            continue
        l = link.lower()
        if l.startswith("//"):
            link = "https:" + link
        if "linkedin.com" in l:
            cands["linkedin"].append(link)
        elif "twitter.com" in l or "x.com" in l:
            cands["twitter"].append(link)
        elif "instagram.com" in l:
            cands["instagram"].append(link)
        elif link.startswith("http"):
            cands["website"].append(link)
    return cands


def get_openai_client() -> Optional[OpenAI]:
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        print("âŒ OPENAI_API_KEY not set; cannot run brand extraction.", file=sys.stderr)
        return None
    return OpenAI(api_key=key)


def extract_brand_fields(website_text: str, linkedin_text: str) -> Dict:
    client = get_openai_client()
    if not client:
        return {}
    material = f"{website_text}\n\n{linkedin_text}"
    if len(material) > 8000:
        material = material[:8000]
    prompt = (
        "Extract structured metadata for a brand. Write SHORT, HIGH-SIGNAL content only. Output JSON with keys:\n"
        "- brand_vertical\n"
        "- brand_audience_profile\n"
        "- brand_use_cases\n"
        "- brand_pain_points\n"
        "- brand_geo_focus\n"
        "- summary (2â€“4 sentences on what the product does and for whom)\n"
        "- keywords (comma-separated business concepts)\n"
        "- flag_inconsistent_positioning (true/false if positioning seems contradictory)\n"
        "- candidate_verticals (array)\n"
        "- recommended_vertical\n"
        "- recommended_confidence (0..1)\n"
        "Identify if website text appears MRP/manufacturing vs ecommerce-op. If signals conflict, set flag_inconsistent_positioning=true.\n\n"
        "Input:\n"
        f"{material}"
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content
        data = json.loads(content)
        return data
    except Exception as exc:
        # Try to surface any returned text for debugging
        raw = ""
        try:
            raw = resp.choices[0].message.content  # type: ignore
        except Exception:
            raw = ""
        print(f"âš ï¸ GPT extraction failed: {exc} raw={raw}", file=sys.stderr)
        return {}


def extract_creator_metadata(raw_text: str) -> Dict:
    """
    Use GPT to extract structured creator metadata for embeddings.
    """
    provider = resolve_llm_provider(LLM_PROVIDER_META_ENRICH, "openai")
    if len(raw_text) > 15000:
        raw_text = raw_text[:15000]
    prompt = (
        "Extract structured metadata for a podcast.\n"
        "Write SHORT, HIGH-SIGNAL content only. No fluff.\n"
        "Output JSON with keys: vertical, audience_profile, industry_keywords, business_functions, pain_points, geo_focus, summary.\n"
        "- vertical: main industry verticals (e.g., ecommerce, retail, supply chain)\n"
        "- audience_profile: who listens (roles, industries, company stage)\n"
        "- industry_keywords: comma-separated topical keywords\n"
        "- business_functions: ops / marketing / tech / leadership etc.\n"
        "- pain_points: problems listeners want to solve\n"
        "- geo_focus: geography if clear (UK, US, global)\n"
        "- summary: 2-4 sentence distilled summary of the show identity\n\n"
        "Input text:\n"
        f"{raw_text}"
    )
    if provider == "openai":
        if not ensure_openai_confirmation("creator metadata extraction", allow_fallback=False):
            return {}
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            return {}
        client = OpenAI(api_key=key)
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                response_format={"type": "json_object"},
            )
            return json.loads(resp.choices[0].message.content)
        except Exception as exc:
            print(f"âš ï¸ Creator metadata extraction failed: {exc}", file=sys.stderr)
            return {}
    text = ollama_generate(prompt, LOCAL_OLLAMA_MODEL)
    if not text:
        return {}
    try:
        return json.loads(text)
    except Exception as exc:
        print(f"âš ï¸ Creator metadata extraction failed (local): {exc}", file=sys.stderr)
        return {}


# ---- Normalization helpers ----
NORMAL_VECTOR_TYPES = [
    "semantic_summary",
    "topics",
    "vertical",
    "audience",
    "pain_points",
    "use_cases",
    "themes",
    "tone",
    "geo",
]


def normalize_entity_to_json(
    entity_type: str,
    name: str,
    fields_text: str,
    categories_hint: str = "",
    provider: str = "local",
    model: str = LOCAL_OLLAMA_MODEL,
    debug_tag: str = "",
) -> Optional[Dict]:
    schema = (
        "{"
        "\"summary\":\"...\","  # summary of what they are/do
        "\"vertical\":\"...\","  # concise industry label
        "\"audience\":\"...\","  # who this is for
        "\"pain_points\":\"...\","  # problems solved/talked about
        "\"use_cases\":\"...\","  # how used / scenarios
        "\"themes\":\"...\","  # key themes/topics
        "\"tone\":\"...\","  # tone descriptors
        "\"geo\":\"...\","  # geography focus
        "\"topics\":[{\"apple_category_name\":\"...\",\"apple_subcategory_name\":\"...\",\"apple_category_id\":\"...\",\"apple_subcategory_id\":\"...\"}]"
        "}"
    )
    prompt = (
        "You are a classification and summarisation assistant.\n"
        "Normalize noisy marketing/technical text about an entity into a simple schema, using plain language a non-expert would understand.\n"
        "Return JSON ONLY with keys: summary, vertical, audience, pain_points, use_cases, themes, tone, geo, topics (array as specified).\n"
        "Be concise, high-signal, no fluff. Use common industry labels.\n"
        "Output MUST be valid JSON (no trailing commas, close all brackets/quotes). Keep lists short (max 3 items each).\n"
        "If multiple positions appear, prefer the CURRENT go-to-market positioning.\n"
        "Do not hallucinate missing facts; leave fields short and generic instead.\n\n"
        f"ENTITY_TYPE: {entity_type}\n"
        f"NAME: {name}\n"
        f"RAW FIELDS:\n{fields_text}\n\n"
        f"Apple Podcast categories (examples):\n{categories_hint}\n\n"
        "Output JSON in this structure:\n"
        + schema
    )
    if provider == "openai":
        if not ensure_openai_confirmation("normalization", allow_fallback=False):
            return None
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            print("âŒ OPENAI_API_KEY not set.", file=sys.stderr)
            return None
        client = OpenAI(api_key=key)
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"},
            )
            return json.loads(resp.choices[0].message.content)
        except Exception as exc:
            print(f"âš ï¸ Normalization failed: {exc}", file=sys.stderr)
            return None
    # local via Ollama
    text = ollama_generate(prompt, model)
    if not text:
        return None
    # Clean code fences / extra text
    cleaned = text.strip()
    for fence in ("```json", "```", "`"):
        cleaned = cleaned.replace(fence, "")

    def try_parse(val: str) -> Optional[Dict]:
        try:
            return json.loads(val)
        except Exception:
            return None

    def balance(text: str) -> str:
        open_curly = text.count("{")
        close_curly = text.count("}")
        open_brack = text.count("[")
        close_brack = text.count("]")
        fix = text
        fix += "}" * max(0, open_curly - close_curly)
        fix += "]" * max(0, open_brack - close_brack)
        return fix

    def clean_trailing(val: str) -> str:
        # Replace trailing colon with empty array to close fields
        val = re.sub(r":\s*$", ": []", val.strip())
        # Drop trailing commas before closing braces/brackets
        val = re.sub(r",\s*([}\]])", r"\1", val)
        return val

    def truncate_to_last_block(val: str) -> str:
        # Cut off trailing incomplete field by trimming after last closing brace/bracket
        last_brace = val.rfind("}")
        last_brack = val.rfind("]")
        end = max(last_brace, last_brack)
        if end != -1:
            return val[: end + 1]
        return val

    parsed = try_parse(cleaned)
    if parsed is None:
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = cleaned[start : end + 1]
            parsed = try_parse(candidate)
    if parsed is None:
        fixed = truncate_to_last_block(cleaned)
        fixed = clean_trailing(fixed)
        fixed = balance(fixed)
        parsed = try_parse(fixed)
    if parsed is None:
        # Iteratively drop the last line and retry a few times to remove truncated tails
        tmp = cleaned
        for _ in range(3):
            if "\n" in tmp:
                tmp = "\n".join(tmp.split("\n")[:-1])
            else:
                tmp = tmp[:-1]
            fixed = truncate_to_last_block(tmp)
            fixed = clean_trailing(fixed)
            fixed = balance(fixed)
            parsed = try_parse(fixed)
            if parsed is not None:
                break
    if parsed is None:
        excerpt = cleaned[:400]
        print(f"âš ï¸ Normalization ({provider}) parse failed for {debug_tag or 'entity'}; excerpt:\n{excerpt}\n", file=sys.stderr)
        try:
            out_dir = Path("tests")
            out_dir.mkdir(parents=True, exist_ok=True)
            with (out_dir / "normalize_failures.log").open("a") as f:
                f.write(f"{datetime.utcnow().isoformat()} | {debug_tag or entity_type} | provider={provider} | model={model}\n")
                f.write(excerpt + "\n\n")
        except Exception:
            pass
    return parsed


def load_static(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_apple_categories_table(conn)
    if getattr(args, "apple_topics", False):
        inserted = 0
        for topic in APPLE_TOPICS:
            full_path = topic["name"]
            if topic["parent"]:
                parent_name = next((t["name"] for t in APPLE_TOPICS if t["apple_id"] == topic["parent"]), "")
                full_path = f"{parent_name} > {topic['name']}" if parent_name else topic["name"]
            conn.execute(
                """
                INSERT OR REPLACE INTO apple_podcast_categories (apple_id, parent_apple_id, name, full_path)
                VALUES (?, ?, ?, ?)
                """,
                (topic["apple_id"], topic["parent"], topic["name"], full_path),
            )
            inserted += 1
        conn.commit()
        print(f"âœ… Loaded {inserted} Apple podcast categories.")
    else:
        print("Nothing to load.")
    conn.close()
    return 0


def assign_creator_topics(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_creators_table(conn)
    ensure_podcastindex_tables(conn)
    ensure_apple_categories_table(conn)
    ensure_creator_topics_table(conn)
    cat_rows = conn.execute("SELECT apple_id, parent_apple_id, name, full_path FROM apple_podcast_categories").fetchall()
    if not cat_rows:
        print("âŒ apple_podcast_categories is empty. Run: python3 adl static load --apple-topics", file=sys.stderr)
        conn.close()
        return 1
    cat_names = [r["full_path"] for r in cat_rows]
    cat_map = {r["name"].lower(): r["apple_id"] for r in cat_rows}
    for r in cat_rows:
        cat_map[r["full_path"].lower()] = r["apple_id"]

    if not ensure_ollama_running():
        conn.close()
        return 1
    model = getattr(args, "ollama_model", LOCAL_OLLAMA_MODEL)

    creators = []
    total_candidates = 0
    total_existing = conn.execute("SELECT COUNT(*) FROM creator_topics").fetchone()[0]
    if getattr(args, "nolimit", False):
        resp = input("Type YES to process ALL creators without limit: ").strip()
        if resp != "YES":
            print("Aborted.")
            conn.close()
            return 1
        total_candidates = conn.execute(
            """
            SELECT COUNT(*) FROM creators c
            WHERE NOT EXISTS (SELECT 1 FROM creator_topics ct WHERE ct.podcastindex_id = c.podcastindex_id)
            """
        ).fetchone()[0]
        creators = conn.execute(
            """
            SELECT c.podcastindex_id, c.title, c.description,
                   pf.title as feed_title, pf.description as feed_description,
                   pf.category1, pf.category2, pf.category3, pf.category4, pf.category5
            FROM creators c
            LEFT JOIN podcastindex_feeds pf ON pf.id = c.podcastindex_id
            WHERE NOT EXISTS (
                SELECT 1 FROM creator_topics ct WHERE ct.podcastindex_id = c.podcastindex_id
            )
            """
        ).fetchall()
    else:
        total_candidates = conn.execute(
            """
            SELECT COUNT(*) FROM creators c
            WHERE NOT EXISTS (SELECT 1 FROM creator_topics ct WHERE ct.podcastindex_id = c.podcastindex_id)
            """
        ).fetchone()[0]
        creators = conn.execute(
            """
            SELECT c.podcastindex_id, c.title, c.description,
                   pf.title as feed_title, pf.description as feed_description,
                   pf.category1, pf.category2, pf.category3, pf.category4, pf.category5
            FROM creators c
            LEFT JOIN podcastindex_feeds pf ON pf.id = c.podcastindex_id
            WHERE NOT EXISTS (
                SELECT 1 FROM creator_topics ct WHERE ct.podcastindex_id = c.podcastindex_id
            )
            LIMIT ? OFFSET ?
            """,
            (args.limit, args.offset),
        ).fetchall()
    if not creators:
        print(f"No creators found for topic assignment. Existing mappings: {total_existing}; remaining: {total_candidates}")
        conn.close()
        return 0

    def llm_topics(payload: str) -> List[str]:
        prompt = (
            "You are a classifier. Based on the podcast TITLE, DESCRIPTION, and CATEGORIES below, choose the 1 to 3 most relevant Apple Podcast categories from the list provided.\n"
            "Return ONLY a JSON array of strings with valid Apple category names. Do not invent new categories. No explanations.\n\n"
            "PODCAST DATA:\n"
            f"{payload}\n\n"
            "APPLE CATEGORIES:\n" + "\n".join(cat_names) + "\n\n"
            "Respond JSON only."
        )
        try:
            res = requests.post(
                f"{OLLAMA_HOST}/api/generate",
                json={"model": model, "prompt": prompt, "format": "json", "options": {"num_predict": 200, "temperature": 0.3}},
                timeout=120,
            )
            res.raise_for_status()
            # accumulate streamed response
            chunks = []
            for line in res.iter_lines():
                if not line:
                    continue
                try:
                    obj = json.loads(line.decode("utf-8"))
                    if "response" in obj:
                        chunks.append(obj["response"])
                except Exception:
                    continue
            text = "".join(chunks).strip()
            for fence in ("```json", "```", "`"):
                text = text.replace(fence, "")
            try:
                data = json.loads(text)
                return data.get("categories") or []
            except Exception:
                # try brace substring
                start = text.find("{")
                end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try:
                        data = json.loads(text[start : end + 1])
                        return data.get("categories") or []
                    except Exception:
                        return []
            return []
        except Exception as exc:
            print(f"âš ï¸ Topic LLM failed (ollama): {exc}", file=sys.stderr)
            return []

    inserted = 0
    total = len(creators)
    if total:
        print(f"Assigning topics for {total} creators (eligible remaining: {total_candidates}; existing: {total_existing})...")
    for idx, c in enumerate(creators, 1):
        payload_parts = [
            f"Title: {c['title'] or ''}",
            f"Description: {c['description'] or ''}",
            "Categories: " + ", ".join([c[f"category{i}"] or "" for i in range(1, 6)]),
        ]
        topics = llm_topics("\n".join(payload_parts))
        mapped_ids = []
        for t in topics:
            key = str(t).lower().strip()
            if key in cat_map:
                mapped_ids.append(cat_map[key])
        if not mapped_ids and topics:
            # try partial matches
            for t in topics:
                key = str(t).lower().strip()
                for name, apple_id in cat_map.items():
                    if key in name:
                        mapped_ids.append(apple_id)
                        break
        mapped_ids = mapped_ids[:3]
        if mapped_ids:
            conn.execute(
                "DELETE FROM creator_topics WHERE podcastindex_id=?",
                (c["podcastindex_id"],),
            )
            for cat_id in mapped_ids:
                conn.execute(
                    "INSERT OR IGNORE INTO creator_topics (podcastindex_id, apple_category_id) VALUES (?, ?)",
                    (c["podcastindex_id"], cat_id),
                )
            try:
                conn.execute(
                    "UPDATE creators SET apple_topics=? WHERE podcastindex_id=?",
                    (json.dumps([cat_id for cat_id in mapped_ids]), c["podcastindex_id"]),
                )
            except Exception:
                pass
            inserted += 1
        else:
            try:
                out_dir = Path("tests")
                out_dir.mkdir(parents=True, exist_ok=True)
                with (out_dir / "creator_topics_failures.log").open("a") as f:
                    f.write(f"{datetime.utcnow().isoformat()} | pid={c['podcastindex_id']} | title={c['title'] or ''}\n")
            except Exception:
                pass
        if idx % 5 == 0 or idx == total:
            print(f"Topics progress: {idx}/{total} creators... inserted {inserted}", end="\r")
    conn.commit()
    remaining = max(total_candidates - inserted, 0)
    print(f"\nâœ… Assigned topics for {inserted}/{total} creators. Existing before run: {total_existing}; remaining unmapped now: {remaining}")
    conn.close()
    return 0


def assign_brand_topics(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_brands_table(conn)
    ensure_apple_categories_table(conn)
    ensure_brand_topics_table(conn)
    cat_rows = conn.execute("SELECT apple_id, parent_apple_id, name, full_path FROM apple_podcast_categories").fetchall()
    if not cat_rows:
        print("âŒ apple_podcast_categories is empty. Run: python3 adl static load --apple-topics", file=sys.stderr)
        conn.close()
        return 1
    cat_names = [r["full_path"] for r in cat_rows]
    cat_map = {r["name"].lower(): r["apple_id"] for r in cat_rows}
    for r in cat_rows:
        cat_map[r["full_path"].lower()] = r["apple_id"]

    use_openai = getattr(args, "openai", False)
    if use_openai:
        if not ensure_openai_confirmation("brand topic assignment", allow_fallback=False):
            conn.close()
            return 1
    else:
        if not ensure_ollama_running():
            conn.close()
            return 1
    model = getattr(args, "ollama_model", LOCAL_OLLAMA_MODEL)

    total_candidates = conn.execute(
        """
        SELECT COUNT(*) FROM brands b
        WHERE NOT EXISTS (SELECT 1 FROM brand_topics bt WHERE bt.adlid = b.adlid)
        """
    ).fetchone()[0]
    if getattr(args, "nolimit", False):
        resp = input("Type YES to process ALL brands without limit: ").strip()
        if resp != "YES":
            print("Aborted.")
            conn.close()
            return 1
        brands = conn.execute(
            """
            SELECT b.adlid, b.brand_name, b.brand_summary, b.brand_vertical, b.brand_audience_profile,
                   b.brand_use_cases, b.brand_pain_points, b.brand_keywords, b.brand_geo_focus
            FROM brands b
            WHERE NOT EXISTS (SELECT 1 FROM brand_topics bt WHERE bt.adlid = b.adlid)
            """
        ).fetchall()
    else:
        brands = conn.execute(
            """
            SELECT b.adlid, b.brand_name, b.brand_summary, b.brand_vertical, b.brand_audience_profile,
                   b.brand_use_cases, b.brand_pain_points, b.brand_keywords, b.brand_geo_focus
            FROM brands b
            WHERE NOT EXISTS (SELECT 1 FROM brand_topics bt WHERE bt.adlid = b.adlid)
            LIMIT ? OFFSET ?
            """,
            (args.limit, args.offset),
        ).fetchall()
    if not brands:
        print("No brands found for topic assignment.")
        conn.close()
        return 0

    def llm_topics(payload: str) -> List[str]:
        prompt = (
            "You are mapping a brand to Apple Podcast categories. Pick 1-3 categories from the list that best match the brand's focus.\n"
            "Return JSON ONLY: {\"categories\": [\"<category name>\"]}. Use names exactly from the list provided.\n"
            f"AVAILABLE CATEGORIES:\n" + "\n".join(cat_names) + "\n\n"
            f"BRAND INFO:\n{payload}\n"
        )
        if use_openai:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            try:
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    response_format={"type": "json_object"},
                )
                data = json.loads(resp.choices[0].message.content)
                return data.get("categories") or []
            except Exception as exc:
                print(f"âš ï¸ Topic LLM failed (openai): {exc}", file=sys.stderr)
                return []
        try:
            res = requests.post(
                f"{OLLAMA_HOST}/api/generate",
                json={"model": model, "prompt": prompt, "format": "json", "options": {"num_predict": 256}},
                timeout=120,
                stream=True,
            )
            res.raise_for_status()
            chunks = []
            for line in res.iter_lines():
                if not line:
                    continue
                try:
                    obj = json.loads(line.decode("utf-8"))
                    if "response" in obj:
                        chunks.append(obj["response"])
                except Exception:
                    continue
            text = "".join(chunks).strip()
            for fence in ("```json", "```", "`"):
                text = text.replace(fence, "")
            try:
                data = json.loads(text)
                return data.get("categories") or []
            except Exception:
                start = text.find("{")
                end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try:
                        data = json.loads(text[start : end + 1])
                        return data.get("categories") or []
                    except Exception:
                        return []
            return []
        except Exception as exc:
            print(f"âš ï¸ Topic LLM failed (ollama): {exc}", file=sys.stderr)
            return []

    inserted = 0
    total = len(brands)
    if total:
        print(f"Assigning topics for {total} brands (eligible: {total_candidates})...")
    for idx, b in enumerate(brands, 1):
        payload_parts = [
            f"NAME: {b['brand_name'] or ''}",
            f"SUMMARY: {b['brand_summary'] or ''}",
            f"VERTICAL: {b['brand_vertical'] or ''}",
            f"AUDIENCE: {b['brand_audience_profile'] or ''}",
            f"USE_CASES: {b['brand_use_cases'] or ''}",
            f"PAIN_POINTS: {b['brand_pain_points'] or ''}",
            f"KEYWORDS: {b['brand_keywords'] or ''}",
            f"GEO: {b['brand_geo_focus'] or ''}",
        ]
        topics = llm_topics("\n".join(payload_parts))
        mapped_ids = []
        for t in topics:
            key = str(t).lower().strip()
            if key in cat_map:
                mapped_ids.append(cat_map[key])
        if not mapped_ids and topics:
            for t in topics:
                key = str(t).lower().strip()
                for name, apple_id in cat_map.items():
                    if key in name:
                        mapped_ids.append(apple_id)
                        break
        mapped_ids = mapped_ids[:3]
        if mapped_ids:
            conn.execute(
                "DELETE FROM brand_topics WHERE adlid=?",
                (b["adlid"],),
            )
            for cat_id in mapped_ids:
                conn.execute(
                    "INSERT OR IGNORE INTO brand_topics (adlid, apple_category_id) VALUES (?, ?)",
                    (b["adlid"], cat_id),
                )
            inserted += 1
        if idx % 5 == 0 or idx == total:
            print(f"Topics progress: {idx}/{total} brands... inserted {inserted}", end="\r")
    conn.commit()
    print(f"\nâœ… Assigned topics for {inserted}/{total} brands.")
    conn.close()
    return 0



def build_brand_text_block(row: sqlite3.Row) -> str:
    def clip(val: Optional[str], max_len: int = 800) -> str:
        if not val:
            return ""
        s = str(val)
        if len(s) > max_len:
            return s[:max_len]
        return s

    parts = []
    parts.append(f"NAME: {clip(row['brand_name'])}")
    parts.append(f"SUMMARY: {clip(row['brand_summary'] or row['extracted_description'])}")
    vertical = row["brand_vertical_override"] or row["recommended_vertical"] or row["brand_vertical"] or row["extracted_product_category"]
    parts.append(f"VERTICAL: {clip(vertical)}")
    parts.append(f"AUDIENCE: {clip(row['brand_audience_profile'] or row['extracted_target_audience'])}")
    parts.append(f"FUNCTIONS: ")
    parts.append(f"USE_CASES: {clip(row['brand_use_cases'])}")
    parts.append(f"PAIN_POINTS: {clip(row['brand_pain_points'] or row['extracted_goals'])}")
    parts.append(f"KEYWORDS: {clip(row['brand_keywords'] or row['extracted_key_themes'])}")
    parts.append(f"GEO: {clip(row['brand_geo_focus'])}")
    return "\n".join(parts)


def embed_text_block(
    text: str,
    provider: Optional[str] = None,
    model: str = "text-embedding-3-large",
) -> Optional[List[float]]:
    if not text:
        return None
    if provider is None:
        provider = resolve_llm_provider(LLM_PROVIDER_EMBED, "local")
    if provider == "openai":
        if not ensure_openai_confirmation("embeddings", allow_fallback=False):
            return None
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            print("âŒ OPENAI_API_KEY not set; cannot embed.", file=sys.stderr)
            return None
        client = OpenAI(api_key=key)
        try:
            resp = client.embeddings.create(
                model=model,
                input=text,
            )
            return resp.data[0].embedding
        except Exception as exc:
            print(f"âš ï¸ Embedding failed: {exc}", file=sys.stderr)
            return None
    # Local embedding via Ollama
    if not ensure_ollama_running():
        return None
    try:
        res = requests.post(
            f"{OLLAMA_HOST}/api/embed",
            json={"model": model, "input": text},
            timeout=120,
        )
        if res.status_code == 404:
            print("âŒ Ollama embeddings endpoint not available (404). Ensure your Ollama version supports /api/embed or use --openai.", file=sys.stderr)
            return None
        res.raise_for_status()
        data = res.json()
        vec = data.get("embedding") or data.get("embeddings")
        if not vec:
            print("âš ï¸ Ollama embeddings response missing 'embedding' field.", file=sys.stderr)
        return vec
    except Exception as exc:
        print(f"âš ï¸ Local embedding failed: {exc}", file=sys.stderr)
        return None


def parse_embedding(text: str) -> Optional[List[float]]:
    try:
        return json.loads(text) if text else None
    except Exception:
        return None


# ---- Lexicon / taxonomy helpers ----
def map_label_from_lexicon(conn: sqlite3.Connection, raw_label: Optional[str]) -> Optional[str]:
    if raw_label is None:
        return None
    if isinstance(raw_label, list):
        raw = ", ".join(str(x) for x in raw_label if x)
    else:
        raw = str(raw_label)
    raw = raw.strip()
    if not raw:
        return raw
    row = conn.execute(
        """
        SELECT normalized_label
        FROM normalization_lexicon
        WHERE lower(raw_label) = lower(?)
        LIMIT 1
        """,
        (raw,),
    ).fetchone()
    if row and row["normalized_label"]:
        return row["normalized_label"]
    return raw


def ollama_generate(prompt: str, model: str) -> Optional[str]:
    if not ensure_ollama_running():
        return None
    try:
        res = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "format": "json",
                "options": {"num_predict": 1024, "stop": ["\n\n", "\n###"]},
            },
            stream=True,
            timeout=120,
        )
        res.raise_for_status()
        chunks = []
        for line in res.iter_lines():
            if not line:
                continue
            try:
                obj = json.loads(line.decode("utf-8"))
                if "response" in obj:
                    chunks.append(obj["response"])
            except Exception:
                continue
        return "".join(chunks).strip()
    except Exception as exc:
        print(f"âš ï¸ Ollama request failed: {exc}", file=sys.stderr)
        return None


def simplify_entity_text(entity_type: str, name: str, raw_text: str, provider: str, model: str) -> Optional[str]:
    if not raw_text:
        return raw_text
    prompt = (
        "Rewrite the following noisy text into a short, plain-language description.\n"
        "Keep only high-signal details: what it is, who it's for, core topics/offerings.\n"
        "Remove navigation/UI/boilerplate, testimonials, pricing, CTA fluff.\n"
        "Stay factual; do not invent missing details.\n\n"
        f"ENTITY_TYPE: {entity_type}\n"
        f"NAME: {name}\n"
        f"RAW TEXT:\n{raw_text}\n"
    )
    if provider == "openai":
        if not ensure_openai_confirmation("entity simplification", allow_fallback=False):
            return raw_text
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            return raw_text
        client = OpenAI(api_key=key)
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
            )
            return resp.choices[0].message.content.strip()
        except Exception:
            return raw_text
    # local / ollama
    text = ollama_generate(prompt, model)
    return text or raw_text


def map_topics_to_apple(conn: sqlite3.Connection, topics_val) -> str:
    if not topics_val:
        return ""
    # If apple categories table empty, just serialize strings
    has_cats = conn.execute("SELECT 1 FROM apple_podcast_categories LIMIT 1").fetchone()
    if not has_cats:
        if isinstance(topics_val, list):
            return ",".join([t if isinstance(t, str) else t.get("apple_subcategory_name") or t.get("apple_category_name") or "" for t in topics_val])
        return str(topics_val)
    rows = conn.execute("SELECT name, full_path FROM apple_podcast_categories").fetchall()
    names = {r["name"].lower(): r["full_path"] for r in rows}
    collected: List[str] = []
    def handle_item(item):
        if isinstance(item, list):
            for sub in item:
                handle_item(sub)
            return
        if isinstance(item, str):
            key = item.lower()
        elif isinstance(item, dict):
            key = (
                item.get("apple_subcategory_name")
                or item.get("apple_category_name")
                or item.get("apple_category_id")
                or ""
            ).lower()
        else:
            key = str(item).lower()
        if key in names:
            collected.append(names[key])
        elif key:
            collected.append(key)

    if isinstance(topics_val, list):
        handle_item(topics_val)
    else:
        handle_item(topics_val)
    return ",".join(collected)


def ingest_brand(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_brands_table(conn)

    website_url = args.url
    website_text, links = scrape_website_text(website_url)
    about = fetch_website_about(website_url)
    if about and about.get("about"):
        website_text = about["about"]
    socials = find_social_links(links)

    linkedin_text = ""
    if socials["linkedin"]:
        linkedin_text = scrape_linkedin_about(socials["linkedin"][0])

    social_raw = []
    twitter_bio = None
    instagram_bio = None
    if socials["twitter"]:
        twitter_bio = fetch_twitter_bio(socials["twitter"][0])
        if twitter_bio:
            social_raw.append({"twitter": twitter_bio})
    if socials["instagram"]:
        instagram_bio = fetch_instagram_bio(socials["instagram"][0])
        if instagram_bio:
            social_raw.append({"instagram": instagram_bio})
    if socials["website"]:
        about = fetch_website_about(website_url)
        if about:
            social_raw.append({"website": about})

    extracted = extract_brand_fields(website_text, linkedin_text)
    if not extracted:
        print("âŒ GPT extraction failed or no OPENAI_API_KEY; brand not inserted.")
        return 1

    brand_name = args.brand_name or extracted.get("brand_name")
    def as_str(val):
        if val is None:
            return None
        if isinstance(val, (list, dict)):
            return json.dumps(val)
        return str(val)

    # Map extracted fields into both legacy extracted_* and new brand_* columns
    ex_description = as_str(extracted.get("summary"))
    ex_product_category = as_str(extracted.get("brand_vertical"))
    ex_target = as_str(extracted.get("brand_audience_profile"))
    ex_tone = None
    ex_key_themes = as_str(extracted.get("keywords"))
    ex_goals = as_str(extracted.get("brand_pain_points"))

    conn.execute(
        """
        INSERT INTO brands (
            brand_name, website_url, source_website_raw, source_linkedin_raw, source_social_raw,
            extracted_description, extracted_product_category, extracted_target_audience,
            extracted_tone, extracted_key_themes, extracted_goals,
            brand_vertical, brand_audience_profile, brand_use_cases, brand_pain_points, brand_geo_focus, brand_keywords, brand_summary,
            brand_vertical_override, flag_inconsistent_positioning,
            recommended_vertical, recommended_confidence, candidate_verticals,
            created_at, updated_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        ON CONFLICT(website_url) DO UPDATE SET
            brand_name=excluded.brand_name,
            source_website_raw=excluded.source_website_raw,
            source_linkedin_raw=excluded.source_linkedin_raw,
            source_social_raw=excluded.source_social_raw,
            extracted_description=excluded.extracted_description,
            extracted_product_category=excluded.extracted_product_category,
            extracted_target_audience=excluded.extracted_target_audience,
            extracted_tone=excluded.extracted_tone,
            extracted_key_themes=excluded.extracted_key_themes,
            extracted_goals=excluded.extracted_goals,
            brand_vertical=excluded.brand_vertical,
            brand_audience_profile=excluded.brand_audience_profile,
            brand_use_cases=excluded.brand_use_cases,
            brand_pain_points=excluded.brand_pain_points,
            brand_geo_focus=excluded.brand_geo_focus,
            brand_keywords=excluded.brand_keywords,
            brand_summary=excluded.brand_summary,
            brand_vertical_override=excluded.brand_vertical_override,
            flag_inconsistent_positioning=excluded.flag_inconsistent_positioning,
            recommended_vertical=excluded.recommended_vertical,
            recommended_confidence=excluded.recommended_confidence,
            candidate_verticals=excluded.candidate_verticals,
            updated_at=CURRENT_TIMESTAMP
        """,
        (
            as_str(brand_name),          # brand_name
            website_url,                 # website_url
            website_text,                # source_website_raw
            linkedin_text,               # source_linkedin_raw
            json.dumps(social_raw),      # source_social_raw
            ex_description,              # extracted_description
            ex_product_category,         # extracted_product_category
            ex_target,                   # extracted_target_audience
            ex_tone,                     # extracted_tone
            ex_key_themes,               # extracted_key_themes
            ex_goals,                    # extracted_goals
            as_str(extracted.get("brand_vertical")),          # brand_vertical
            as_str(extracted.get("brand_audience_profile")),  # brand_audience_profile
            as_str(extracted.get("brand_use_cases")),         # brand_use_cases
            as_str(extracted.get("brand_pain_points")),       # brand_pain_points
            as_str(extracted.get("brand_geo_focus")),         # brand_geo_focus
            as_str(extracted.get("keywords")),                # brand_keywords
            as_str(extracted.get("summary")),                 # brand_summary
            None,                                             # brand_vertical_override (manual)
            1 if extracted.get("flag_inconsistent_positioning") else 0,  # flag
            as_str(extracted.get("recommended_vertical")),
            float(extracted.get("recommended_confidence")) if extracted.get("recommended_confidence") is not None else None,
            json.dumps(extracted.get("candidate_verticals")) if extracted.get("candidate_verticals") is not None else None,
        ),
    )
    conn.commit()
    # Fetch inserted/updated row for optional test output
    row = conn.execute("SELECT * FROM brands WHERE website_url = ?", (website_url,)).fetchone()
    adlid = row["adlid"] if row else None
    print(f"âœ… Stored brand: {brand_name or website_url}")

    if args.write_test and row and adlid:
        out_dir = Path("tests")
        out_dir.mkdir(parents=True, exist_ok=True)
        txt_path = out_dir / f"test-enrich-brand-adlid-{adlid}_cleaned.txt"
        txt_path.write_text(build_brand_text_block(row))
        print(f"ðŸ“ Wrote {txt_path}")
    conn.close()
    return 0



def test_compare(args: argparse.Namespace) -> int:
    def load_text(txt: Optional[str], path: Optional[Path]) -> Optional[str]:
        if txt:
            return txt
        if path and path.exists():
            return path.read_text()
        return None

    if hasattr(args, "brand_id") and getattr(args, "brand_id", None) and hasattr(args, "creator_id") and getattr(args, "creator_id", None):
        db_path = Path(getattr(args, "adl_db", "adelined_matching.db"))
        if not db_path.exists():
            print(f"âŒ DB not found: {db_path}", file=sys.stderr)
            return 1
        conn = sqlite3.connect(db_path, timeout=10)
        conn.row_factory = sqlite3.Row
        configure_sqlite(conn)
        bmap = load_embeddings_map(conn, "brand", args.brand_id)
        cmap = load_embeddings_map(conn, "creator", args.creator_id)
        if not bmap or not cmap:
            print("âŒ Missing embeddings for provided ids.", file=sys.stderr)
            return 1
        weights = {
            "semantic_summary": 0.25,
            "audience": 0.25,
            "pain_points": 0.15,
            "vertical": 0.15,
            "topics": 0.10,
            "tone": 0.05,
            "geo": 0.05,
        }
        sims = {}
        for vt, w in weights.items():
            if vt in bmap and vt in cmap:
                sims[vt] = cosine_similarity(bmap[vt], cmap[vt])
        num = sum(sims.get(vt, 0) * w for vt, w in weights.items() if vt in sims)
        denom = sum(w for vt, w in weights.items() if vt in sims)
        final_score = num / denom if denom else 0.0
        print("TEST COMPARISON RESULT")
        print("======================")
        print(f"Final score: {final_score:.3f}")
        print(f"Interpretation: {relevance_label(final_score)}")
        print("\nPer-dimension:")
        for vt, val in sims.items():
            print(f"  {vt:16s}: {val:.3f}")
        conn.close()
        return 0

    t1 = load_text(args.text1, args.file1)
    t2 = load_text(args.text2, args.file2)
    if not t1 or not t2:
        print("âŒ Provide both inputs via --text1/--file1 and --text2/--file2", file=sys.stderr)
        return 1

    v1 = embed_text_block(t1)
    v2 = embed_text_block(t2)
    if not v1 or not v2:
        print("âŒ Failed to embed one or both inputs.", file=sys.stderr)
        return 1

    score = cosine_similarity(v1, v2)
    print("TEST COMPARISON RESULT")
    print("======================")
    print(f"Similarity score: {score:.3f}")
    print(f"Interpretation: {relevance_label(score)}")
    return 0


def export_entity(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    target = args.target
    entity_id = args.id
    out_path = args.out or Path(f"tests/export-{target}-{entity_id}.txt")
    out_path.parent.mkdir(parents=True, exist_ok=True)

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        use_norm = getattr(args, "usenorm", False)
        if target == "creator":
            row = conn.execute(
                "SELECT cleaned_text_block, normalized_text FROM creator_enrichment WHERE podcastindex_id = ?",
                (entity_id,),
            ).fetchone()
            if not row:
                print(f"âŒ No enrichment found for creator pid={entity_id}", file=sys.stderr)
                return 1
            text = None
            if use_norm and row["normalized_text"]:
                text = row["normalized_text"]
            elif row["cleaned_text_block"]:
                text = row["cleaned_text_block"]
            if not text:
                print(f"âŒ No text found for creator pid={entity_id}", file=sys.stderr)
                return 1
            out_path.write_text(text)
            print(f"ðŸ“ Exported creator pid={entity_id} {'normalized' if use_norm and row['normalized_text'] else 'cleaned'} block to {out_path}")
            return 0
        if target == "brand":
            row = conn.execute(
                "SELECT * FROM brands WHERE adlid = ?",
                (entity_id,),
            ).fetchone()
            if not row:
                print(f"âŒ No brand found for adlid={entity_id}", file=sys.stderr)
                return 1
            text_block = None
            if use_norm and row["normalized_summary"]:
                parts = [
                    f"NAME: {row['brand_name'] or ''}",
                    f"SUMMARY:\n{row['normalized_summary'] or ''}",
                    f"VERTICAL:\n{row['normalized_vertical'] or ''}",
                    f"AUDIENCE:\n{row['normalized_audience'] or ''}",
                    f"USE_CASES:\n{row['normalized_use_cases'] or ''}",
                    f"PAIN_POINTS:\n{row['normalized_pain_points'] or ''}",
                    f"THEMES:\n{row['normalized_themes'] or ''}",
                    f"TONE:\n{row['normalized_tone'] or ''}",
                    f"GEO:\n{row['normalized_geo'] or ''}",
                    f"TOPICS:\n{row['normalized_topics'] or ''}",
                ]
                text_block = "\n".join(parts)
            else:
                text_block = build_brand_text_block(row)
            out_path.write_text(text_block)
            print(f"ðŸ“ Exported brand adlid={entity_id} {'normalized' if use_norm and row['normalized_summary'] else 'cleaned'} block to {out_path}")
            return 0
        print(f"âŒ Unknown target: {target}", file=sys.stderr)
        return 1
    finally:
        conn.close()


def find_creator_country(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    if not os.getenv("PODSCAN_API_KEY"):
        print("âŒ PODSCAN_API_KEY not set; cannot lookup creator country.", file=sys.stderr)
        return 1

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        spinner_frames = ["|", "/", "-", "\\"]

        def start_spinner(label: str) -> Tuple[threading.Event, threading.Thread]:
            stop_event = threading.Event()

            def run() -> None:
                idx = 0
                while not stop_event.is_set():
                    frame = spinner_frames[idx % len(spinner_frames)]
                    sys.stdout.write(f"\r{frame} {label}")
                    sys.stdout.flush()
                    idx += 1
                    time.sleep(0.1)

            thread = threading.Thread(target=run, daemon=True)
            thread.start()
            return stop_event, thread

        def sleep_with_countdown(seconds: int) -> None:
            print(f"â¸ï¸ Sleeping for {seconds}s to respect rate limits.")
            for remaining in range(seconds, 0, -1):
                sys.stdout.write(f"\râ³ Sleeping... {remaining}s")
                sys.stdout.flush()
                time.sleep(1)
            sys.stdout.write("\râ–¶ï¸ Restarting requests.        \n")
            sys.stdout.flush()

        ensure_podcastindex_tables(conn)
        ensure_creator_country_table(conn)

        excluded = (
            "religion",
            "",
            "government",
            "christianity",
            "politics",
            "spirituality",
            "pets",
            "management",
            "health",
        )
        params: List = []
        if args.pid is not None:
            print("âš ï¸ --pid forces print-only mode; no DB writes will occur.")
            base_query = """
                SELECT id AS podcastindex_id, itunesId, title
                FROM podcastindex_feeds
                WHERE id = ?
                  AND NOT EXISTS (
                      SELECT 1 FROM creator_country cc
                      WHERE cc.podcastindex_id = podcastindex_feeds.id
                  )
            """
            params.append(args.pid)
            args.print_only = True
        else:
            base_query = """
                SELECT id AS podcastindex_id, itunesId, title
                FROM podcastindex_feeds
                WHERE dead = 0
                  AND CAST(itunesId AS INTEGER) > 0
                  AND language IN ('en', 'en-us', 'en-gb')
                  AND episodeCount >= 10
                  AND lastUpdate >= strftime('%s','now') - (180*24*60*60)
                  AND link IS NOT NULL AND TRIM(link) <> ''
                  AND url  IS NOT NULL AND TRIM(url)  <> ''
                  AND lower(category1) NOT IN ({excluded})
                  AND NOT EXISTS (
                      SELECT 1 FROM creator_country cc
                      WHERE cc.podcastindex_id = podcastindex_feeds.id
                  )
                ORDER BY lastUpdate DESC, episodeCount DESC
            """.format(excluded=",".join("?" for _ in excluded))
            params.extend(list(excluded))
            if args.limit != 0:
                base_query += " LIMIT ?"
                params.append(args.limit)

        rows = conn.execute(base_query, params).fetchall()
        if not rows:
            print("No creators matched the filter.")
            return 0

        inserted = 0
        missing = 0
        missing_itunes = 0
        missing_country = 0
        not_found_404 = 0
        podscan_hits = 0
        batch_num = 0
        for row in rows:
            pid = row["podcastindex_id"]
            itunes_id = row["itunesId"]
            if not itunes_id:
                missing += 1
                missing_itunes += 1
                continue
            try:
                label = f"Fetching Podscan... total={podscan_hits + 1}"
                stop_event, thread = start_spinner(label)
                try:
                    country, email, podscan_data = fetch_podscan_country(itunes_id)
                finally:
                    stop_event.set()
                    thread.join()
                    sys.stdout.write("\r" + " " * (len(label) + 4) + "\r")
                    sys.stdout.flush()
            except Exception as exc:
                print(
                    f"âŒ Podscan lookup failed for itunesId={itunes_id}: {exc}",
                    file=sys.stderr,
                )
                return 1
            podscan_hits += 1
            if args.pid is not None:
                out_path = Path("tests") / f"podscan_by_itunesid_{pid}.txt"
                if podscan_data is not None:
                    out_path.write_text(
                        json.dumps(podscan_data, indent=2, ensure_ascii=True) + "\n"
                    )
            if country == "404":
                missing += 1
                not_found_404 += 1
            elif not country:
                missing += 1
                missing_country += 1
            if args.print_only:
                print(
                    f"podcastindex_id={pid} itunesId={itunes_id} country={country or ''} email={email or ''}"
                )
            else:
                conn.execute(
                    """
                    INSERT OR IGNORE INTO creator_country (podcastindex_id, itunesid, country)
                    VALUES (?, ?, ?)
                    """,
                    (pid, itunes_id, country),
                )
                inserted += 1
            if podscan_hits % 25 == 0:
                batch_num += 1
                print(
                    "ðŸ“Š Batch {batch} done: podscan_hits={hits} "
                    "warnings(missing_itunes={mi}, country_missing={mc}, podscan_404={nf}).".format(
                        batch=batch_num,
                        hits=podscan_hits,
                        mi=missing_itunes,
                        mc=missing_country,
                        nf=not_found_404,
                    )
                )
                sleep_with_countdown(20)

        if not args.print_only:
            conn.commit()
            print(
                "âœ… Stored country for {inserted} creators. Warnings: {missing} "
                "(missing_itunes={mi}, country_missing={mc}, podscan_404={nf}).".format(
                    inserted=inserted,
                    missing=missing,
                    mi=missing_itunes,
                    mc=missing_country,
                    nf=not_found_404,
                )
            )
        else:
            print(
                "âœ… Print-only complete. Found {found} countries; warnings: {missing} "
                "(missing_itunes={mi}, country_missing={mc}, podscan_404={nf}).".format(
                    found=len(rows) - missing,
                    missing=missing,
                    mi=missing_itunes,
                    mc=missing_country,
                    nf=not_found_404,
                )
            )
        return 0
    finally:
        conn.close()


def update_from_podscan(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    key = os.getenv("PODSCAN_API_KEY")
    if not key:
        print("âŒ PODSCAN_API_KEY not set; cannot pull from Podscan.", file=sys.stderr)
        return 1

    per_page = max(1, min(int(args.per_page), 50))
    limit = int(args.limit)
    order_by = args.order_by
    order_dir = "asc"
    region = "GB"
    month_only = args.month_only
    min_last_episode = (datetime.utcnow() - timedelta(days=30 * 14)).strftime("%Y-%m-%d")
    max_last_episode = None
    if month_only:
        try:
            month_str, year_str = month_only.split("_", 1)
            month = int(month_str)
            year = int(year_str)
            year = 2000 + year if year < 100 else year
            if not (1 <= month <= 12):
                raise ValueError("month out of range")
        except ValueError as exc:
            print(f"âŒ Invalid --month_only value '{month_only}': {exc}", file=sys.stderr)
            return 1
        start = datetime(year, month, 1)
        if month == 12:
            end = datetime(year + 1, 1, 1) - timedelta(days=1)
        else:
            end = datetime(year, month + 1, 1) - timedelta(days=1)
        min_last_episode = start.strftime("%Y-%m-%d")
        max_last_episode = end.strftime("%Y-%m-%d")

    headers = {
        "Authorization": f"Bearer {key}",
        "Accept": "application/json",
        "User-Agent": "adelined/1.0",
    }
    params = {
        "per_page": per_page,
        "order_by": order_by,
        "order_dir": order_dir,
        "region": region,
        "min_last_episode_posted_at": min_last_episode,
    }
    if max_last_episode:
        params["max_last_episode_posted_at"] = max_last_episode

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        ensure_podscan_table(conn)

        stored = 0
        page = 1
        total_pages = None
        while True:
            if limit != 0 and stored >= limit:
                break
            params["page"] = page
            res = requests.get(
                f"{PODSCAN_API_BASE}/podcasts/search",
                headers=headers,
                params=params,
                timeout=20,
            )
            res.raise_for_status()
            payload = res.json()
            podcasts = payload.get("podcasts") or []
            if total_pages is None:
                pagination = payload.get("pagination") or {}
                try:
                    total_pages = int(pagination.get("last_page") or 1)
                except (TypeError, ValueError):
                    total_pages = 1
            if not podcasts:
                break

            for podcast in podcasts:
                if limit != 0 and stored >= limit:
                    break
                email, website = extract_podscan_contact(podcast)
                verb = "INSERT OR REPLACE" if args.force else "INSERT OR IGNORE"
                conn.execute(
                    f"""
                    {verb} INTO podscan
                        (id, guid, podcastindex_id, itunesid, email, website, full_json, region)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        podcast.get("podcast_id"),
                        podcast.get("podcast_guid"),
                        None,
                        podcast.get("podcast_itunes_id"),
                        email,
                        website,
                        json.dumps(podcast, ensure_ascii=True),
                        podcast.get("region"),
                    ),
                )
                stored += 1

            if total_pages is not None and page >= total_pages:
                break
            page += 1

        conn.commit()
        print(
            "âœ… Stored {count} Podscan records (region=GB, min_last_episode_posted_at={min_date}{max_suffix}).".format(
                count=stored,
                min_date=min_last_episode,
                max_suffix=f', max_last_episode_posted_at={max_last_episode}' if max_last_episode else "",
            )
        )
        return 0
    finally:
        conn.close()


def get_normalized_entity(conn: sqlite3.Connection, entity_type: str, entity_id: int) -> Optional[sqlite3.Row]:
    return conn.execute(
        """
        SELECT *
        FROM entity_normalized
        WHERE entity_type=? AND entity_id=? AND version='v1'
        """,
        (entity_type, entity_id),
    ).fetchone()


def normalize_entities(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    if getattr(args, "podscan", False) and args.entity_type != "creator":
        print("âŒ --podscan is only supported for creator normalization.", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_creators_table(conn)
    ensure_creator_enrichment_table(conn)
    ensure_brands_table(conn)
    ensure_entity_normalized_table(conn)
    ensure_normalization_lexicon_table(conn)
    ensure_apple_categories_table(conn)
    if getattr(args, "podscan", False):
        ensure_podscan_table(conn)

    entity_type = args.entity_type
    provider = getattr(args, "llm_provider", "local")
    ollama_model = getattr(args, "ollama_model", LOCAL_OLLAMA_MODEL)
    ids: List[int] = []
    if getattr(args, "id", None):
        ids = [args.id]
    elif getattr(args, "podcastindex_ids", None):
        if entity_type != "creator":
            print("âŒ --podcastindex_ids is only supported for creators.", file=sys.stderr)
            conn.close()
            return 1
        ids = list(args.podcastindex_ids)
    elif getattr(args, "id_range", None):
        start_id, end_id = args.id_range
        if entity_type == "brand":
            rows = conn.execute(
                "SELECT adlid FROM brands WHERE adlid BETWEEN ? AND ? ORDER BY adlid",
                (start_id, end_id),
            ).fetchall()
            ids = [r[0] for r in rows]
        else:
            if getattr(args, "podscan", False):
                rows = conn.execute(
                    """
                    SELECT c.podcastindex_id
                    FROM creators c
                    JOIN podscan ps ON ps.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id BETWEEN ? AND ?
                      AND ps.full_json IS NOT NULL
                      AND TRIM(ps.full_json) <> ''
                    ORDER BY c.podcastindex_id
                    """,
                    (start_id, end_id),
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT podcastindex_id FROM creators WHERE podcastindex_id BETWEEN ? AND ? ORDER BY podcastindex_id",
                    (start_id, end_id),
                ).fetchall()
            ids = [r[0] for r in rows]
    else:
        if entity_type == "brand" and getattr(args, "all_brands", False):
            rows = conn.execute(
                "SELECT adlid FROM brands LIMIT ? OFFSET ?",
                (args.limit, args.offset),
            ).fetchall()
            ids = [r[0] for r in rows]
        if entity_type == "creator" and getattr(args, "all_creators", False):
            if getattr(args, "podscan", False):
                rows = conn.execute(
                    """
                    SELECT c.podcastindex_id
                    FROM creators c
                    JOIN podscan ps ON ps.podcastindex_id = c.podcastindex_id
                    WHERE ps.full_json IS NOT NULL
                      AND TRIM(ps.full_json) <> ''
                    LIMIT ? OFFSET ?
                    """,
                    (args.limit, args.offset),
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT podcastindex_id FROM creators LIMIT ? OFFSET ?",
                    (args.limit, args.offset),
                ).fetchall()
            ids = [r[0] for r in rows]
    if not ids:
        print("âŒ Provide --id, --podcastindex_ids, or --all-brands/--all-creators", file=sys.stderr)
        conn.close()
        return 1

    # Provide a short hint list of Apple categories if present
    cat_rows = conn.execute(
        "SELECT full_path FROM apple_podcast_categories LIMIT 30"
    ).fetchall()
    categories_hint = "\n".join([r["full_path"] for r in cat_rows]) if cat_rows else ""

    # If provider is openai, confirm once; if declined and fallback allowed, switch to local.
    if provider == "openai":
        if not ensure_openai_confirmation("normalization", allow_fallback=True):
            provider = "local"

    normalized = 0
    attempted = 0
    for eid in ids:
        exists = conn.execute(
            "SELECT 1 FROM entity_normalized WHERE entity_type=? AND entity_id=? AND version='v1'",
            (entity_type, eid),
        ).fetchone()
        if exists and not getattr(args, "force", False):
            continue
        name = ""
        fields_parts: List[str] = []
        if entity_type == "brand":
            row = conn.execute("SELECT * FROM brands WHERE adlid=?", (eid,)).fetchone()
            if not row:
                continue
            name = row["brand_name"] or f"brand {eid}"
            for key in ["brand_summary", "brand_vertical", "brand_audience_profile", "brand_use_cases", "brand_pain_points", "brand_keywords", "source_website_raw", "source_linkedin_raw"]:
                if row[key]:
                    fields_parts.append(f"{key.upper()}: {row[key]}")
        else:
            if getattr(args, "podscan", False):
                ensure_podscan_table(conn)
                row = conn.execute(
                    """
                    SELECT c.*
                    FROM creators c
                    JOIN podscan ps ON ps.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id=?
                      AND ps.full_json IS NOT NULL
                      AND TRIM(ps.full_json) <> ''
                    """,
                    (eid,),
                ).fetchone()
            else:
                row = conn.execute(
                    """
                    SELECT c.*, ce.cleaned_text_block, ce.raw_text_sources
                    FROM creators c
                    LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                    WHERE c.podcastindex_id=?
                    """,
                    (eid,),
                ).fetchone()
            if not row:
                continue
            name = row["title"] or f"podcast {eid}"
            if getattr(args, "podscan", False):
                podscan_row = conn.execute(
                    """
                    SELECT full_json
                    FROM podscan
                    WHERE CAST(podcastindex_id AS INTEGER) = ?
                      AND full_json IS NOT NULL
                      AND TRIM(full_json) <> ''
                    ORDER BY LENGTH(full_json) DESC
                    LIMIT 1
                    """,
                    (eid,),
                ).fetchone()
                podscan_json = podscan_row["full_json"] if podscan_row else None
                if not podscan_json:
                    print(f"âš ï¸ No Podscan JSON for creator {eid}; skipping.")
                    continue
                try:
                    pod = json.loads(podscan_json)
                except Exception:
                    print(f"âš ï¸ Invalid Podscan JSON for creator {eid}; skipping.")
                    continue
                desc = pod.get("podcast_description") or ""
                if pod.get("podcast_name"):
                    name = pod.get("podcast_name") or name
                if not desc:
                    print(f"âš ï¸ Podscan description missing for creator {eid}; skipping.")
                    continue
                fields_parts.append(f"DESCRIPTION: {desc}")
            else:
                for key in ["title", "description", "categories", "cleaned_text_block"]:
                    if row.get(key) if isinstance(row, dict) else row[key]:
                        fields_parts.append(f"{key.upper()}: {row[key]}")
                try:
                    raw_sources = json.loads(row["raw_text_sources"]) if row["raw_text_sources"] else {}
                except Exception:
                    raw_sources = {}
                for k, v in (raw_sources or {}).items():
                    if v:
                        fields_parts.append(f"{k.upper()}: {v}")
        raw_blob = "\n".join(fields_parts)
        if len(raw_blob) > 12000:
            raw_blob = raw_blob[:12000]
        simplified = simplify_entity_text(entity_type, name, raw_blob, provider, ollama_model) or raw_blob
        fields_text = f"SIMPLIFIED:\n{simplified}\n\nRAW:\n{raw_blob}"
        debug_tag = f"{entity_type}:{eid}"
        attempted += 1
        norm = normalize_entity_to_json(entity_type, name, fields_text, categories_hint, provider=provider, model=ollama_model, debug_tag=debug_tag)
        if not norm or not norm.get("summary"):
            print(f"âš ï¸ Normalization failed for {entity_type} {eid}")
            continue
        # Coerce and map labels through lexicon where possible
        for key in ["vertical", "audience", "themes", "use_cases", "pain_points", "tone", "geo"]:
            norm[key] = map_label_from_lexicon(conn, norm.get(key))
        topics_val = norm.get("topics")
        topics_str = map_topics_to_apple(conn, topics_val)
        # Build a canonical normalized block for reference/embedding
        block_parts = [
            f"SUMMARY:\n{norm.get('summary','')}",
            f"VERTICAL:\n{norm.get('vertical','')}",
            f"AUDIENCE:\n{norm.get('audience','')}",
            f"PAIN_POINTS:\n{norm.get('pain_points','')}",
            f"USE_CASES:\n{norm.get('use_cases','')}",
            f"THEMES:\n{norm.get('themes','')}",
            f"TONE:\n{norm.get('tone','')}",
            f"GEO:\n{norm.get('geo','')}",
            f"TOPICS:\n{topics_str}",
        ]
        normalized_block = "\n".join(block_parts)
        conn.execute(
            """
            INSERT OR REPLACE INTO entity_normalized (entity_type, entity_id, version, normalized_block, norm_fields)
            VALUES (?, ?, 'v1', ?, ?)
            """,
            (entity_type, eid, normalized_block, json.dumps(norm)),
        )
        # persist normalized_* columns
        def to_str(val):
            if val is None:
                return None
            if isinstance(val, list):
                return ", ".join(str(x) for x in val if x)
            return str(val)

        if entity_type == "brand":
            conn.execute(
                """
                UPDATE brands SET
                    normalized_summary=?, normalized_vertical=?, normalized_audience=?, normalized_pain_points=?,
                    normalized_use_cases=?, normalized_themes=?, normalized_tone=?, normalized_geo=?, normalized_topics=?
                WHERE adlid=?
                """,
                (
                    to_str(norm.get("summary")),
                    to_str(norm.get("vertical")),
                    to_str(norm.get("audience")),
                    to_str(norm.get("pain_points")),
                    to_str(norm.get("use_cases")),
                    to_str(norm.get("themes")),
                    to_str(norm.get("tone")),
                    to_str(norm.get("geo")),
                    to_str(topics_str),
                    eid,
                ),
            )
        else:
            conn.execute(
                """
                UPDATE creators SET
                    normalized_summary=?, normalized_vertical=?, normalized_audience=?, normalized_pain_points=?,
                    normalized_use_cases=?, normalized_themes=?, normalized_tone=?, normalized_geo=?, normalized_topics=?
                WHERE podcastindex_id=?
                """,
                (
                    to_str(norm.get("summary")),
                    to_str(norm.get("vertical")),
                    to_str(norm.get("audience")),
                    to_str(norm.get("pain_points")),
                    to_str(norm.get("use_cases")),
                    to_str(norm.get("themes")),
                    to_str(norm.get("tone")),
                    to_str(norm.get("geo")),
                    to_str(topics_str),
                    eid,
                ),
            )
        normalized += 1
        if normalized % 10 == 0:
            conn.commit()
            print(f"Normalized {normalized}/{len(ids)}...", end="\r")
    conn.commit()
    print(f"\nâœ… Normalized {normalized} entities of {attempted} attempted.")
    conn.close()
    return 0


def build_embedding_text(entity: sqlite3.Row, entity_type: str, vector_type: str) -> Optional[str]:
    summary = entity["normalized_summary"] or ""
    vertical = entity["normalized_vertical"] or ""
    audience = entity["normalized_audience"] or ""
    pains = entity["normalized_pain_points"] or ""
    use_cases = entity["normalized_use_cases"] or ""
    themes = entity["normalized_themes"] or ""
    tone = entity["normalized_tone"] or ""
    geo = entity["normalized_geo"] or ""
    topics = entity["normalized_topics"] or ""
    if vector_type == "semantic_summary":
        return f"SUMMARY:\n{summary}\nVERTICAL:\n{vertical}\nAUDIENCE:\n{audience}\nTHEMES:\n{themes}"
    if vector_type == "topics":
        return f"TOPICS (APPLE CATEGORIES):\n{topics}"
    if vector_type == "vertical":
        return f"INDUSTRY / VERTICAL:\n{vertical}"
    if vector_type == "audience":
        return f"AUDIENCE:\n{audience}\nGEO:\n{geo}"
    if vector_type == "pain_points":
        return f"PAIN POINTS:\n{pains}"
    if vector_type == "use_cases":
        return f"USE CASES:\n{use_cases}"
    if vector_type == "themes":
        return f"THEMES:\n{themes}"
    if vector_type == "tone":
        return f"TONE:\n{tone}"
    if vector_type == "geo":
        return f"GEO:\n{geo}"
    return None


def compute_embedding_confidence(entity: sqlite3.Row) -> float:
    score = 1.0
    def missing(val: Optional[str]) -> bool:
        return not val or not str(val).strip()
    if missing(entity["normalized_vertical"]):
        score -= 0.25
    if missing(entity["normalized_audience"]):
        score -= 0.2
    if missing(entity["normalized_summary"]):
        score -= 0.15
    if missing(entity["normalized_topics"]):
        score -= 0.1
    if missing(entity["normalized_pain_points"]):
        score -= 0.05
    if score < 0:
        score = 0.0
    if score > 1:
        score = 1.0
    return score


def embed_entity(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_entity_normalized_table(conn)
    ensure_entity_embeddings_table(conn)
    ensure_creators_table(conn)
    ensure_brands_table(conn)

    vector_types = [
        "semantic_summary",
        "topics",
        "vertical",
        "audience",
        "pain_points",
        "use_cases",
        "themes",
        "tone",
        "geo",
    ]
    entity_type = args.entity_type
    if getattr(args, "openai", False):
        embedding_provider = "openai"
    elif getattr(args, "embedding_provider", None):
        embedding_provider = args.embedding_provider
    else:
        embedding_provider = resolve_llm_provider(LLM_PROVIDER_EMBED, "local")
    if embedding_provider == "openai":
        embedding_model = "text-embedding-3-large"
    else:
        embedding_model = getattr(args, "embedding_model", LOCAL_OLLAMA_EMBED_MODEL)
    if embedding_provider == "openai":
        if not ensure_openai_confirmation("embeddings", allow_fallback=False):
            conn.close()
            return 1
    # delete-only operations
    if getattr(args, "deleteall", False):
        confirm = input(f"Type YES to delete ALL embeddings for {entity_type}s: ").strip()
        if confirm != "YES":
            print("Aborted.")
            conn.close()
            return 1
        cur = conn.execute(
            "DELETE FROM entity_embeddings WHERE entity_type=?",
            (entity_type,),
        )
        conn.commit()
        print(f"ðŸ—‘ï¸ Deleted all embeddings for {entity_type}s.")
        conn.close()
        return 0
    if getattr(args, "delete", False):
        if args.id:
            cur = conn.execute(
                "DELETE FROM entity_embeddings WHERE entity_type=? AND entity_id=?",
                (entity_type, args.id),
            )
            conn.commit()
            print(f"ðŸ—‘ï¸ Deleted embeddings for {entity_type} id={args.id}.")
            conn.close()
            return 0
        if getattr(args, "id_range", None):
            start_id, end_id = args.id_range
            cur = conn.execute(
                "DELETE FROM entity_embeddings WHERE entity_type=? AND entity_id BETWEEN ? AND ?",
                (entity_type, start_id, end_id),
            )
            conn.commit()
            print(f"ðŸ—‘ï¸ Deleted embeddings for {entity_type} ids {start_id}..{end_id}.")
            conn.close()
            return 0
        print("âŒ --delete requires --id or --id-range", file=sys.stderr)
        conn.close()
        return 1
    # Optional bulk delete for this entity type
    if getattr(args, "refreshall", False):
        confirm = input(f"Type YES to delete all embeddings for {entity_type}s: ").strip()
        if confirm != "YES":
            print("Aborted.")
            conn.close()
            return 1
        conn.execute("DELETE FROM entity_embeddings WHERE entity_type=?", (entity_type,))
        conn.commit()
        print(f"ðŸ—‘ï¸ Deleted all {entity_type} embeddings.")

    base_query_creator = """
        SELECT en.id as norm_id, en.normalized_block, c.*
        FROM entity_normalized en
        JOIN creators c ON c.podcastindex_id = en.entity_id
        WHERE en.entity_type='creator' AND en.version='v1'
    """
    base_query_brand = """
        SELECT en.id as norm_id, en.normalized_block, b.*
        FROM entity_normalized en
        JOIN brands b ON b.adlid = en.entity_id
        WHERE en.entity_type='brand' AND en.version='v1'
    """

    rows: List[sqlite3.Row]
    if args.id:
        rows = conn.execute(
            base_query_creator + " AND en.entity_id=?" if entity_type == "creator" else base_query_brand + " AND en.entity_id=?",
            (args.id,),
        ).fetchall()
    else:
        rows = conn.execute(
            (base_query_creator if entity_type == "creator" else base_query_brand) + " LIMIT ? OFFSET ?",
            (args.limit, args.offset),
        ).fetchall()
    if not rows:
        print("No normalized records to embed.")
        conn.close()
        return 0
    model_name = embedding_model
    print(f"Embedding {len(rows)} {entity_type}(s) using provider={embedding_provider}, model={model_name}")
    inserted = 0
    total = len(rows)
    for r in rows:
        ent_id = r["podcastindex_id"] if entity_type == "creator" else r["adlid"]
        conn.execute(
            "DELETE FROM entity_embeddings WHERE entity_type=? AND entity_id=?",
            (entity_type, ent_id),
        )
        confidence = compute_embedding_confidence(r)
        for vt in vector_types:
            text = build_embedding_text(r, entity_type, vt)
            if not text or not text.strip():
                continue
            vec = embed_text_block(text, provider=embedding_provider, model=embedding_model)
            if not vec:
                continue
            conn.execute(
                """
                INSERT INTO entity_embeddings (entity_type, entity_id, vector_type, embedding_vector, confidence, source_text, source_normalized_id, model_name)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (entity_type, ent_id, vt, json.dumps(vec), confidence, text[:4000], r["norm_id"], model_name),
            )
            inserted += 1
        if inserted % 5 == 0 or inserted == total * len(vector_types):
            print(f"Embedding progress: {inserted} vectors for {total} {entity_type}(s)...", end="\r")
    conn.commit()
    print(f"\nâœ… Embedded {inserted} vectors for {len(rows)} {entity_type}(s).")
    conn.close()
    return 0


def load_embeddings_map(conn: sqlite3.Connection, entity_type: str, entity_id: int) -> Dict[str, List[float]]:
    rows = conn.execute(
        """
        SELECT vector_type, embedding_vector
        FROM entity_embeddings
        WHERE entity_type=? AND entity_id=?
        """,
        (entity_type, entity_id),
    ).fetchall()
    result = {}
    for r in rows:
        vec = parse_embedding(r["embedding_vector"])
        if vec:
            result[r["vector_type"]] = vec
    return result

def test_enrich(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    pid = args.pid
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    base_row = conn.execute(
        """
        SELECT p.id AS podcastindex_id,
               p.title AS pi_title,
               p.description AS pi_description,
               p.url AS feed_url,
               p.link AS web_link,
               p.category1, p.category2, p.category3, p.category4, p.category5,
               p.category6, p.category7, p.category8, p.category9, p.category10,
               c.itunesid
        FROM creators c
        LEFT JOIN podcastindex_feeds p ON p.id = c.podcastindex_id
        WHERE c.podcastindex_id = ?
        """,
        (pid,),
    ).fetchone()
    if not base_row:
        print(f"âŒ No creator/podcastindex row found for podcastindex_id={pid}", file=sys.stderr)
        conn.close()
        return 1

    base = dict(base_row)
    print(f"ðŸ”Ž Test-enrich for podcastindex_id={pid} title='{base.get('pi_title') or ''}'")

    title_for_lookup = base.get("pi_title") or ""
    spotify_url = base.get("feed_url") if (base.get("feed_url") or "").startswith("https://open.spotify.com/show/") else None

    print("â†’ Fetching Apple metadata...")
    apple_meta = fetch_apple_metadata(base.get("itunesid"), title_for_lookup)
    print("â†’ Fetching Spotify metadata...")
    spotify_meta = fetch_spotify_metadata(spotify_url, title_for_lookup)
    print("â†’ Fetching YouTube metadata...")
    youtube_meta = fetch_youtube_metadata(title_for_lookup, mode="lean")
    print("â†’ Fetching social bios...")
    social_meta = fetch_social_bios_from_sources(base, apple_meta, spotify_meta, youtube_meta)

    print("â†’ Building text block...")
    raw_pieces = []
    for val in [
        base.get("pi_description"),
        apple_meta.get("description"),
        (spotify_meta.get("show", {}) if spotify_meta else {}).get("description"),
        (youtube_meta.get("channel", {}) if youtube_meta else {}).get("about"),
        (social_meta.get("website") or {}).get("about") if social_meta else None,
    ]:
        if val:
            raw_pieces.append(val)
    if spotify_meta.get("episodes_sample"):
        ep_titles = [ep.get("name") or "" for ep in spotify_meta["episodes_sample"] if ep]
        raw_pieces.append("; ".join(ep_titles[:30]))
    combined_raw = "\n\n".join(raw_pieces)
    summary = extract_creator_metadata(combined_raw) if combined_raw else {}
    cleaned_text, raw_sources = build_text_block(
        base=base,
        apple=apple_meta,
        spotify=spotify_meta,
        youtube=youtube_meta,
        social=social_meta,
        extracted=summary,
    )

    out_dir = Path("tests")
    out_dir.mkdir(parents=True, exist_ok=True)
    base_name = f"test-enrich-pid-{pid}"
    json_path = out_dir / f"{base_name}.json"
    txt_path = out_dir / f"{base_name}_cleaned.txt"

    payload = {
        "podcastindex_id": pid,
        "podcast_title": base.get("pi_title"),
        "base": base,
        "apple": apple_meta,
        "spotify": spotify_meta,
        "youtube": youtube_meta,
        "social": social_meta,
        "raw_text_sources": raw_sources,
        "cleaned_text_block": cleaned_text,
        "summary": summary,
    }
    json_path.write_text(json.dumps(payload, indent=2))
    txt_path.write_text(cleaned_text or "")
    print(f"âœ… Wrote {json_path} and {txt_path}")
    conn.close()
    return 0


def cosine_similarity(a: List[float], b: List[float]) -> float:
    va = np.array(a, dtype=float).ravel()
    vb = np.array(b, dtype=float).ravel()
    if va.shape != vb.shape or va.size == 0:
        return 0.0
    denom = (np.linalg.norm(va) * np.linalg.norm(vb))
    if denom == 0:
        return 0.0
    return float(np.dot(va, vb) / denom)


def format_table(rows: List[Tuple[str, float, str, str]], header: Tuple[str, str, str, str]) -> str:
    cols = list(zip(*([header] + rows)))
    widths = [max(len(str(cell)) for cell in col) for col in cols]
    def fmt_row(row):
        return " | ".join(str(cell).ljust(widths[i]) for i, cell in enumerate(row))
    sep = "-+-".join("-" * w for w in widths)
    lines = [fmt_row(header), sep]
    for r in rows:
        lines.append(fmt_row(r))
    return "\n".join(lines)


def relevance_label(score: float) -> str:
    if score >= 0.8:
        return "strong"
    if score >= 0.7:
        return "medium"
    return "weak"


# Override with multi-dimensional ranking using entity_embeddings
def rank_entities(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"âŒ DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_entity_embeddings_table(conn)
    limit = args.limit
    adlid = args.adlid
    weights = {
        "semantic_summary": 0.25,
        "audience": 0.25,
        "pain_points": 0.15,
        "vertical": 0.15,
        "topics": 0.10,
        "tone": 0.05,
        "geo": 0.05,
    }

    def score_pair(bmap: Dict[str, List[float]], cmap: Dict[str, List[float]]) -> Tuple[float, Dict[str, float]]:
        sims = {}
        for vt, w in weights.items():
            if vt in bmap and vt in cmap:
                sims[vt] = cosine_similarity(bmap[vt], cmap[vt])
        num = sum(sims.get(vt, 0) * w for vt, w in weights.items() if vt in sims)
        denom = sum(w for vt, w in weights.items() if vt in sims)
        return (num / denom if denom else 0.0), sims

    def warn_normalization(row: sqlite3.Row) -> List[str]:
        warnings = []
        if not (row["normalized_vertical"] or "").strip():
            warnings.append("vertical missing")
        if not (row["normalized_audience"] or "").strip():
            warnings.append("audience missing")
        if not (row["normalized_summary"] or "").strip():
            warnings.append("summary missing")
        return warnings

    def warn_sims(sims: Dict[str, float]) -> List[str]:
        warnings = []
        if sims.get("topics", 0) > 0.9 and sims.get("vertical", 0) < 0.2 and sims.get("audience", 0) < 0.2:
            warnings.append("topics dominating")
        if sims.get("tone", 0) > 0.9 and sims.get("vertical", 0) < 0.2 and sims.get("audience", 0) < 0.2:
            warnings.append("tone dominating")
        return warnings

    if args.bybrand:
        brand = conn.execute("SELECT adlid, brand_name FROM brands WHERE adlid = ?", (adlid,)).fetchone()
        if not brand:
            print(f"âŒ Brand adlid {adlid} not found.", file=sys.stderr)
            return 1
        bmap = load_embeddings_map(conn, "brand", adlid)
        if not bmap:
            print(f"âŒ No embeddings for brand adlid {adlid}. Run normalize-entity then embed-entity.", file=sys.stderr)
            return 1
        creator_rows = conn.execute(
            """
            SELECT c.podcastindex_id, COALESCE(c.title, pf.title, '') AS name,
                   c.normalized_vertical, c.normalized_audience, c.normalized_pain_points,
                   c.normalized_use_cases, c.normalized_themes, c.normalized_tone,
                   c.normalized_geo, c.normalized_topics, c.normalized_summary
            FROM creators c
            LEFT JOIN podcastindex_feeds pf ON pf.id = c.podcastindex_id
            WHERE EXISTS (SELECT 1 FROM entity_embeddings ee WHERE ee.entity_type='creator' AND ee.entity_id=c.podcastindex_id)
            """
        ).fetchall()
        scored = []
        debug_rows = []
        for r in creator_rows:
            cmap = load_embeddings_map(conn, "creator", r["podcastindex_id"])
            if not cmap:
                continue
            final_score, sims = score_pair(bmap, cmap)
            top_dims = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:2]
            notes = "; ".join([f"{d}:{v:.2f}" for d, v in top_dims])
            warn_list = warn_normalization(r) + warn_sims(sims)
            if warn_list:
                notes = notes + ("; " if notes else "") + " | ".join(warn_list)
            scored.append((r["podcastindex_id"], r["name"] or "", final_score, relevance_label(final_score), notes))
            debug_rows.append(
                {
                    "id": r["podcastindex_id"],
                    "name": r["name"] or "",
                    "sims": sims,
                    "norm": {
                        "vertical": r["normalized_vertical"],
                        "audience": r["normalized_audience"],
                        "pain_points": r["normalized_pain_points"],
                        "use_cases": r["normalized_use_cases"],
                        "themes": r["normalized_themes"],
                        "tone": r["normalized_tone"],
                        "geo": r["normalized_geo"],
                        "topics": r["normalized_topics"],
                        "summary": r["normalized_summary"],
                    },
                    "notes": notes,
                }
            )
        scored.sort(key=lambda x: x[2], reverse=True)
        rows = scored[:limit]
        print(f"Ranking by Brand: adlid={adlid}, brand=\"{brand['brand_name'] or ''}\"")
        if args.debug:
            def fmt(val):
                return f"{val:.2f}" if val is not None else ""
            header = ("id", "title", "score", "relevance", "summary", "audience", "vertical", "topics", "pain", "tone", "geo")
            dbg_rows = []
            for row in rows:
                dbg = next((d for d in debug_rows if d["id"] == row[0]), None)
                sims = dbg["sims"] if dbg else {}
                dbg_rows.append(
                    (
                        row[0],
                        row[1],
                        f"{row[2]:.3f}",
                        row[3],
                        fmt(sims.get("semantic_summary")),
                        fmt(sims.get("audience")),
                        fmt(sims.get("vertical")),
                        fmt(sims.get("topics")),
                        fmt(sims.get("pain_points")),
                        fmt(sims.get("tone")),
                        fmt(sims.get("geo")),
                    )
                )
            print(format_table(dbg_rows, header))
        else:
            table = format_table(rows, ("id", "podcast_title", "score", "relevance", "top_dims"))
            print(table)
        if args.raw:
            out_dir = Path("tests")
            out_dir.mkdir(parents=True, exist_ok=True)
            guid = datetime.utcnow().strftime("%Y%m%d%H%M%S%f")
            out_path = out_dir / f"rank-debug-brand{adlid}-{guid}.txt"
            with out_path.open("w") as f:
                f.write(f"Ranking by Brand: adlid={adlid}, brand=\"{brand['brand_name'] or ''}\"\n")
                for row in rows:
                    dbg = next((d for d in debug_rows if d["id"] == row[0]), None)
                    if not dbg:
                        continue
                    f.write(f"\nCreator {row[0]} | {row[1]}\n")
                    norm = dbg["norm"]
                    for k in ["vertical", "audience", "pain_points", "use_cases", "themes", "tone", "geo", "topics", "summary"]:
                        f.write(f"{k.upper()}: {norm.get(k) or ''}\n")
                    f.write("SIMS:\n")
                    for vt, val in sorted(dbg["sims"].items()):
                        f.write(f"  {vt}: {val:.3f}\n")
                    if dbg["notes"]:
                        f.write(f"NOTES: {dbg['notes']}\n")
            print(f"ðŸ“ Raw debug written to {out_path}")
        if getattr(args, "csv", False):
            out_name = f"{datetime.utcnow():%Y%m%d}-brand{adlid}.csv"
            with open(out_name, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["id", "podcast_title", "score", "relevance", "top_dims"])
                for r in rows:
                    writer.writerow(r)
            print(f"ðŸ’¾ Wrote {out_name}")
        conn.close()
        return 0

    if args.bycreator:
        creator = conn.execute(
            "SELECT c.podcastindex_id, COALESCE(c.title, pf.title, '') AS name, c.normalized_vertical, c.normalized_audience, c.normalized_pain_points, c.normalized_use_cases, c.normalized_themes, c.normalized_tone, c.normalized_geo, c.normalized_topics, c.normalized_summary FROM creators c LEFT JOIN podcastindex_feeds pf ON pf.id=c.podcastindex_id WHERE c.podcastindex_id = ?",
            (adlid,),
        ).fetchone()
        if not creator:
            print(f"âŒ Creator adlid {adlid} not found.", file=sys.stderr)
            return 1
        cmap = load_embeddings_map(conn, "creator", adlid)
        if not cmap:
            print(f"âŒ No embeddings for creator adlid {adlid}. Run normalize-entity then embed-entity.", file=sys.stderr)
            return 1
        brand_rows = conn.execute(
            """
            SELECT b.adlid, COALESCE(b.brand_name, '') AS brand_name,
                   b.normalized_vertical, b.normalized_audience, b.normalized_pain_points,
                   b.normalized_use_cases, b.normalized_themes, b.normalized_tone,
                   b.normalized_geo, b.normalized_topics, b.normalized_summary
            FROM brands b
            WHERE EXISTS (SELECT 1 FROM entity_embeddings ee WHERE ee.entity_type='brand' AND ee.entity_id=b.adlid)
            """
        ).fetchall()
        scored = []
        debug_rows = []
        for r in brand_rows:
            bmap = load_embeddings_map(conn, "brand", r["adlid"])
            if not bmap:
                continue
            final_score, sims = score_pair(bmap, cmap)
            top_dims = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:2]
            notes = "; ".join([f"{d}:{v:.2f}" for d, v in top_dims])
            warn_list = warn_normalization(r) + warn_sims(sims)
            if warn_list:
                notes = notes + ("; " if notes else "") + " | ".join(warn_list)
            scored.append((r["adlid"], r["brand_name"] or "", final_score, relevance_label(final_score), notes))
            debug_rows.append(
                {
                    "id": r["adlid"],
                    "name": r["brand_name"] or "",
                    "sims": sims,
                    "norm": {
                        "vertical": r["normalized_vertical"],
                        "audience": r["normalized_audience"],
                        "pain_points": r["normalized_pain_points"],
                        "use_cases": r["normalized_use_cases"],
                        "themes": r["normalized_themes"],
                        "tone": r["normalized_tone"],
                        "geo": r["normalized_geo"],
                        "topics": r["normalized_topics"],
                        "summary": r["normalized_summary"],
                    },
                    "notes": notes,
                }
            )
        scored.sort(key=lambda x: x[2], reverse=True)
        rows = scored[:limit]
        print(f"Ranking by Creator: adlid={adlid}, podcast=\"{creator['name'] or ''}\"")
        if args.debug:
            def fmt(val):
                return f"{val:.2f}" if val is not None else ""
            header = ("id", "brand_name", "score", "relevance", "summary", "audience", "vertical", "topics", "pain", "tone", "geo")
            dbg_rows = []
            for row in rows:
                dbg = next((d for d in debug_rows if d["id"] == row[0]), None)
                sims = dbg["sims"] if dbg else {}
                dbg_rows.append(
                    (
                        row[0],
                        row[1],
                        f"{row[2]:.3f}",
                        row[3],
                        fmt(sims.get("semantic_summary")),
                        fmt(sims.get("audience")),
                        fmt(sims.get("vertical")),
                        fmt(sims.get("topics")),
                        fmt(sims.get("pain_points")),
                        fmt(sims.get("tone")),
                        fmt(sims.get("geo")),
                    )
                )
            print(format_table(dbg_rows, header))
        else:
            table = format_table(rows, ("id", "brand_name", "score", "relevance", "notes"))
            print(table)
        if args.raw:
            out_dir = Path("tests")
            out_dir.mkdir(parents=True, exist_ok=True)
            guid = datetime.utcnow().strftime("%Y%m%d%H%M%S%f")
            out_path = out_dir / f"rank-debug-creator{adlid}-{guid}.txt"
            with out_path.open("w") as f:
                f.write(f"Ranking by Creator: adlid={adlid}, podcast=\"{creator['name'] or ''}\"\n")
                for row in rows:
                    dbg = next((d for d in debug_rows if d["id"] == row[0]), None)
                    if not dbg:
                        continue
                    f.write(f"\nBrand {row[0]} | {row[1]}\n")
                    norm = dbg["norm"]
                    for k in ["vertical", "audience", "pain_points", "use_cases", "themes", "tone", "geo", "topics", "summary"]:
                        f.write(f"{k.upper()}: {norm.get(k) or ''}\n")
                    f.write("SIMS:\n")
                    for vt, val in sorted(dbg["sims"].items()):
                        f.write(f"  {vt}: {val:.3f}\n")
                    if dbg["notes"]:
                        f.write(f"NOTES: {dbg['notes']}\n")
            print(f"ðŸ“ Raw debug written to {out_path}")
        if getattr(args, "csv", False):
            out_name = f"{datetime.utcnow():%Y%m%d}-creator{adlid}.csv"
            with open(out_name, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["id", "brand_name", "score", "relevance", "notes"])
                for r in rows:
                    writer.writerow(r)
            print(f"ðŸ’¾ Wrote {out_name}")
        conn.close()
        return 0

    print("âŒ Must specify --bybrand or --bycreator", file=sys.stderr)
    conn.close()
    return 1


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Adelined data CLI")
    subparsers = parser.add_subparsers(dest="action", required=True)

    fetch_parser = subparsers.add_parser("fetch", help="Fetch creator data from a source")
    fetch_parser.add_argument(
        "source",
        choices=["youtube", "spotify"],
        help="Data source to pull from",
    )
    fetch_parser.add_argument(
        "--output",
        type=Path,
        help="Optional JSON output file to write results",
    )

    # YouTube options
    fetch_parser.add_argument(
        "--channels",
        help="Comma-separated YouTube channel handles/IDs/URLs to fetch (for source youtube)",
    )
    fetch_parser.add_argument(
        "--channels-file",
        type=Path,
        help="Path to newline-delimited list of YouTube channel handles/IDs/URLs",
    )

    # Spotify options
    fetch_parser.add_argument(
        "--shows",
        help="Comma-separated Spotify show IDs or URLs to fetch (for source spotify)",
    )
    fetch_parser.add_argument(
        "--shows-file",
        type=Path,
        help="Path to newline-delimited list of Spotify show IDs or URLs",
    )

    ingest_parser = subparsers.add_parser("ingest", help="Ingest data into the Adelined DB")
    ingest_parser.add_argument(
        "source",
        choices=["podcastindex", "apple"],
        help="Dataset to ingest",
    )
    ingest_parser.add_argument(
        "--source-db",
        type=Path,
        default=Path("data/podcastindex_feeds.db"),
        help="Path to source SQLite database (default: data/podcastindex_feeds.db)",
    )
    ingest_parser.add_argument(
        "--target-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to target Adelined SQLite database (default: adelined_matching.db)",
    )
    ingest_parser.add_argument(
        "--refresh-feeds",
        action="store_true",
        help="Refresh podcastindex_feeds from the source DB before filtering into creators (default: use existing feeds only)",
    )
    ingest_parser.add_argument(
        "--batch-size",
        type=int,
        default=5000,
        help="Batch size for streaming copy (default: 5000)",
    )

    enrich_parser = subparsers.add_parser("enrich", help="Enrich creators with external signals")
    enrich_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    enrich_parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="Max creators to process in this run (default: 100)",
    )
    enrich_parser.add_argument(
        "--offset",
        type=int,
        default=0,
        help="Offset into creators table (default: 0)",
    )
    enrich_parser.add_argument(
        "--only-id",
        type=int,
        help="Optional specific podcastindex_id to process",
    )
    enrich_parser.add_argument(
        "--pidrange",
        nargs=2,
        type=int,
        metavar=("START_PID", "END_PID"),
        help="Optional inclusive podcastindex_id range to process (START_PID END_PID)",
    )
    enrich_parser.add_argument(
        "--sources",
        help="Comma-separated sources to fetch (default: all). Options: spotify,apple,youtube,social",
    )
    enrich_parser.add_argument(
        "--from-podscan",
        action="store_true",
        help="Build enrichment from Podscan JSON for creators missing enrichment (no external API calls)",
    )
    enrich_parser.add_argument(
        "--auto-norm",
        action="store_true",
        help="Normalize creators as they are enriched (writes entity_normalized + creators.normalized_*)",
    )

    discover_parser = subparsers.add_parser("discover", help="Discover new creators from external sources")
    discover_parser.add_argument(
        "source",
        choices=["apple"],
        help="Discovery source",
    )
    discover_parser.add_argument(
        "--terms",
        help="Comma-separated search terms",
    )
    discover_parser.add_argument(
        "--terms-file",
        type=Path,
        help="File with search terms (one per line)",
    )
    discover_parser.add_argument(
        "--limit-per-term",
        type=int,
        default=200,
        help="Results per page (max 200)",
    )
    discover_parser.add_argument(
        "--max-pages",
        type=int,
        default=5,
        help="Max pages per term",
    )
    discover_parser.add_argument(
        "--country",
        default="GB",
        help="iTunes country code (default: GB)",
    )
    discover_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    report_parser = subparsers.add_parser("enrichment", help="Manage or inspect enrichment data")
    report_sub = report_parser.add_subparsers(dest="enrichment_action", required=True)

    clear_parser = report_sub.add_parser("clear", help="Clear creator_enrichment table (requires confirmation)")
    clear_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    show_parser = report_sub.add_parser("show", help="Show enrichment summaries or source data")
    show_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    show_parser.add_argument(
        "--source",
        choices=["spotify", "apple", "youtube", "social", "twitter", "instagram", "website", "patreon"],
        help="Specific source to list; omit for summary counts",
    )
    show_parser.add_argument(
        "--limit",
        type=int,
        default=5,
        help="Max rows to display (default: 5)",
    )

    summary_parser = report_sub.add_parser("summary", help="Report overall enrichment coverage")
    summary_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    static_parser = subparsers.add_parser("static", help="Load static reference data")
    static_sub = static_parser.add_subparsers(dest="static_action", required=True)
    load_parser = static_sub.add_parser("load", help="Load static data sets")
    load_parser.add_argument(
        "--apple-topics",
        action="store_true",
        help="Load Apple podcast categories into apple_podcast_categories table",
    )
    load_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    topics_parser = static_sub.add_parser("creator-topics", help="Assign Apple topics to creators using local LLM")
    topics_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    topics_parser.add_argument("--limit", type=int, default=200, help="Max creators to process")
    topics_parser.add_argument("--offset", type=int, default=0, help="Offset into creators table")
    topics_parser.add_argument(
        "--ollama-model",
        default=LOCAL_OLLAMA_MODEL,
        help="Ollama model name for topic assignment (default from OLLAMA_MODEL or phi3.5)",
    )
    topics_parser.add_argument(
        "--nolimit",
        action="store_true",
        help="Process all creators (ignore limit/offset) â€” requires confirmation",
    )

    brand_topics_parser = static_sub.add_parser("brand-topics", help="Assign Apple topics to brands using local LLM")
    brand_topics_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    brand_topics_parser.add_argument("--limit", type=int, default=200, help="Max brands to process")
    brand_topics_parser.add_argument("--offset", type=int, default=0, help="Offset into brands table")
    brand_topics_parser.add_argument(
        "--openai",
        action="store_true",
        help="Use OpenAI instead of local (default local Ollama)",
    )
    brand_topics_parser.add_argument(
        "--ollama-model",
        default=LOCAL_OLLAMA_MODEL,
        help="Ollama model name for topic assignment (default from OLLAMA_MODEL or phi3.5)",
    )
    brand_topics_parser.add_argument(
        "--nolimit",
        action="store_true",
        help="Process all brands (ignore limit/offset) â€” requires confirmation",
    )

    brand_parser = subparsers.add_parser("brand", help="Ingest a brand by URL")
    brand_parser.add_argument(
        "--url",
        required=True,
        help="Brand website URL",
    )
    brand_parser.add_argument(
        "--brand-name",
        help="Optional brand name override",
    )
    brand_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    brand_parser.add_argument(
        "--write-test",
        action="store_true",
        help="Write test brand embedding text to tests/test-enrich-brand-adlid-<id>_cleaned.txt",
    )

    norm_entity_parser = subparsers.add_parser("normalize-entity", help="Normalize a brand or creator into structured fields")
    norm_entity_parser.add_argument(
        "--entity-type",
        required=True,
        choices=["brand", "creator"],
        help="Which entity type to normalize",
    )
    norm_entity_parser.add_argument("--id", type=int, help="Specific id (adlid for brand, podcastindex_id for creator)")
    norm_entity_parser.add_argument("--all-brands", action="store_true", help="Normalize all brands (paged with limit/offset)")
    norm_entity_parser.add_argument("--all-creators", action="store_true", help="Normalize all creators (paged with limit/offset)")
    norm_entity_parser.add_argument(
        "--id-range",
        nargs=2,
        type=int,
        metavar=("START_ID", "END_ID"),
        help="Normalize a contiguous id range (inclusive). Uses adlid for brands, podcastindex_id for creators.",
    )
    norm_entity_parser.add_argument(
        "--podcastindex_ids",
        nargs="+",
        type=int,
        help="Space-delimited list of podcastindex_id values to process (creator-only)",
    )
    norm_entity_parser.add_argument("--limit", type=int, default=100, help="Max rows to process (default 100)")
    norm_entity_parser.add_argument("--offset", type=int, default=0, help="Offset for bulk mode")
    norm_entity_parser.add_argument(
        "--llm-provider",
        choices=["local", "openai"],
        default="local",
        help="Provider for normalization LLM (default: local via Ollama)",
    )
    norm_entity_parser.add_argument(
        "--ollama-model",
        default=LOCAL_OLLAMA_MODEL,
        help="Ollama model name for local normalization (default from OLLAMA_MODEL or phi3.5)",
    )
    norm_entity_parser.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing normalized rows for these entities",
    )
    norm_entity_parser.add_argument(
        "--podscan",
        action="store_true",
        help="Use Podscan podcast_description as the sole source for creator normalization",
    )
    norm_entity_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    embed_entity_parser = subparsers.add_parser("embed-entity", help="Embed normalized brands or creators into per-dimension vectors")
    embed_entity_parser.add_argument(
        "--entity-type",
        required=True,
        choices=["brand", "creator"],
        help="Which entity type to embed",
    )
    embed_entity_parser.add_argument("--id", type=int, help="Specific id (adlid for brand, podcastindex_id for creator)")
    embed_entity_parser.add_argument(
        "--id-range",
        nargs=2,
        type=int,
        metavar=("START_ID", "END_ID"),
        help="Contiguous id range (inclusive) for delete operations",
    )
    embed_entity_parser.add_argument("--limit", type=int, default=100, help="Max rows to embed (default 100)")
    embed_entity_parser.add_argument("--offset", type=int, default=0, help="Offset for bulk mode")
    embed_entity_parser.add_argument(
        "--refreshall",
        action="store_true",
        help="Delete existing embeddings for this entity type before embedding (requires typing YES)",
    )
    embed_entity_parser.add_argument(
        "--delete",
        action="store_true",
        help="Delete embeddings for --id or --id-range instead of embedding",
    )
    embed_entity_parser.add_argument(
        "--deleteall",
        action="store_true",
        help="Delete ALL embeddings for this entity type (requires typing YES)",
    )
    embed_entity_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    embed_entity_parser.add_argument(
        "--embedding-provider",
        choices=["openai", "local"],
        default=None,
        help="Provider for embeddings (default from LLM_PROVIDER_EMBED or local via Ollama; use openai to call OpenAI embeddings)",
    )
    embed_entity_parser.add_argument(
        "--openai",
        action="store_true",
        help="Shortcut to set --embedding-provider openai",
    )
    embed_entity_parser.add_argument(
        "--embedding-model",
        default=LOCAL_OLLAMA_EMBED_MODEL,
        help="Embedding model name (local default: bge-large:335m-en-v1.5-fp16 via OLLAMA_EMBED_MODEL; openai default: text-embedding-3-large)",
    )

    export_parser = subparsers.add_parser("export", help="Export a stored embedding text block to a file")
    export_parser.add_argument(
        "--target",
        required=True,
        choices=["creator", "brand"],
        help="Which entity to export",
    )
    export_parser.add_argument(
        "--id",
        required=True,
        type=int,
        help="podcastindex_id for creator or adlid for brand",
    )
    export_parser.add_argument(
        "--out",
        type=Path,
        help="Output file path (default: tests/export-<target>-<id>.txt)",
    )
    export_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    export_parser.add_argument(
        "--usenorm",
        action="store_true",
        help="If set, export normalized text when available",
    )

    rank_parser = subparsers.add_parser("rank", help="Rank creators vs brands via embeddings")
    rank_mode = rank_parser.add_mutually_exclusive_group(required=True)
    rank_mode.add_argument("--bybrand", action="store_true", help="Rank creators for a brand")
    rank_mode.add_argument("--bycreator", action="store_true", help="Rank brands for a creator")
    rank_parser.add_argument(
        "--adlid",
        type=int,
        required=True,
        help="adlid of the brand (when --bybrand) or creator (when --bycreator)",
    )
    rank_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    rank_parser.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Number of top results to show (default: 20)",
    )
    rank_parser.add_argument(
        "--debug",
        action="store_true",
        help="Show per-dimension similarity columns and warnings",
    )
    rank_parser.add_argument(
        "--raw",
        action="store_true",
        help="Dump normalized fields and sims for each candidate",
    )
    rank_parser.add_argument(
        "--csv",
        action="store_true",
        help="If set, also write results to CSV (yyyyMMdd-brand{n}.csv or creator{n}.csv)",
    )

    test_compare = subparsers.add_parser("test-compare", help="Compare two text inputs via embeddings")
    test_compare.add_argument("--text1", help="First text input")
    test_compare.add_argument("--text2", help="Second text input")
    test_compare.add_argument("--file1", type=Path, help="Path to first text file")
    test_compare.add_argument("--file2", type=Path, help="Path to second text file")
    test_compare.add_argument("--brand-id", type=int, help="Compare using brand embeddings (adlid)")
    test_compare.add_argument("--creator-id", type=int, help="Compare using creator embeddings (podcastindex_id)")
    test_compare.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    test_enrich = subparsers.add_parser("test-enrich", help="Dump enrichment for a creator")
    test_enrich.add_argument(
        "--pid",
        type=int,
        required=True,
        help="podcastindex_id (from creators) to inspect",
    )
    test_enrich.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    find_country = subparsers.add_parser(
        "find-creator-country",
        help="Lookup and store creator country via Podscan (itunesId)",
    )
    find_country.add_argument(
        "--limit",
        type=int,
        default=50,
        help="Max rows to process (default: 50; set 0 for no limit)",
    )
    find_country.add_argument(
        "--print-only",
        action="store_true",
        help="Print resolved countries without writing to the database",
    )
    find_country.add_argument(
        "--pid",
        type=int,
        help="Specific podcastindex_id to lookup (forces print-only)",
    )
    find_country.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    update_podscan = subparsers.add_parser(
        "update_from_podscan",
        help="Pull podcasts from Podscan search and store in the podscan table",
    )
    update_podscan.add_argument(
        "--limit",
        type=int,
        default=50,
        help="Max results to store (default: 50; set 0 for no limit)",
    )
    update_podscan.add_argument(
        "--per_page",
        type=int,
        default=5,
        help="Results per page (default: 5; max: 50)",
    )
    update_podscan.add_argument(
        "--order_by",
        default="created_at",
        choices=[
            "best_match",
            "name",
            "created_at",
            "episode_count",
            "rating",
            "audience_size",
            "last_posted_at",
        ],
        help="Field to order by (default: created_at)",
    )
    update_podscan.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing podscan rows instead of skipping duplicates",
    )
    update_podscan.add_argument(
        "--month_only",
        help="Restrict last_posted_at to a month like 1_25 (Jan 2025)",
    )
    update_podscan.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    return parser.parse_args(argv)


def handle_fetch_youtube(args: argparse.Namespace) -> int:
    channel_inputs = load_list_inputs(args.channels, args.channels_file)
    if not channel_inputs:
        print("âŒ Please provide YouTube channels via --channels or --channels-file", file=sys.stderr)
        return 1

    fetcher = YouTubeChannelFetcher(channels=channel_inputs, delay=args.delay)
    print(f"Fetching YouTube channels: {channel_inputs}")
    records = fetcher.fetch()
    print(f"âœ… Retrieved {len(records)} YouTube channels.")

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        payload = [record.__dict__ for record in records]
        args.output.write_text(json.dumps(payload, indent=2))
        print(f"ðŸ’¾ Wrote JSON: {args.output}")

    preview = records[:5]
    if preview:
        print("Sample rows:")
        for rec in preview:
            print(f" â€¢ {rec.title} â€” {rec.channel_url}")
    return 0


def handle_fetch_spotify(args: argparse.Namespace) -> int:
    show_inputs = load_list_inputs(args.shows, args.shows_file)
    if not show_inputs:
        print("âŒ Please provide Spotify shows via --shows or --shows-file", file=sys.stderr)
        return 1

    fetcher = SpotifyShowFetcher(shows=show_inputs, delay=args.delay)
    print(f"Fetching Spotify shows: {show_inputs}")
    records = fetcher.fetch()
    print(f"âœ… Retrieved {len(records)} Spotify shows.")

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        payload = [record.__dict__ for record in records]
        args.output.write_text(json.dumps(payload, indent=2))
        print(f"ðŸ’¾ Wrote JSON: {args.output}")

    preview = records[:5]
    if preview:
        print("Sample rows:")
        for rec in preview:
            print(f" â€¢ {rec.title} â€” {rec.show_url}")
    return 0


def main(argv: Optional[Sequence[str]] = None) -> int:
    if argv is None:
        argv = sys.argv[1:]

    if len(argv) == 1 and argv[0].lower() == "help":
        print_help()
        return 0

    args = parse_args(argv)
    if args.action == "fetch":
        if args.source == "youtube":
            return handle_fetch_youtube(args)
        if args.source == "spotify":
            return handle_fetch_spotify(args)
    if args.action == "ingest":
        if args.source == "podcastindex":
            return ingest_podcastindex_feeds(args)
        if args.source == "apple":
            return ingest_apple_discovery(args)
    if args.action == "enrich":
        return enrich_creators(args)
    if args.action == "enrichment":
        if args.enrichment_action == "clear":
            return clear_enrichment(args)
        if args.enrichment_action == "show":
            return show_enrichment(args)
        if args.enrichment_action == "summary":
            return summary_enrichment(args)
    if args.action == "discover":
        if args.source == "apple":
            return discover_apple(args)
    if args.action == "brand":
        return ingest_brand(args)
    if args.action == "normalize-entity":
        return normalize_entities(args)
    if args.action == "embed-entity":
        return embed_entity(args)
    if args.action == "static":
        if args.static_action == "load":
            return load_static(args)
        if args.static_action == "creator-topics":
            return assign_creator_topics(args)
        if args.static_action == "brand-topics":
            return assign_brand_topics(args)
    if args.action == "rank":
        return rank_entities(args)
    if args.action == "test-compare":
        return test_compare(args)
    if args.action == "test-enrich":
        return test_enrich(args)
    if args.action == "export":
        return export_entity(args)
    if args.action == "find-creator-country":
        return find_creator_country(args)
    if args.action == "update_from_podscan":
        return update_from_podscan(args)
    source = getattr(args, "source", "")
    print(f"Unknown action/source combination: {args.action} {source}")
    return 1


def print_help() -> None:
    help_text = """
Adelined CLI

Commands:
  python3 adl fetch youtube [options]
  python3 adl fetch spotify [options]
  python3 adl ingest podcastindex [options]
  python3 adl discover apple [options]
  python3 adl enrich [options]
  python3 adl enrichment clear [options]
  python3 adl enrichment show [options]
  python3 adl enrichment summary [options]
  python3 adl brand --url <website>
  python3 adl normalize-entity --entity-type brand|creator [--id <id> | --all-brands/--all-creators]
  python3 adl embed-entity --entity-type brand|creator [--id <id> | --limit N --offset M]
  python3 adl static load --apple-topics
  python3 adl static creator-topics [--limit N --offset M | --nolimit]
  python3 adl static brand-topics [--limit N --offset M | --nolimit]
  python3 adl rank --bybrand/--bycreator --adlid <id> [options]
  python3 adl test-compare --text1/--file1 --text2/--file2
  python3 adl test-enrich --pid <podcastindex_id>
  python3 adl export --target creator|brand --id <id> [--out path]
  python3 adl find-creator-country [--limit N]
  python3 adl update_from_podscan [--limit N] [--per_page N] [--order_by field]

Examples:
  # YouTube: fetch channel metadata by handles
  python3 adl fetch youtube --channels @lexfridman,@hubermanlab

  # YouTube: from file (one handle/URL/channel_id per line)
  python3 adl fetch youtube --channels-file channels.txt --output cache/youtube.json

  # Spotify: fetch show metadata by show IDs
  python3 adl fetch spotify --shows 5AvwZVawapvyhJUIx71pdJ,4rOoJ6Egrf8K2IrywzwOMk

  # Spotify: from URLs file
  python3 adl fetch spotify --shows-file spotify_urls.txt --output cache/spotify.json

  # Ingest PodcastIndex snapshot into Adelined DB (itunesId dedupe)
  python3 adl ingest podcastindex --source-db data/podcastindex_feeds.db --target-db adelined_matching.db

  # Discover Apple shows via iTunes Search API (GB)
  python3 adl discover apple --terms "comedy,news" --limit-per-term 200 --max-pages 5 --country GB --adl-db adelined_matching.db

  # Enrich a slice of creators
  python3 adl enrich --adl-db adelined_matching.db --limit 50 --offset 0
  python3 adl enrich --adl-db adelined_matching.db --only-id 12345

  # Normalize and embed (multi-vector)
  python3 adl normalize-entity --entity-type brand --all-brands --limit 50
  python3 adl embed-entity --entity-type brand --limit 50
  # Ingest a brand (scrape + GPT extract)
  python3 adl brand --url https://brandsite.com
  # Embed creators or brands (OpenAI embedding)
  python3 adl embed creators --adl-db adelined_matching.db --limit 100
  python3 adl embed brands --adl-db adelined_matching.db --limit 50
Options:
  --output    Optional JSON file to write scraped records
  --source-db Source SQLite path for ingest (default: data/podcastindex_feeds.db)
  --target-db Target Adelined SQLite path (default: adelined_matching.db)
  --batch-size Batch size for ingest (default: 5000)
  --limit     Max creators to enrich per run (default: 100)
  --offset    Offset into creators table (default: 0)
  --only-id   Enrich only this podcastindex_id
  --limit-per-term Apple discovery results per page (default: 200)
  --max-pages     Apple discovery max pages per term (default: 5)
  --limit     Max rows to embed (default: 100)
  --offset    Offset rows to embed (default: 0)
"""
    print(help_text.strip())


if __name__ == "__main__":
    sys.exit(main())
