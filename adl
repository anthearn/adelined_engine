#!/usr/bin/env python3
"""
Adelined data CLI.

Usage example:
    python3 adl fetch youtube --channels @lexfridman,@hubermanlab --output cache/youtube.json
"""

import argparse
import json
import csv
import os
import re
import sqlite3
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import requests
from bs4 import BeautifulSoup
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

# OpenAI confirmation (one per command run)
OPENAI_CONFIRMED = False

LOCAL_OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "phi3.5")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")

PODCASTINDEX_COLUMNS: Sequence[str] = [
    "id",
    "url",
    "title",
    "lastUpdate",
    "link",
    "lastHttpStatus",
    "dead",
    "contentType",
    "itunesId",
    "originalUrl",
    "itunesAuthor",
    "itunesOwnerName",
    "explicit",
    "imageUrl",
    "itunesType",
    "generator",
    "newestItemPubdate",
    "language",
    "oldestItemPubdate",
    "episodeCount",
    "popularityScore",
    "priority",
    "createdOn",
    "updateFrequency",
    "chash",
    "host",
    "newestEnclosureUrl",
    "podcastGuid",
    "description",
    "category1",
    "category2",
    "category3",
    "category4",
    "category5",
    "category6",
    "category7",
    "category8",
    "category9",
    "category10",
    "newestEnclosureDuration",
]


@dataclass
class YouTubeChannelRecord:
    channel_url: str
    title: str
    description: str
    about: str
    subscribers: Optional[str]
    scraped_at: str


# --- LLM / Embedding abstraction wrappers ---
class LLMClient:
    def __init__(self, provider: str = "openai", model: str = "gpt-4o-mini"):
        self.provider = provider
        self.model = model
        self.api_key = os.getenv("OPENAI_API_KEY")

    def normalize_entity(self, entity_type: str, name: str, fields_text: str) -> Optional[Dict]:
        if self.provider != "openai" or not self.api_key:
            return None
        client = OpenAI(api_key=self.api_key)
        schema = (
            "{"
            "\"summary\":\"...\","
            "\"vertical\":\"...\","
            "\"audience\":\"...\","
            "\"pain_points\":\"...\","
            "\"use_cases\":\"...\","
            "\"themes\":\"...\","
            "\"tone\":\"...\","
            "\"geo\":\"...\","
            "\"topics\":[{\"apple_category_name\":\"...\",\"apple_subcategory_name\":\"...\",\"apple_category_id\":\"...\",\"apple_subcategory_id\":\"...\"}]"
            "}"
        )
        prompt = (
            "You are a classification and summarisation assistant.\n"
            "Normalize noisy marketing/technical text about an entity into a simple schema, using plain language a non-expert would understand.\n"
            "Return JSON ONLY with keys: summary, vertical, audience, pain_points, use_cases, themes, tone, geo, topics (array as specified).\n"
            "Be concise, high-signal, no fluff. Use common industry labels.\n\n"
            f"ENTITY_TYPE: {entity_type}\n"
            f"NAME: {name}\n"
            f"RAW FIELDS:\n{fields_text}\n\n"
            "Output JSON in this structure:\n"
            + schema
        )
        try:
            resp = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"},
            )
            return json.loads(resp.choices[0].message.content)
        except Exception as exc:
            print(f"‚ö†Ô∏è Normalization failed: {exc}", file=sys.stderr)
            return None

    def explain_match(self, brand_profile: Dict, creator_profile: Dict, scores: Dict[str, float]) -> Optional[str]:
        if self.provider != "openai" or not self.api_key:
            return None
        if not ensure_openai_confirmation("match explanation", allow_fallback=False):
            return None
        client = OpenAI(api_key=self.api_key)
        prompt = (
            "Given the normalized profiles of a brand and a creator plus per-dimension similarity scores, provide a short rationale (2 sentences) "
            "on commercial fit. Keep it factual and concise.\n\n"
            f"BRAND: {json.dumps(brand_profile)}\nCREATOR: {json.dumps(creator_profile)}\nSCORES: {json.dumps(scores)}"
        )
        try:
            resp = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            return resp.choices[0].message.content
        except Exception:
            return None


class EmbeddingClient:
    def __init__(self, provider: str = "openai", model: str = "text-embedding-3-large"):
        self.provider = provider
        self.model = model
        self.api_key = os.getenv("OPENAI_API_KEY")

    def embed(self, texts: List[str]) -> List[Optional[List[float]]]:
        if self.provider != "openai" or not self.api_key:
            return [None for _ in texts]
        client = OpenAI(api_key=self.api_key)
        try:
            resp = client.embeddings.create(model=self.model, input=texts)
            return [item.embedding for item in resp.data]
        except Exception as exc:
            print(f"‚ö†Ô∏è Embedding batch failed: {exc}", file=sys.stderr)
            return [None for _ in texts]


def ensure_openai_confirmation(action: str, allow_fallback: bool = False) -> bool:
    """
    Returns True if OpenAI can be used (confirmed), False if fallback should occur.
    """
    global OPENAI_CONFIRMED
    if OPENAI_CONFIRMED:
        return True
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        print("‚ùå OPENAI_API_KEY not set.", file=sys.stderr)
        return False
    prompt = (
        f"‚ö†Ô∏è You are about to use the OpenAI API for: {action}. This may incur cost.\n"
        'Type YES to continue, anything else to abort: '
    )
    resp = input(prompt).strip()
    if resp == "YES":
        OPENAI_CONFIRMED = True
        return True
    if allow_fallback:
        print("‚Ü©Ô∏è OpenAI not confirmed; falling back to local provider.")
        return False
    print("Aborted; OpenAI not confirmed.")
    return False


def ensure_ollama_running() -> bool:
    try:
        res = requests.get(f"{OLLAMA_HOST}/api/version", timeout=5)
        res.raise_for_status()
        return True
    except Exception:
        print('‚ùå Ollama server not running ‚Äî start with "ollama serve"', file=sys.stderr)
        return False


@dataclass
class SpotifyShowRecord:
    show_url: str
    title: str
    author: str
    description: str
    thumbnail: Optional[str]
    scraped_at: str


class YouTubeChannelFetcher:
    def __init__(
        self,
        channels: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.channels = channels
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch(self) -> List[YouTubeChannelRecord]:
        results: List[YouTubeChannelRecord] = []
        for raw in self.channels:
            url = self._normalise_channel_url(raw)
            about_url = url.rstrip("/") + "/about"
            html = self._get(about_url)
            if not html:
                continue
            record = self._parse_about_page(url, html)
            if record:
                results.append(record)
            if self.delay:
                time.sleep(self.delay)
        return results

    def _normalise_channel_url(self, identifier: str) -> str:
        ident = identifier.strip()
        if ident.startswith("http://") or ident.startswith("https://"):
            return ident
        if ident.startswith("@"):
            return f"https://www.youtube.com/{ident}"
        if ident.startswith("UC"):
            return f"https://www.youtube.com/channel/{ident}"
        return f"https://www.youtube.com/@{ident}"

    def _get(self, url: str) -> Optional[str]:
        try:
            res = self.session.get(url, timeout=20)
            res.raise_for_status()
            return res.text
        except Exception as exc:
            print(f"‚ùå Failed to fetch {url}: {exc}", file=sys.stderr)
            return None

    def _parse_about_page(self, channel_url: str, html: str) -> Optional[YouTubeChannelRecord]:
        soup = BeautifulSoup(html, "html.parser")
        title = (soup.find("meta", {"property": "og:title"}) or {}).get("content", "")
        description = (soup.find("meta", {"property": "og:description"}) or {}).get(
            "content", ""
        )

        about_text = ""
        about_el = soup.select_one("#description-container") or soup.select_one(
            "#description"
        )
        if about_el:
            about_text = about_el.get_text(separator=" ", strip=True)

        subscriber_text = None
        sub_el = soup.select_one("#subscriber-count")
        if sub_el:
            subscriber_text = sub_el.get_text(strip=True)

        scraped_at = datetime.utcnow().isoformat()
        if not title and not description and not about_text:
            return None
        return YouTubeChannelRecord(
            channel_url=channel_url,
            title=title,
            description=description,
            about=about_text,
            subscribers=subscriber_text,
            scraped_at=scraped_at,
        )


class SpotifyShowFetcher:
    def __init__(
        self,
        shows: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.shows = shows
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch(self) -> List[SpotifyShowRecord]:
        results: List[SpotifyShowRecord] = []
        for raw in self.shows:
            url = self._normalise_show_url(raw)
            html = self._get(url)
            if not html:
                continue
            oembed = self._get_oembed(url)
            record = self._parse_show_page(url, html, oembed)
            if record:
                results.append(record)
            if self.delay:
                time.sleep(self.delay)
        return results

    def _normalise_show_url(self, identifier: str) -> str:
        ident = identifier.strip()
        if ident.startswith("http://") or ident.startswith("https://"):
            return ident
        return f"https://open.spotify.com/show/{ident}"

    def _get(self, url: str) -> Optional[str]:
        try:
            res = self.session.get(url, timeout=20)
            res.raise_for_status()
            return res.text
        except Exception as exc:
            print(f"‚ùå Failed to fetch {url}: {exc}", file=sys.stderr)
            return None

    def _get_oembed(self, url: str) -> Dict:
        try:
            res = self.session.get(
                "https://open.spotify.com/oembed",
                params={"url": url},
                timeout=15,
            )
            res.raise_for_status()
            return res.json()
        except Exception:
            return {}

    def _parse_show_page(self, show_url: str, html: str, oembed: Dict) -> Optional[SpotifyShowRecord]:
        soup = BeautifulSoup(html, "html.parser")
        title = oembed.get("title") or (soup.find("meta", {"property": "og:title"}) or {}).get(
            "content", ""
        )
        description = (
            (soup.find("meta", {"property": "og:description"}) or {}).get("content", "")
        )
        author = oembed.get("author_name", "")
        thumbnail = oembed.get("thumbnail_url") or (soup.find("meta", {"property": "og:image"}) or {}).get(
            "content"
        )

        scraped_at = datetime.utcnow().isoformat()
        if not title and not description:
            return None
        return SpotifyShowRecord(
            show_url=show_url,
            title=title or "",
            author=author or "",
            description=description or "",
            thumbnail=thumbnail,
            scraped_at=scraped_at,
        )


def ensure_podcastindex_tables(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS podcastindex_feeds (
            id INTEGER PRIMARY KEY,
            url TEXT,
            title TEXT,
            lastUpdate INTEGER,
            link TEXT,
            lastHttpStatus INTEGER,
            dead INTEGER,
            contentType TEXT,
            itunesId INTEGER,
            originalUrl TEXT,
            itunesAuthor TEXT,
            itunesOwnerName TEXT,
            explicit INTEGER,
            imageUrl TEXT,
            itunesType TEXT,
            generator TEXT,
            newestItemPubdate INTEGER,
            language TEXT,
            oldestItemPubdate INTEGER,
            episodeCount INTEGER,
            popularityScore INTEGER,
            priority INTEGER,
            createdOn INTEGER,
            updateFrequency INTEGER,
            chash TEXT,
            host TEXT,
            newestEnclosureUrl TEXT,
            podcastGuid TEXT,
            description TEXT,
            category1 TEXT,
            category2 TEXT,
            category3 TEXT,
            category4 TEXT,
            category5 TEXT,
            category6 TEXT,
            category7 TEXT,
            category8 TEXT,
            category9 TEXT,
            category10 TEXT,
            newestEnclosureDuration INTEGER
        )
        """
    )
    conn.execute("CREATE INDEX IF NOT EXISTS idx_podcastindex_feeds_itunesId ON podcastindex_feeds(itunesId)")


def ensure_creators_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creators (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            podcastindex_id INTEGER,
            podcastguid TEXT,
            itunesid INTEGER,
            language TEXT,
            normalized_summary TEXT,
            normalized_vertical TEXT,
            normalized_audience TEXT,
            normalized_pain_points TEXT,
            normalized_use_cases TEXT,
            normalized_themes TEXT,
            normalized_tone TEXT,
            normalized_geo TEXT,
            normalized_topics TEXT,
            title TEXT,
            description TEXT,
            feed_url TEXT,
            web_link TEXT,
            categories TEXT,
            newestItemPubdate INTEGER,
            oldestItemPubdate INTEGER,
            episodeCount INTEGER,
            updateFrequency INTEGER,
            vertical TEXT,
            audience_profile TEXT,
            industry_keywords TEXT,
            business_functions TEXT,
            pain_points TEXT,
            geo_focus TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_creators_pid ON creators(podcastindex_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_creators_itunesid ON creators(itunesid)")
    for col, coltype in [
        ("normalized_summary", "TEXT"),
        ("normalized_vertical", "TEXT"),
        ("normalized_audience", "TEXT"),
        ("normalized_pain_points", "TEXT"),
        ("normalized_use_cases", "TEXT"),
        ("normalized_themes", "TEXT"),
        ("normalized_tone", "TEXT"),
        ("normalized_geo", "TEXT"),
        ("normalized_topics", "TEXT"),
    ]:
        ensure_column(conn, "creators", col, coltype)


def ensure_creator_enrichment_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_enrichment (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            podcastindex_id INTEGER,
            uk_score REAL,
            source_spotify TEXT,
            source_apple TEXT,
            source_youtube TEXT,
            source_social_twitter TEXT,
            source_social_instagram TEXT,
            source_social_website TEXT,
            source_social_patreon TEXT,
            cleaned_text_block TEXT,
            raw_text_sources TEXT,
            normalized_text TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    ensure_column(conn, "creator_enrichment", "normalized_text", "TEXT")


def ensure_column(conn: sqlite3.Connection, table: str, column: str, coltype: str) -> None:
    existing = {row[1] for row in conn.execute(f"PRAGMA table_info({table})")}
    if column not in existing:
        conn.execute(f"ALTER TABLE {table} ADD COLUMN {column} {coltype}")


def ensure_brands_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS brands (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            brand_name TEXT,
            website_url TEXT UNIQUE,
            source_website_raw TEXT,
            source_linkedin_raw TEXT,
            source_social_raw TEXT,
            extracted_description TEXT,
            extracted_product_category TEXT,
            extracted_target_audience TEXT,
            extracted_tone TEXT,
            extracted_key_themes TEXT,
            extracted_goals TEXT,
            brand_vertical TEXT,
            brand_audience_profile TEXT,
            brand_use_cases TEXT,
            brand_pain_points TEXT,
            brand_geo_focus TEXT,
            brand_keywords TEXT,
            brand_summary TEXT,
            brand_vertical_override TEXT,
            flag_inconsistent_positioning INTEGER,
            recommended_vertical TEXT,
            recommended_confidence REAL,
            candidate_verticals TEXT,
            normalized_summary TEXT,
            normalized_vertical TEXT,
            normalized_audience TEXT,
            normalized_pain_points TEXT,
            normalized_use_cases TEXT,
            normalized_themes TEXT,
            normalized_tone TEXT,
            normalized_geo TEXT,
            normalized_topics TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    # Ensure new columns on existing DBs
    for col, coltype in [
        ("brand_vertical", "TEXT"),
        ("brand_audience_profile", "TEXT"),
        ("brand_use_cases", "TEXT"),
        ("brand_pain_points", "TEXT"),
        ("brand_geo_focus", "TEXT"),
        ("brand_keywords", "TEXT"),
        ("brand_summary", "TEXT"),
        ("brand_vertical_override", "TEXT"),
        ("flag_inconsistent_positioning", "INTEGER"),
        ("recommended_vertical", "TEXT"),
        ("recommended_confidence", "REAL"),
        ("candidate_verticals", "TEXT"),
        ("normalized_summary", "TEXT"),
        ("normalized_vertical", "TEXT"),
        ("normalized_audience", "TEXT"),
        ("normalized_pain_points", "TEXT"),
        ("normalized_use_cases", "TEXT"),
        ("normalized_themes", "TEXT"),
        ("normalized_tone", "TEXT"),
        ("normalized_geo", "TEXT"),
        ("normalized_topics", "TEXT"),
    ]:
        ensure_column(conn, "brands", col, coltype)


def ensure_creator_vectors_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_vectors (
            podcastindex_id INTEGER PRIMARY KEY,
            embedding TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def ensure_brand_vectors_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS brand_vectors (
            adlid INTEGER PRIMARY KEY,
            embedding TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def ensure_entity_embeddings_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS entity_embeddings (
            embedding_id INTEGER PRIMARY KEY AUTOINCREMENT,
            entity_type TEXT CHECK(entity_type IN ('creator', 'brand')),
            entity_id INTEGER,
            vector_type TEXT,
            embedding_vector TEXT,
            confidence REAL,
            source_text TEXT,
            source_normalized_id INTEGER,
            model_name TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_entity_embeddings ON entity_embeddings(entity_type, entity_id)"
    )
    ensure_column(conn, "entity_embeddings", "source_normalized_id", "INTEGER")
    ensure_column(conn, "entity_embeddings", "vector_type", "TEXT")
    ensure_column(conn, "entity_embeddings", "model_name", "TEXT")


def ensure_entity_normalized_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS entity_normalized (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            entity_type TEXT NOT NULL,
            entity_id INTEGER NOT NULL,
            version TEXT DEFAULT 'v1',
            normalized_block TEXT NOT NULL,
            norm_fields TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute(
        """
        CREATE UNIQUE INDEX IF NOT EXISTS idx_entity_norm_unique
        ON entity_normalized(entity_type, entity_id, version)
        """
    )


def ensure_apple_categories_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS apple_podcast_categories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            apple_id TEXT,
            parent_apple_id TEXT,
            name TEXT,
            full_path TEXT
        )
        """
    )
    conn.execute(
        "CREATE INDEX IF NOT EXISTS idx_apple_cat_parent ON apple_podcast_categories(parent_apple_id)"
    )


def ensure_normalization_lexicon_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS normalization_lexicon (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            raw_label TEXT,
            normalized_label TEXT,
            label_type TEXT,
            notes TEXT
        )
        """
    )


def configure_sqlite(conn: sqlite3.Connection) -> None:
    # Reduce lock contention and waits.
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA busy_timeout=5000;")


def ensure_apple_discovery_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS apple_discovery (
            trackId INTEGER PRIMARY KEY,
            collectionName TEXT,
            artistName TEXT,
            feedUrl TEXT,
            artworkUrl TEXT,
            primaryGenreName TEXT,
            genres TEXT,
            country TEXT,
            trackCount INTEGER,
            releaseDate TEXT,
            description TEXT,
            raw_json TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def clear_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    response = input(
        f"This will DELETE all rows from creator_enrichment in {db_path}. Type YES to confirm: "
    ).strip()
    if response != "YES":
        print("Aborted; no changes made.")
        return 0
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        conn.execute("DELETE FROM creator_enrichment")
        conn.commit()
        print("‚úÖ Cleared creator_enrichment.")
        return 0
    finally:
        conn.close()


def show_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        def load_json(val):
            try:
                return json.loads(val) if val else {}
            except Exception:
                return {}

        def count_nonempty(d: Dict) -> int:
            if not isinstance(d, dict):
                return 0
            return sum(1 for v in d.values() if v not in (None, "", [], {}))

        if not args.source:
            cur = conn.execute(
                """
                SELECT ce.podcastindex_id, ce.source_spotify, ce.source_apple, ce.source_youtube,
                       ce.source_social_twitter, ce.source_social_instagram, ce.source_social_website, ce.source_social_patreon,
                       pf.title as name
                FROM creator_enrichment ce
                LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
                LIMIT ?
                """,
                (args.limit,),
            )
            rows = cur.fetchall()
            if not rows:
                print("No enrichment rows found.")
                return 0
            for row in rows:
                spotify_j = load_json(row["source_spotify"])
                apple_j = load_json(row["source_apple"])
                youtube_j = load_json(row["source_youtube"])
                socials_all = [
                    load_json(row["source_social_twitter"]),
                    load_json(row["source_social_instagram"]),
                    load_json(row["source_social_website"]),
                    load_json(row["source_social_patreon"]),
                ]
                social_count = sum(count_nonempty(s) for s in socials_all)
                print(f"name: {row['name'] or 'N/A'}")
                print(f"spotify: {count_nonempty(spotify_j)} data")
                print(f"apple: {count_nonempty(apple_j)} data")
                print(f"youtube: {count_nonempty(youtube_j)} data")
                print(f"social: {social_count} data")
                print("")
            return 0

        if args.source == "social":
            cur = conn.execute(
                """
                SELECT ce.podcastindex_id, ce.source_social_twitter, ce.source_social_instagram,
                       ce.source_social_website, ce.source_social_patreon, pf.title as name
                FROM creator_enrichment ce
                LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
                LIMIT ?
                """,
                (args.limit,),
            )
            rows = cur.fetchall()
            if not rows:
                print("No rows with social data.")
                return 0
            for row in rows:
                print(f"{row['name'] or row['podcastindex_id']}:")
                for label, col in [
                    ("twitter", "source_social_twitter"),
                    ("instagram", "source_social_instagram"),
                    ("website", "source_social_website"),
                    ("patreon", "source_social_patreon"),
                ]:
                    data = load_json(row[col])
                    if not data or not isinstance(data, dict):
                        continue
                    if count_nonempty(data) == 0:
                        continue
                    print(f"  {label}: {data}")
                print("")
            return 0

        col = {
            "spotify": "source_spotify",
            "apple": "source_apple",
            "youtube": "source_youtube",
            "twitter": "source_social_twitter",
            "instagram": "source_social_instagram",
            "website": "source_social_website",
            "patreon": "source_social_patreon",
        }[args.source]
        cur = conn.execute(
            f"""
            SELECT ce.podcastindex_id, ce.{col}, pf.title as name
            FROM creator_enrichment ce
            LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
            WHERE ce.{col} IS NOT NULL
            LIMIT ?
            """,
            (args.limit,),
        )
        rows = cur.fetchall()
        if not rows:
            print(f"No rows with {args.source} data.")
            return 0
        for row in rows:
            data = load_json(row[col])
            if not isinstance(data, dict):
                print(f"{row['name'] or row['podcastindex_id']}: (invalid JSON)")
                continue
            print(f"{row['name'] or row['podcastindex_id']}:")
            for k, v in data.items():
                if v in (None, "", [], {}):
                    continue
                print(f"  {k}: {v}")
            print("")
        return 0
    finally:
        conn.close()


def summary_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        total_creators = conn.execute("SELECT COUNT(*) FROM creators").fetchone()[0]
        total_enriched = conn.execute("SELECT COUNT(*) FROM creator_enrichment").fetchone()[0]

        def has_payload(val) -> bool:
            if isinstance(val, dict):
                if not val:
                    return False
                status = val.get("status")
                if status in ("error", "not_found", "missing_credentials"):
                    return False
                for v in val.values():
                    if has_payload(v):
                        return True
                return False
            if isinstance(val, list):
                return any(has_payload(v) for v in val)
            return val not in (None, "", [], {})

        def count_valid(col: str) -> int:
            rows = conn.execute(
                f"SELECT {col} FROM creator_enrichment WHERE {col} IS NOT NULL"
            ).fetchall()
            count = 0
            for r in rows:
                try:
                    d = json.loads(r[col]) if r[col] else {}
                except Exception:
                    d = {}
                if has_payload(d):
                    count += 1
            return count

        def count_any_social() -> int:
            rows = conn.execute(
                """
                SELECT source_social_twitter, source_social_instagram, source_social_website, source_social_patreon
                FROM creator_enrichment
                """
            ).fetchall()
            count = 0
            for r in rows:
                blobs = [r["source_social_twitter"], r["source_social_instagram"], r["source_social_website"], r["source_social_patreon"]]
                payload = False
                for blob in blobs:
                    try:
                        d = json.loads(blob) if blob else {}
                    except Exception:
                        d = {}
                    if has_payload(d):
                        payload = True
                        break
                if payload:
                    count += 1
            return count

        spotify_ok = count_valid("source_spotify")
        apple_ok = count_valid("source_apple")
        youtube_ok = count_valid("source_youtube")
        social_ok = count_any_social()

        print(f"Total creators: {total_creators}")
        print(f"Enriched: {total_enriched} / {total_creators}")
        print(f"    spotify: {spotify_ok} / {total_enriched}")
        print(f"    apple: {apple_ok} / {total_enriched}")
        print(f"    youtube: {youtube_ok} / {total_enriched}")
        print(f"    socials: {social_ok} / {total_enriched}")
        return 0
    finally:
        conn.close()


def ingest_podcastindex_feeds(args: argparse.Namespace) -> int:
    source_db = Path(args.source_db)
    target_db = Path(args.target_db)

    batch_size = args.batch_size
    refresh_feeds = bool(getattr(args, "refresh_feeds", False))
    if refresh_feeds and not source_db.exists():
        print(f"‚ùå Source DB not found: {source_db}", file=sys.stderr)
        return 1

    if refresh_feeds:
        print(f"üîÑ Refreshing feeds from {source_db} -> {target_db} (batch size {batch_size})")
    else:
        print(f"üîÑ Populating creators from existing feeds in {target_db} (batch size {batch_size})")

    tgt = sqlite3.connect(target_db, timeout=10)
    tgt.row_factory = sqlite3.Row
    configure_sqlite(tgt)

    try:
        ensure_podcastindex_tables(tgt)
        ensure_creators_table(tgt)
        ensure_creator_enrichment_table(tgt)
        ensure_column(tgt, "creator_enrichment", "source_social_twitter", "TEXT")
        ensure_column(tgt, "creator_enrichment", "source_social_instagram", "TEXT")
        ensure_column(tgt, "creator_enrichment", "source_social_website", "TEXT")
        ensure_column(tgt, "creator_enrichment", "source_social_patreon", "TEXT")
        # New creator metadata columns
        for col in [
            ("vertical", "TEXT"),
            ("audience_profile", "TEXT"),
            ("industry_keywords", "TEXT"),
            ("business_functions", "TEXT"),
            ("pain_points", "TEXT"),
            ("geo_focus", "TEXT"),
            ("title", "TEXT"),
            ("description", "TEXT"),
            ("feed_url", "TEXT"),
            ("web_link", "TEXT"),
            ("categories", "TEXT"),
            ("newestItemPubdate", "INTEGER"),
            ("oldestItemPubdate", "INTEGER"),
            ("episodeCount", "INTEGER"),
            ("updateFrequency", "INTEGER"),
        ]:
            ensure_column(tgt, "creators", col[0], col[1])

        inserted_feeds = 0
        inserted_creators = 0

        if refresh_feeds:
            src = sqlite3.connect(source_db, timeout=10)
            src.row_factory = sqlite3.Row
            configure_sqlite(src)
            try:
                existing_ids = {row[0] for row in tgt.execute("SELECT id FROM podcastindex_feeds")}
                placeholders = ",".join("?" for _ in PODCASTINDEX_COLUMNS)
                feed_sql = f"""
                    INSERT OR REPLACE INTO podcastindex_feeds ({','.join(PODCASTINDEX_COLUMNS)})
                    VALUES ({placeholders})
                """
                total_processed = 0
                src_cursor = src.execute("SELECT * FROM podcasts")
                while True:
                    rows = src_cursor.fetchmany(batch_size)
                    if not rows:
                        break
                    feed_rows = []
                    for row in rows:
                        podcastindex_id = row["id"]
                        feed_rows.append(tuple(row[col] for col in PODCASTINDEX_COLUMNS))
                        if podcastindex_id not in existing_ids:
                            inserted_feeds += 1
                            existing_ids.add(podcastindex_id)
                    if feed_rows:
                        tgt.executemany(feed_sql, feed_rows)
                        tgt.commit()
                    total_processed += len(rows)
                    if total_processed % (batch_size * 5) == 0:
                        print(
                            f"Feeds pass: processed {total_processed:,} rows | "
                            f"inserted/updated feeds: {inserted_feeds:,}"
                        )
                print(f"‚úÖ Feeds refresh complete. Total feeds inserted/updated: {inserted_feeds:,}")
            finally:
                src.close()

        # Step 2: filter feeds -> creators (always runs)
        existing_creators_by_pid = {
            row[0] for row in tgt.execute("SELECT podcastindex_id FROM creators")
        }
        three_years_ago = int(time.time()) - 3 * 365 * 24 * 3600
        filtered = tgt.execute(
            """
            SELECT *
            FROM podcastindex_feeds
            WHERE language LIKE '%en%'
              AND (lastHttpStatus IS NULL OR lastHttpStatus = 200)
              AND LENGTH(description) >= 50
              AND newestItemPubdate IS NOT NULL
              AND newestItemPubdate >= ?
              AND episodeCount IS NOT NULL AND episodeCount >= 3
            ORDER BY newestItemPubdate DESC
            """,
            (three_years_ago,),
        )
        creator_sql = """
            INSERT OR IGNORE INTO creators (
                podcastindex_id, podcastguid, itunesid, language,
                title, description, feed_url, web_link, categories,
                newestItemPubdate, oldestItemPubdate, episodeCount, updateFrequency
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        processed_filtered = 0
        while True:
            rows = filtered.fetchmany(batch_size)
            if not rows:
                break
            creator_rows = []
            for row in rows:
                pid = row["id"]
                if pid in existing_creators_by_pid:
                    continue
                categories = []
                for key in ("category1", "category2", "category3", "category4", "category5", "category6", "category7", "category8", "category9", "category10"):
                    if row[key]:
                        categories.append(row[key])
                categories_text = ", ".join(categories) if categories else None
                creator_rows.append(
                    (
                        pid,
                        row["podcastGuid"],
                        row["itunesId"],
                        row["language"],
                        row["title"],
                        row["description"],
                        row["url"],
                        row["link"],
                        categories_text,
                        row["newestItemPubdate"],
                        row["oldestItemPubdate"],
                        row["episodeCount"],
                        row["updateFrequency"],
                    )
                )
                existing_creators_by_pid.add(pid)
                inserted_creators += 1
            if creator_rows:
                tgt.executemany(creator_sql, creator_rows)
                tgt.commit()
            processed_filtered += len(rows)
            if processed_filtered % (batch_size * 5) == 0:
                print(
                    f"Creators pass: processed {processed_filtered:,} filtered rows | "
                    f"inserted creators: {inserted_creators:,}"
                )

        print(
            f"‚úÖ Done. Feeds inserted/updated: {inserted_feeds:,}; "
            f"creators inserted: {inserted_creators:,}"
        )
        return 0
    finally:
        tgt.close()
        tgt.close()


def parse_comma_list(raw: Optional[str], *, default: Sequence[str]) -> List[str]:
    if not raw:
        return list(default)
    if raw.lower() == "all":
        return list(default)
    return [chunk.strip() for chunk in raw.split(",") if chunk.strip()]


def load_list_inputs(raw: Optional[str], path: Optional[Path]) -> List[str]:
    items: List[str] = []
    if raw:
        items.extend(parse_comma_list(raw, default=[]))
    if path and path.exists():
        items.extend([line.strip() for line in path.read_text().splitlines() if line.strip()])
    # Deduplicate while preserving order
    seen = set()
    deduped = []
    for item in items:
        if item not in seen:
            seen.add(item)
            deduped.append(item)
    return deduped


def discover_apple(args: argparse.Namespace) -> int:
    terms = load_list_inputs(args.terms, args.terms_file)
    if not terms:
        print("‚ùå No search terms provided.", file=sys.stderr)
        return 1

    db_path = Path(args.adl_db)
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_apple_discovery_table(conn)

    seen_ids = {
        row[0] for row in conn.execute("SELECT trackId FROM apple_discovery")
    }

    inserted = 0
    total_terms = len(terms)
    try:
        for idx, term in enumerate(terms, start=1):
            print(f"üîé Term {idx}/{total_terms}: '{term}'")
            offset = 0
            pages = 0
            while pages < args.max_pages:
                backoff = 5
                params = {
                    "term": term,
                    "country": args.country,
                    "media": "podcast",
                    "limit": args.limit_per_term,
                    "offset": offset,
                }
                while True:
                    res = requests.get("https://itunes.apple.com/search", params=params, timeout=15)
                    if res.status_code == 429:
                        print(f"  429 rate limit hit; sleeping {backoff}s...")
                        time.sleep(backoff)
                        backoff = min(backoff * 2, 60)
                        continue
                    res.raise_for_status()
                    break
                payload = res.json()
                results = payload.get("results", [])
                if not results:
                    print(f"  No more results at page {pages}.")
                    break
                rows = []
                for r in results:
                    tid = r.get("trackId")
                    if not tid or tid in seen_ids:
                        continue
                    seen_ids.add(tid)
                    rows.append(
                        (
                            tid,
                            r.get("collectionName"),
                            r.get("artistName"),
                            r.get("feedUrl"),
                            r.get("artworkUrl100"),
                            r.get("primaryGenreName"),
                            json.dumps(r.get("genres")),
                            r.get("country"),
                            r.get("trackCount"),
                            r.get("releaseDate"),
                            r.get("description") or r.get("longDescription"),
                            json.dumps(r),
                        )
                    )
                if rows:
                    conn.executemany(
                        """
                        INSERT INTO apple_discovery (
                            trackId, collectionName, artistName, feedUrl, artworkUrl,
                            primaryGenreName, genres, country, trackCount, releaseDate,
                            description, raw_json
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        rows,
                    )
                    conn.commit()
                    inserted += len(rows)
                    print(f"  Page {pages+1}: inserted {len(rows)} new (total {inserted})")
                offset += args.limit_per_term
                pages += 1
                time.sleep(0.5)
        print(f"‚úÖ Apple discovery inserted {inserted} new rows into apple_discovery.")
        return 0
    finally:
        conn.close()


def ingest_apple_discovery(args: argparse.Namespace) -> int:
    source_db = Path(args.source_db)
    target_db = Path(args.target_db)

    if not target_db.exists():
        print(f"‚ùå Target DB not found: {target_db}", file=sys.stderr)
        return 1

    tgt = sqlite3.connect(target_db, timeout=10)
    tgt.row_factory = sqlite3.Row
    configure_sqlite(tgt)
    try:
        ensure_creators_table(tgt)
        ensure_apple_discovery_table(tgt)

        existing_itunes = {
            row[0] for row in tgt.execute("SELECT itunesid FROM creators WHERE itunesid IS NOT NULL")
        }

        rows = tgt.execute(
            "SELECT trackId, collectionName, country FROM apple_discovery"
        ).fetchall()
        to_insert = []
        for r in rows:
            tid = r["trackId"]
            if tid in existing_itunes:
                continue
            # Use negative trackId to avoid clashing with PodcastIndex IDs
            pid = -int(tid)
            language = "en-gb" if (r["country"] or "").lower() == "gb" else "en"
            to_insert.append((pid, None, tid, language))

        if not to_insert:
            print("No new Apple discovery rows to merge.")
            return 0

        tgt.executemany(
            """
            INSERT OR IGNORE INTO creators (podcastindex_id, podcastguid, itunesid, language)
            VALUES (?, ?, ?, ?)
            """,
            to_insert,
        )
        tgt.commit()
        print(f"‚úÖ Merged {len(to_insert)} Apple discovery rows into creators (pid = -trackId).")
        return 0
    finally:
        tgt.close()


def fetch_apple_metadata(itunes_id: Optional[int], title: str) -> Dict:
    if not title and not itunes_id:
        return {}
    try:
        params = {"entity": "podcast"}
        if itunes_id:
            params["id"] = itunes_id
            url = "https://itunes.apple.com/lookup"
        else:
            params["term"] = title
            params["limit"] = 3
            url = "https://itunes.apple.com/search"
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        results = data.get("results", [])
        if not results:
            return {}
        best = results[0]
        desc = (
            best.get("description")
            or best.get("longDescription")
            or best.get("collectionDescription")
            or ""
        )
        if desc and desc.lower() in ("notexplicit", "explicit"):
            desc = ""
        return {
            "title": best.get("collectionName") or best.get("trackName"),
            "description": desc,
            "genres": best.get("genres"),
            "publisher": best.get("artistName"),
            "artistUrl": best.get("artistViewUrl"),
            "collectionUrl": best.get("collectionViewUrl"),
            "feedUrl": best.get("feedUrl"),
            "status": "ok",
        }
    except Exception as exc:
        print(f"‚ö†Ô∏è Apple lookup failed: {exc}", file=sys.stderr)
        return {}


def fetch_spotify_metadata(show_url: Optional[str], title: str) -> Dict:
    client_id = os.getenv("SPOTIFY_CLIENT_ID")
    client_secret = os.getenv("SPOTIFY_CLIENT_SECRET")
    if not client_id or not client_secret:
        return {"status": "missing_credentials"}

    try:
        token_res = requests.post(
            "https://accounts.spotify.com/api/token",
            data={"grant_type": "client_credentials"},
            auth=(client_id, client_secret),
            timeout=15,
        )
        token_res.raise_for_status()
        access_token = token_res.json().get("access_token")
        headers = {"Authorization": f"Bearer {access_token}"}

        show_id = None
        if show_url and "open.spotify.com/show/" in show_url:
            show_id = show_url.rstrip("/").split("/")[-1]
        else:
            if not title:
                return {"status": "not_found"}
            search_res = requests.get(
                "https://api.spotify.com/v1/search",
                headers=headers,
                params={"q": title, "type": "show", "market": "GB", "limit": 3},
                timeout=15,
            )
            search_res.raise_for_status()
            items = search_res.json().get("shows", {}).get("items", [])
            if items:
                # pick best by title similarity
                from difflib import SequenceMatcher

                scored = []
                for it in items:
                    score = SequenceMatcher(None, title.lower(), (it.get("name") or "").lower()).ratio()
                    scored.append((score, it))
                scored.sort(reverse=True, key=lambda x: x[0])
                top_score, top_item = scored[0]
                if top_score >= 0.8:
                    show_id = top_item["id"]

        if not show_id:
            return {"status": "not_found"}

        show_res = requests.get(
            f"https://api.spotify.com/v1/shows/{show_id}",
            headers=headers,
            params={"market": "GB"},
            timeout=15,
        )
        show_res.raise_for_status()
        show_data = show_res.json()

        # Verify title match to avoid mismatches
        if title:
            from difflib import SequenceMatcher

            if SequenceMatcher(None, title.lower(), (show_data.get("name") or "").lower()).ratio() < 0.8:
                return {"status": "not_found"}

        ep_res = requests.get(
            f"https://api.spotify.com/v1/shows/{show_id}/episodes",
            headers=headers,
            params={"market": "GB", "limit": 10},
            timeout=15,
        )
        ep_res.raise_for_status()
        eps = ep_res.json().get("items", []) or []
        ep_samples = [
            {"name": ep.get("name"), "description": ep.get("description")}
            for ep in eps[:10]
        ]

        return {
            "show": {
                "id": show_id,
                "name": show_data.get("name"),
                "description": show_data.get("description"),
                "publisher": show_data.get("publisher"),
                "languages": show_data.get("languages"),
                "total_episodes": show_data.get("total_episodes"),
            },
            "episodes_sample": ep_samples,
            "status": "ok",
        }
    except requests.HTTPError as http_err:
        status_code = http_err.response.status_code if http_err.response else None
        detail = ""
        try:
            detail = http_err.response.json()
        except Exception:
            detail = http_err.response.text if hasattr(http_err, "response") else ""
        print(f"‚ö†Ô∏è Spotify fetch failed: {http_err} detail={detail}", file=sys.stderr)
        return {"status": "error", "error": str(http_err), "detail": detail, "http_status": status_code}
    except Exception as exc:
        print(f"‚ö†Ô∏è Spotify fetch failed: {exc}", file=sys.stderr)
        return {"status": "error", "error": str(exc)}


def fetch_youtube_metadata(title: str, *, mode: str = "lean") -> Dict:
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        return {"status": "missing_credentials"}

    try:
        # Search channel
        search_res = requests.get(
            "https://www.googleapis.com/youtube/v3/search",
            params={
                "part": "snippet",
                "q": title,
                "type": "channel",
                "regionCode": "GB",
                "maxResults": 8,
                "key": api_key,
            },
            timeout=15,
        )
        search_res.raise_for_status()
        items = search_res.json().get("items", [])
        picked_channel = None

        for ch in items:
            channel_id = ch["snippet"]["channelId"]
            # Fetch channel details to get full description
            chan_res = requests.get(
                "https://www.googleapis.com/youtube/v3/channels",
                params={
                    "part": "snippet",
                    "id": channel_id,
                    "key": api_key,
                },
                timeout=15,
            )
            if chan_res.status_code != 200:
                continue
            chan_items = chan_res.json().get("items", [])
            if not chan_items:
                continue
            chan_snip = chan_items[0].get("snippet", {}) or {}
            channel_title = chan_snip.get("title", "")
            channel_description = chan_snip.get("description", "")

            lower_title = channel_title.lower()
            if "topic" in lower_title or lower_title.endswith(" - topic"):
                # Reject auto-generated topic channels
                continue
            if not channel_description or len(channel_description.strip()) < 20:
                # Reject channels with no meaningful about text
                continue

            picked_channel = {
                "id": channel_id,
                "title": channel_title,
                "about": channel_description,
            }
            break

        if not picked_channel:
            return {"status": "not_found"}

        channel_id = picked_channel["id"]
        channel_title = picked_channel["title"]
        channel_description = picked_channel["about"]

        # Fetch top videos by viewCount
        video_limit = 1 if mode == "lean" else 3
        videos_res = requests.get(
            "https://www.googleapis.com/youtube/v3/search",
            params={
                "part": "snippet",
                "channelId": channel_id,
                "order": "viewCount",
                "maxResults": video_limit,
                "type": "video",
                "key": api_key,
            },
            timeout=15,
        )
        videos_res.raise_for_status()
        video_items = videos_res.json().get("items", []) or []
        videos_sample = []
        comments_sample: List[str] = []

        for vid in video_items:
            vid_id = vid["id"]["videoId"]
            vid_title = vid["snippet"]["title"]
            vid_desc = vid["snippet"].get("description", "")
            videos_sample.append({"id": vid_id, "title": vid_title, "description": vid_desc})

            # Comments
            if mode != "lean":
                comments_res = requests.get(
                    "https://www.googleapis.com/youtube/v3/commentThreads",
                    params={
                        "part": "snippet",
                        "videoId": vid_id,
                        "maxResults": 50,
                        "order": "relevance",
                        "textFormat": "plainText",
                        "key": api_key,
                    },
                    timeout=15,
                )
                if comments_res.status_code == 200:
                    threads = comments_res.json().get("items", []) or []
                    for th in threads:
                        top = th.get("snippet", {}).get("topLevelComment", {}).get("snippet", {})
                        txt = top.get("textDisplay")
                        if txt:
                            comments_sample.append(txt)
                        if len(comments_sample) >= 50:
                            break
                if len(comments_sample) >= 50:
                    break

        return {
            "channel": picked_channel,
            "videos_sample": videos_sample,
            "comments_sample": comments_sample[:50],
            "status": "ok",
        }
    except requests.HTTPError as http_err:
        detail = ""
        status_code = http_err.response.status_code if http_err.response else None
        try:
            detail = http_err.response.json()
        except Exception:
            detail = http_err.response.text if hasattr(http_err, "response") else ""
        print(f"‚ö†Ô∏è YouTube fetch failed: {http_err} detail={detail}", file=sys.stderr)
        return {"status": "error", "error": str(http_err), "detail": detail, "http_status": status_code}
    except Exception as exc:
        print(f"‚ö†Ô∏è YouTube fetch failed: {exc}", file=sys.stderr)
        return {"status": "error", "error": str(exc)}


def fetch_social_bios(title: str) -> Dict:
    return {}


def build_text_block(
    base: Dict,
    apple: Dict,
    spotify: Dict,
    youtube: Dict,
    social: Dict,
    extracted: Optional[Dict] = None,
) -> Tuple[str, Dict]:
    # Compose raw sources
    raw_sources = {
        "podcastindex_description": base.get("pi_description"),
        "spotify_description": (spotify.get("show", {}) if spotify else {}).get("description"),
        "apple_description": apple.get("description"),
        "youtube_about": (youtube.get("channel", {}) if youtube else {}).get("about"),
        "youtube_descriptions": [v.get("description") for v in (youtube.get("videos_sample") or [])] if youtube else [],
        "youtube_comments": youtube.get("comments_sample") if youtube else [],
        "social_bios": {
            "twitter": (social.get("twitter") or {}) if social else {},
            "instagram": (social.get("instagram") or {}) if social else {},
            "website_about": (social.get("website") or {}).get("about") if social else None,
        },
        "extracted_metadata": extracted or {},
    }

    def val(key: str) -> str:
        if not extracted:
            return ""
        v = extracted.get(key)
        return v if v else ""

    text_block = f"""NAME:
{apple.get('title') or spotify.get('title') or base.get('pi_title') or ''}

SUMMARY:
{val('summary')}

VERTICAL:
{val('vertical')}

AUDIENCE:
{val('audience_profile')}

FUNCTIONS:
{val('business_functions')}

USE_CASES:

KEYWORDS:
{val('industry_keywords')}

PAIN_POINTS:
{val('pain_points')}

GEO:
{val('geo_focus')}
"""
    cleaned = clean_text_block(text_block)
    # Cap length to avoid runaway blobs
    if len(cleaned) > 4000:
        cleaned = cleaned[:4000]
    return cleaned, raw_sources


def enrich_creators(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1

    have_spotify = bool(os.getenv("SPOTIFY_CLIENT_ID") and os.getenv("SPOTIFY_CLIENT_SECRET"))
    have_youtube = bool(os.getenv("YOUTUBE_API_KEY"))
    if not have_spotify:
        print("‚ùå SPOTIFY_CLIENT_ID/SECRET not set; enrichment requires Spotify API.", file=sys.stderr)
        return 1
    if not have_youtube:
        print("‚ùå YOUTUBE_API_KEY not set; enrichment requires YouTube API.", file=sys.stderr)
        return 1

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        ensure_podcastindex_tables(conn)
        ensure_creators_table(conn)
        ensure_creator_enrichment_table(conn)
        ensure_column(conn, "creator_enrichment", "source_social_twitter", "TEXT")
        ensure_column(conn, "creator_enrichment", "source_social_instagram", "TEXT")
        ensure_column(conn, "creator_enrichment", "source_social_website", "TEXT")
        ensure_column(conn, "creator_enrichment", "source_social_patreon", "TEXT")
        for col in [
            ("vertical", "TEXT"),
            ("audience_profile", "TEXT"),
            ("industry_keywords", "TEXT"),
            ("business_functions", "TEXT"),
            ("pain_points", "TEXT"),
            ("geo_focus", "TEXT"),
        ]:
            ensure_column(conn, "creators", col[0], col[1])

        requested_sources = None
        if args.sources:
            requested_sources = {s.strip().lower() for s in args.sources.split(",") if s.strip()}
            print(f"üîé Enriching only requested sources: {', '.join(sorted(requested_sources))}")
            print("   Will prioritize rows with missing/error/not_found for those sources.")
        disabled_sources = set()

        cursor = conn.cursor()
        pidrange = args.pidrange

        if args.only_id:
            cursor.execute(
                """
                SELECT c.podcastindex_id, c.itunesid, c.language,
                       p.url as feed_url, p.link as web_link, p.title as pi_title, p.description as pi_description,
                       p.category1, p.category2, p.category3, p.category4, p.category5,
                       p.category6, p.category7, p.category8, p.category9, p.category10,
                       p.podcastGuid,
                       ce.source_spotify, ce.source_apple, ce.source_youtube,
                       ce.source_social_twitter, ce.source_social_instagram, ce.source_social_website, ce.source_social_patreon
                FROM creators c
                LEFT JOIN podcastindex_feeds p ON c.podcastindex_id = p.id
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                WHERE c.podcastindex_id = ?
                """,
                (args.only_id,),
            )
        else:
            range_clause = ""
            params = []
            if pidrange:
                range_clause = "WHERE c.podcastindex_id BETWEEN ? AND ?"
                params.extend([pidrange[0], pidrange[1]])
            query = f"""
                SELECT c.podcastindex_id, c.itunesid, c.language,
                       p.url as feed_url, p.link as web_link, p.title as pi_title, p.description as pi_description,
                       p.category1, p.category2, p.category3, p.category4, p.category5,
                       p.category6, p.category7, p.category8, p.category9, p.category10,
                       p.podcastGuid,
                       ce.source_spotify, ce.source_apple, ce.source_youtube,
                       ce.source_social_twitter, ce.source_social_instagram, ce.source_social_website, ce.source_social_patreon
                FROM creators c
                LEFT JOIN podcastindex_feeds p ON c.podcastindex_id = p.id
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                {range_clause}
                ORDER BY
                    p.newestItemPubdate DESC NULLS LAST,
                    p.episodeCount DESC NULLS LAST,
                    COALESCE(p.updateFrequency, 999999) ASC
                LIMIT ? OFFSET ?
            """
            params.extend([args.limit * 5, args.offset])
            cursor.execute(
                query,
                params,
            )

        raw_rows = cursor.fetchall()
        if not raw_rows:
            print("No creators to enrich.")
            return 0

        def load_json(val):
            try:
                return json.loads(val) if val else {}
            except Exception:
                return {}

        def needs_source_refresh(source_name: str, existing: Dict) -> bool:
            if requested_sources and source_name not in requested_sources:
                return False
            status = existing.get("status")
            if not existing or status in (None, "", "not_found", "error", "missing_credentials"):
                return True
            return False

        rows = []
        for r in raw_rows:
            existing_spotify = load_json(r["source_spotify"])
            existing_apple = load_json(r["source_apple"])
            existing_youtube = load_json(r["source_youtube"])
            existing_social = {
                "twitter": load_json(r["source_social_twitter"]),
                "instagram": load_json(r["source_social_instagram"]),
                "website": load_json(r["source_social_website"]),
                "patreon": load_json(r["source_social_patreon"]),
            }
            ce_missing = (
                r["source_spotify"] is None
                and r["source_apple"] is None
                and r["source_youtube"] is None
                and r["source_social_twitter"] is None
                and r["source_social_instagram"] is None
                and r["source_social_website"] is None
                and r["source_social_patreon"] is None
            )

            all_social_keys = {"twitter", "instagram", "website", "patreon"}
            if requested_sources:
                if "social" in requested_sources:
                    requested_social_keys = all_social_keys
                else:
                    requested_social_keys = {s for s in requested_sources if s in all_social_keys}
            else:
                requested_social_keys = all_social_keys

            def social_needs_refresh() -> bool:
                for key in requested_social_keys:
                    val = existing_social.get(key)
                    if not val:
                        return True
                    if isinstance(val, dict) and len(val.keys()) == 0:
                        return True
                return False

            if not requested_sources:
                refresh_needed = ce_missing or any(
                    needs_source_refresh(src_name, existing)
                    for src_name, existing in [
                        ("spotify", existing_spotify),
                        ("apple", existing_apple),
                        ("youtube", existing_youtube),
                    ]
                )
                if not refresh_needed:
                    refresh_needed = social_needs_refresh()
                if refresh_needed:
                    rows.append(r)
            else:
                refresh_needed = False
                for src_name, existing in [
                    ("spotify", existing_spotify),
                    ("apple", existing_apple),
                    ("youtube", existing_youtube),
                ]:
                    if src_name in requested_sources and needs_source_refresh(src_name, existing):
                        refresh_needed = True
                        break
                if not refresh_needed and requested_social_keys:
                    refresh_needed = social_needs_refresh()
                if refresh_needed or ce_missing:
                    rows.append(r)

        if not rows:
            print("No creators to enrich for the requested sources.")
            return 0
        print(f"üîÅ Will process {len(rows)} creators (including missing/error/not_found for requested sources).")

        enriched_rows = 0
        stats = {
            "spotify_ok": 0,
            "spotify_missing": 0,
            "spotify_not_found": 0,
            "youtube_ok": 0,
            "youtube_missing": 0,
            "youtube_not_found": 0,
            "apple_ok": 0,
            "apple_not_found": 0,
        }
        for idx, row in enumerate(rows, start=1):
            podcastindex_id = row["podcastindex_id"]
            itunes_id = row["itunesid"]
            language = (row["language"] or "").lower()
            if language and "en" not in language:
                continue
            base = dict(row)
            # Use feed URL to guess spotify URL if it matches open.spotify.com/show/...
            spotify_url = base.get("feed_url") if base.get("feed_url", "").startswith("https://open.spotify.com/show/") else None

            title_for_lookup = base.get("pi_title") or ""

            existing_spotify = load_json(row["source_spotify"])
            existing_apple = load_json(row["source_apple"])
            existing_youtube = load_json(row["source_youtube"])
            existing_social = {
                "twitter": load_json(row["source_social_twitter"]),
                "instagram": load_json(row["source_social_instagram"]),
                "website": load_json(row["source_social_website"]),
                "patreon": load_json(row["source_social_patreon"]),
            }

            fetch_spotify_flag = ((not requested_sources) or ("spotify" in requested_sources)) and ("spotify" not in disabled_sources)
            fetch_apple_flag = ((not requested_sources) or ("apple" in requested_sources)) and ("apple" not in disabled_sources)
            fetch_youtube_flag = ((not requested_sources) or ("youtube" in requested_sources)) and ("youtube" not in disabled_sources)
            social_requested = (not requested_sources) or ("social" in requested_sources) or any(
                s in requested_sources for s in ["twitter", "instagram", "website", "patreon"]
            )
            fetch_social_flag = social_requested and ("social" not in disabled_sources)

            spotify_fetched = fetch_spotify_flag
            youtube_fetched = fetch_youtube_flag
            if idx % 5 == 1 or idx == 1:
                print(f"[{idx}/{len(rows)}] pid={podcastindex_id} fetching sources‚Ä¶", end="\r")
            spotify_meta = existing_spotify if not fetch_spotify_flag else fetch_spotify_metadata(spotify_url, title_for_lookup)
            apple_meta = existing_apple if not fetch_apple_flag else fetch_apple_metadata(itunes_id, title_for_lookup)
            youtube_meta = existing_youtube if not fetch_youtube_flag else fetch_youtube_metadata(title_for_lookup, mode="lean")
            social_meta = existing_social if not fetch_social_flag else fetch_social_bios_from_sources(base, apple_meta, spotify_meta, youtube_meta)

            # Stop on 403 errors for any source
            def is_403(meta: Dict) -> bool:
                if not isinstance(meta, dict):
                    return False
                if meta.get("http_status") == 403:
                    return True
                detail = meta.get("detail")
                if isinstance(detail, str) and "quotaExceeded" in detail:
                    return True
                if isinstance(detail, dict) and meta.get("detail", {}).get("error", {}).get("errors"):
                    # Inspect nested error reasons for quotaExceeded
                    for err in meta["detail"]["error"]["errors"]:
                        if err.get("reason") == "quotaExceeded":
                            return True
                return False

            if spotify_fetched and is_403(spotify_meta):
                if requested_sources and "spotify" in requested_sources:
                    print(f"STOP: Spotify returned 403 for podcastindex_id={podcastindex_id}. Detail: {spotify_meta.get('detail')}")
                    print("\033[31mEnrichment halted due to Spotify 403 (likely quota or key restriction).\033[0m")
                    conn.rollback()
                    return 1
                else:
                    print(f"‚ö†Ô∏è Spotify 403 encountered for podcastindex_id={podcastindex_id}. Skipping further Spotify calls this run.")
                    print("\033[31mSkipping Spotify due to 403 (likely quota or key restriction).\033[0m")
                    disabled_sources.add("spotify")
                    spotify_meta = {"status": "error", "http_status": 403, "detail": spotify_meta.get("detail")}
            if youtube_fetched and is_403(youtube_meta):
                if requested_sources and "youtube" in requested_sources:
                    print(f"STOP: YouTube returned 403 for podcastindex_id={podcastindex_id}. Detail: {youtube_meta.get('detail')}")
                    print("\033[31mEnrichment halted due to YouTube 403 (likely quota or API key restriction).\033[0m")
                    conn.rollback()
                    return 1
                else:
                    print(f"‚ö†Ô∏è YouTube 403 encountered for podcastindex_id={podcastindex_id}. Skipping further YouTube calls this run.")
                    print("\033[31mSkipping YouTube due to 403 (likely quota or API key restriction).\033[0m")
                    disabled_sources.add("youtube")
                    youtube_meta = {"status": "error", "http_status": 403, "detail": youtube_meta.get("detail")}

            # Stats
            apple_status = apple_meta.get("status") or ("ok" if apple_meta else "not_found")
            spotify_status = spotify_meta.get("status") or ("ok" if spotify_meta else "not_found")
            youtube_status = youtube_meta.get("status") or ("ok" if youtube_meta else "not_found")
            if apple_status == "ok":
                stats["apple_ok"] += 1
            else:
                stats["apple_not_found"] += 1
            if spotify_status == "ok":
                stats["spotify_ok"] += 1
            elif spotify_status == "missing_credentials":
                stats["spotify_missing"] += 1
            else:
                stats["spotify_not_found"] += 1
            if youtube_status == "ok":
                stats["youtube_ok"] += 1
            elif youtube_status == "missing_credentials":
                stats["youtube_missing"] += 1
            else:
                stats["youtube_not_found"] += 1

            # Build combined raw text for metadata extraction
            raw_pieces = []
            for val in [
                base.get("pi_description"),
                apple_meta.get("description"),
                (spotify_meta.get("show", {}) if spotify_meta else {}).get("description"),
                (youtube_meta.get("channel", {}) if youtube_meta else {}).get("about"),
                (social_meta.get("website") or {}).get("about") if social_meta else None,
            ]:
                if val:
                    raw_pieces.append(val)
            # Episode titles for extra signal
            if spotify_meta.get("episodes_sample"):
                ep_titles = [ep.get("name") or "" for ep in spotify_meta["episodes_sample"] if ep]
                raw_pieces.append("; ".join(ep_titles[:30]))
            combined_raw = "\n\n".join(raw_pieces) if raw_pieces else ""

            if idx % 5 == 1 or idx == 1:
                print(f"[{idx}/{len(rows)}] pid={podcastindex_id} running GPT extract‚Ä¶", end="\r")
            cleaned_text, raw_sources = build_text_block(
                base=base,
                apple=apple_meta,
                spotify=spotify_meta,
                youtube=youtube_meta,
                social=social_meta,
                extracted=extract_creator_metadata(combined_raw) if combined_raw else {},
            )

            uk_score = compute_uk_score(base, apple_meta, spotify_meta, youtube_meta, social_meta)

            conn.execute(
                """
                INSERT OR REPLACE INTO creator_enrichment (
                    podcastindex_id, uk_score, source_spotify, source_apple, source_youtube,
                    source_social_twitter, source_social_instagram, source_social_website, source_social_patreon,
                    cleaned_text_block, raw_text_sources, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """,
                (
                    podcastindex_id,
                    uk_score,
                    json.dumps(spotify_meta),
                    json.dumps(apple_meta),
                    json.dumps(youtube_meta),
                    json.dumps(social_meta.get("twitter")) if social_meta.get("twitter") else None,
                    json.dumps(social_meta.get("instagram")) if social_meta.get("instagram") else None,
                    json.dumps(social_meta.get("website")) if social_meta.get("website") else None,
                    json.dumps(social_meta.get("patreon")) if social_meta.get("patreon") else None,
                    cleaned_text,
                    json.dumps(raw_sources),
                ),
            )
            # Persist structured creator metadata onto creators table
            extracted = raw_sources.get("extracted_metadata") or {}
            if extracted:
                conn.execute(
                    """
                    UPDATE creators
                    SET vertical = ?, audience_profile = ?, industry_keywords = ?, business_functions = ?, pain_points = ?, geo_focus = ?
                    WHERE podcastindex_id = ?
                    """,
                    (
                        extracted.get("vertical"),
                        extracted.get("audience_profile"),
                        extracted.get("industry_keywords"),
                        extracted.get("business_functions"),
                        extracted.get("pain_points"),
                        extracted.get("geo_focus"),
                        podcastindex_id,
                    ),
                )
            enriched_rows += 1
            if enriched_rows % 10 == 0:
                conn.commit()
                print(
                    f"Enriched {enriched_rows}/{len(rows)} (last podcastindex_id={podcastindex_id})"
                )

        conn.commit()
        print(
            f"‚úÖ Enriched {enriched_rows} creators. "
            f"Apple ok: {stats['apple_ok']}, not_found/missing: {stats['apple_not_found']}; "
            f"Spotify ok: {stats['spotify_ok']}, missing creds: {stats['spotify_missing']}, other: {stats['spotify_not_found']}; "
            f"YouTube ok: {stats['youtube_ok']}, missing creds: {stats['youtube_missing']}, other: {stats['youtube_not_found']}"
        )
        return 0
    finally:
        conn.close()


def clean_text_block(text: str) -> str:
    """
    Apply basic cleaning rules:
    - strip URLs
    - remove hashtags/handles
    - remove boilerplate like 'like and subscribe'
    - remove timestamps like [00:00], 00:00:
    - collapse repeated whitespace
    """
    if not text:
        return ""
    # Remove URLs
    text = re.sub(r"https?://\\S+|www\\.\\S+", " ", text)
    # Remove hashtags/handles
    text = re.sub(r"[@#][A-Za-z0-9_]+", " ", text)
    # Remove timestamps like [00:00] or 00:00:
    text = re.sub(r"\\[?\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\]?\\:?", " ", text)
    # Remove common boilerplate phrases
    boilerplate_patterns = [
        r"like and subscribe",
        r"don‚Äôt forget to subscribe",
        r"don't forget to subscribe",
        r"check out our sponsor",
        r"leave a comment",
        r"support us on patreon",
    ]
    for pat in boilerplate_patterns:
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)
    # Collapse repeated text: naive approach by splitting sentences and deduping consecutive repeats
    parts = text.split(".")
    deduped_parts = []
    prev = None
    for part in parts:
        chunk = part.strip()
        if not chunk:
            continue
        if chunk == prev:
            continue
        deduped_parts.append(chunk)
        prev = chunk
    text = ". ".join(deduped_parts)
    # Normalize whitespace
    text = re.sub(r"\\s+", " ", text).strip()
    return text


UK_KEYWORDS = [
    "united kingdom",
    "uk",
    "british",
    "england",
    "scotland",
    "wales",
    "northern ireland",
    "london",
    "manchester",
    "birmingham",
    "leeds",
    "glasgow",
    "edinburgh",
    "bristol",
    "liverpool",
    "cardiff",
    "belfast",
]


def compute_uk_score(base: Dict, apple: Dict, spotify: Dict, youtube: Dict, social: Dict) -> float:
    score = 0.0
    haystack = " ".join(
        [
            str(base.get("pi_description") or ""),
            str(base.get("pi_title") or ""),
            str(apple.get("description") or ""),
            str((spotify.get("show", {}) if spotify else {}).get("description") or ""),
            str(youtube.get("channel", {}).get("about", "") if youtube else ""),
            str(social.get("website", {}).get("about", "") if social else ""),
        ]
    ).lower()

    # PodcastIndex location not present in schema, so skip; use text heuristics.
    if any(word in haystack for word in ("uk podcast", "british", "london", "uk-based", "britain")):
        score += 0.4

    # Cities / keywords
    for kw in UK_KEYWORDS:
        if kw in haystack:
            score += 0.1
            break

    # Domain heuristic
    feed_url = base.get("feed_url") or ""
    if ".co.uk" in feed_url or feed_url.endswith(".uk"):
        score += 0.2

    # Cap 0..1
    score = max(0.0, min(1.0, score))
    return score


def clean_bio_text(text: str, max_len: int = 500) -> Optional[str]:
    if not text:
        return None
    # Remove URLs, hashtags, handles
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)
    text = re.sub(r"[@#][A-Za-z0-9_]+", " ", text)
    # Remove boilerplate phrases
    boiler = [
        "follow me",
        "link in bio",
        "business enquiries",
        "business inquiries",
        "subscribe",
        "shop",
        "http",
        "https",
    ]
    for b in boiler:
        text = re.sub(b, " ", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) < 10:
        return None
    if len(text) > max_len:
        text = text[:max_len]
    return text


def extract_social_candidates(base: Dict, apple: Dict, spotify: Dict, youtube: Dict) -> Dict[str, List[str]]:
    texts = [
        base.get("pi_description") or "",
        apple.get("description") or "",
        spotify.get("show", {}).get("description") or "",
        (youtube.get("channel", {}) if youtube else {}).get("about") or "",
    ]
    urls = []
    for t in texts:
        urls.extend(re.findall(r"https?://\S+", t))
    # Base links
    if base.get("feed_url"):
        # avoid feed URLs for social scraping
        if not any(tag in base["feed_url"].lower() for tag in ["feedburner", ".xml", "rss"]):
            urls.append(base["feed_url"])
    if base.get("web_link"):
        urls.append(base["web_link"])
    # Apple URLs
    if apple.get("artistUrl"):
        urls.append(apple["artistUrl"])
    if apple.get("collectionUrl"):
        urls.append(apple["collectionUrl"])
    if apple.get("feedUrl"):
        urls.append(apple["feedUrl"])

    candidates = {"twitter": [], "instagram": [], "website": [], "patreon": []}
    for u in urls:
        lu = u.lower()
        if "twitter.com" in lu or "x.com" in lu:
            candidates["twitter"].append(u)
        elif "instagram.com" in lu:
            candidates["instagram"].append(u)
        elif "patreon.com" in lu:
            candidates["patreon"].append(u)
        elif "linktr.ee" in lu or "linktree" in lu:
            candidates["website"].append(u)
        else:
            candidates["website"].append(u)
    return candidates


def fetch_twitter_bio(url: str) -> Optional[Dict]:
    backoff = 2
    for _ in range(3):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                time.sleep(backoff)
                backoff *= 2
                continue
            parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
            soup = BeautifulSoup(res.text, parser)
            meta = soup.find("meta", attrs={"name": "description"})
            if meta and meta.get("content"):
                bio = clean_bio_text(meta["content"])
                if bio:
                    handle = url.split("/")[-1]
                    return {"handle": f"@{handle}", "bio": bio}
            return None
        except Exception:
            time.sleep(backoff)
            backoff *= 2
            continue
    return None


def fetch_instagram_bio(url: str) -> Optional[Dict]:
    backoff = 2
    for _ in range(3):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                time.sleep(backoff)
                backoff *= 2
                continue
            parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
            soup = BeautifulSoup(res.text, parser)
            meta = soup.find("meta", property="og:description")
            if meta and meta.get("content"):
                desc = meta["content"]
                desc = desc.split("Followers")[0]
                bio = clean_bio_text(desc)
                if bio:
                    handle = url.rstrip("/").split("/")[-1]
                    return {"handle": f"@{handle}", "bio": bio}
            return None
        except Exception:
            time.sleep(backoff)
            backoff *= 2
            continue
    return None


def fetch_website_about(url: str) -> Optional[Dict]:
    try:
        targets = [url]
        if url.endswith("/"):
            base = url.rstrip("/")
        else:
            base = url
        targets.append(base + "/about")
        targets.append(base + "/about-us")
        for t in targets:
            try:
                res = requests.get(t, timeout=10)
                if res.status_code != 200:
                    continue
                parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
                soup = BeautifulSoup(res.text, parser)
                paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
                text = " ".join(paragraphs)
                bio = clean_bio_text(text, max_len=500)
                if bio:
                    return {"url": t, "about": bio}
            except Exception:
                continue
    except Exception:
        return None
    return None


def fetch_social_bios_from_sources(base: Dict, apple: Dict, spotify: Dict, youtube: Dict) -> Dict:
    cands = extract_social_candidates(base, apple, spotify, youtube)
    result = {}
    if cands["twitter"]:
        tw = fetch_twitter_bio(cands["twitter"][0])
        if tw:
            result["twitter"] = tw
    if cands["instagram"]:
        ig = fetch_instagram_bio(cands["instagram"][0])
        if ig:
            result["instagram"] = ig
    if cands["patreon"]:
        pt = fetch_website_about(cands["patreon"][0])
        if pt:
            result["patreon"] = pt
    if cands["website"]:
        ws = fetch_website_about(cands["website"][0])
        if ws:
            result["website"] = ws
    return result


def scrape_website_text(url: str) -> Tuple[str, List[str]]:
    try:
        res = requests.get(url, timeout=15)
        res.raise_for_status()
        parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
        try:
            soup = BeautifulSoup(res.text, parser)
        except Exception as exc:
            print(f"‚ö†Ô∏è Parser issue for {url} (parser={parser}): {exc}", file=sys.stderr)
            # Fallback to html parser if xml failed
            soup = BeautifulSoup(res.text, "html.parser")
        for tag in soup(["script", "style", "nav", "footer"]):
            tag.decompose()
        text = soup.get_text(" ", strip=True)
        links = [a.get("href") for a in soup.find_all("a") if a.get("href")]
        # Limit size
        if len(text) > 10000:
            text = text[:10000]
        return text, links
    except Exception as exc:
        print(f"‚ö†Ô∏è Website scrape failed for {url}: {exc}", file=sys.stderr)
        return "", []


def scrape_linkedin_about(url: str) -> str:
    try:
        res = requests.get(url, timeout=15)
        if res.status_code != 200:
            return ""
        parser = "xml" if "xml" in (res.headers.get("Content-Type") or "").lower() else "html.parser"
        soup = BeautifulSoup(res.text, parser)
        meta = soup.find("meta", attrs={"name": "description"})
        if meta and meta.get("content"):
            return meta["content"][:2000]
        return soup.get_text(" ", strip=True)[:2000]
    except Exception:
        return ""


def find_social_links(links: List[str]) -> Dict[str, List[str]]:
    cands = {"twitter": [], "instagram": [], "linkedin": [], "website": []}
    for link in links:
        if not link:
            continue
        l = link.lower()
        if l.startswith("//"):
            link = "https:" + link
        if "linkedin.com" in l:
            cands["linkedin"].append(link)
        elif "twitter.com" in l or "x.com" in l:
            cands["twitter"].append(link)
        elif "instagram.com" in l:
            cands["instagram"].append(link)
        elif link.startswith("http"):
            cands["website"].append(link)
    return cands


def get_openai_client() -> Optional[OpenAI]:
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        print("‚ùå OPENAI_API_KEY not set; cannot run brand extraction.", file=sys.stderr)
        return None
    return OpenAI(api_key=key)


def extract_brand_fields(website_text: str, linkedin_text: str) -> Dict:
    client = get_openai_client()
    if not client:
        return {}
    material = f"{website_text}\n\n{linkedin_text}"
    if len(material) > 8000:
        material = material[:8000]
    prompt = (
        "Extract structured metadata for a brand. Write SHORT, HIGH-SIGNAL content only. Output JSON with keys:\n"
        "- brand_vertical\n"
        "- brand_audience_profile\n"
        "- brand_use_cases\n"
        "- brand_pain_points\n"
        "- brand_geo_focus\n"
        "- summary (2‚Äì4 sentences on what the product does and for whom)\n"
        "- keywords (comma-separated business concepts)\n"
        "- flag_inconsistent_positioning (true/false if positioning seems contradictory)\n"
        "- candidate_verticals (array)\n"
        "- recommended_vertical\n"
        "- recommended_confidence (0..1)\n"
        "Identify if website text appears MRP/manufacturing vs ecommerce-op. If signals conflict, set flag_inconsistent_positioning=true.\n\n"
        "Input:\n"
        f"{material}"
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content
        data = json.loads(content)
        return data
    except Exception as exc:
        # Try to surface any returned text for debugging
        raw = ""
        try:
            raw = resp.choices[0].message.content  # type: ignore
        except Exception:
            raw = ""
        print(f"‚ö†Ô∏è GPT extraction failed: {exc} raw={raw}", file=sys.stderr)
        return {}


def extract_creator_metadata(raw_text: str) -> Dict:
    """
    Use GPT to extract structured creator metadata for embeddings.
    """
    if not ensure_openai_confirmation("creator metadata extraction", allow_fallback=False):
        return {}
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        return {}
    client = OpenAI(api_key=key)
    if len(raw_text) > 15000:
        raw_text = raw_text[:15000]
    prompt = (
        "Extract structured metadata for a podcast.\n"
        "Write SHORT, HIGH-SIGNAL content only. No fluff.\n"
        "Output JSON with keys: vertical, audience_profile, industry_keywords, business_functions, pain_points, geo_focus, summary.\n"
        "- vertical: main industry verticals (e.g., ecommerce, retail, supply chain)\n"
        "- audience_profile: who listens (roles, industries, company stage)\n"
        "- industry_keywords: comma-separated topical keywords\n"
        "- business_functions: ops / marketing / tech / leadership etc.\n"
        "- pain_points: problems listeners want to solve\n"
        "- geo_focus: geography if clear (UK, US, global)\n"
        "- summary: 2-4 sentence distilled summary of the show identity\n\n"
        "Input text:\n"
        f"{raw_text}"
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "json_object"},
        )
        return json.loads(resp.choices[0].message.content)
    except Exception as exc:
        print(f"‚ö†Ô∏è Creator metadata extraction failed: {exc}", file=sys.stderr)
        return {}


# ---- Normalization helpers ----
NORMAL_VECTOR_TYPES = [
    "semantic_summary",
    "topics",
    "vertical",
    "audience",
    "pain_points",
    "use_cases",
    "themes",
    "tone",
    "geo",
]


def normalize_entity_to_json(
    entity_type: str,
    name: str,
    fields_text: str,
    categories_hint: str = "",
    provider: str = "local",
    model: str = LOCAL_OLLAMA_MODEL,
    debug_tag: str = "",
) -> Optional[Dict]:
    schema = (
        "{"
        "\"summary\":\"...\","  # summary of what they are/do
        "\"vertical\":\"...\","  # concise industry label
        "\"audience\":\"...\","  # who this is for
        "\"pain_points\":\"...\","  # problems solved/talked about
        "\"use_cases\":\"...\","  # how used / scenarios
        "\"themes\":\"...\","  # key themes/topics
        "\"tone\":\"...\","  # tone descriptors
        "\"geo\":\"...\","  # geography focus
        "\"topics\":[{\"apple_category_name\":\"...\",\"apple_subcategory_name\":\"...\",\"apple_category_id\":\"...\",\"apple_subcategory_id\":\"...\"}]"
        "}"
    )
    prompt = (
        "You are a classification and summarisation assistant.\n"
        "Normalize noisy marketing/technical text about an entity into a simple schema, using plain language a non-expert would understand.\n"
        "Return JSON ONLY with keys: summary, vertical, audience, pain_points, use_cases, themes, tone, geo, topics (array as specified).\n"
        "Be concise, high-signal, no fluff. Use common industry labels.\n"
        "Output MUST be valid JSON (no trailing commas, close all brackets/quotes). Keep lists short (max 3 items each).\n"
        "If multiple positions appear, prefer the CURRENT go-to-market positioning.\n"
        "Do not hallucinate missing facts; leave fields short and generic instead.\n\n"
        f"ENTITY_TYPE: {entity_type}\n"
        f"NAME: {name}\n"
        f"RAW FIELDS:\n{fields_text}\n\n"
        f"Apple Podcast categories (examples):\n{categories_hint}\n\n"
        "Output JSON in this structure:\n"
        + schema
    )
    if provider == "openai":
        if not ensure_openai_confirmation("normalization", allow_fallback=False):
            return None
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            print("‚ùå OPENAI_API_KEY not set.", file=sys.stderr)
            return None
        client = OpenAI(api_key=key)
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"},
            )
            return json.loads(resp.choices[0].message.content)
        except Exception as exc:
            print(f"‚ö†Ô∏è Normalization failed: {exc}", file=sys.stderr)
            return None
    # local via Ollama
    text = ollama_generate(prompt, model)
    if not text:
        return None
    # Clean code fences / extra text
    cleaned = text.strip()
    for fence in ("```json", "```", "`"):
        cleaned = cleaned.replace(fence, "")

    def try_parse(val: str) -> Optional[Dict]:
        try:
            return json.loads(val)
        except Exception:
            return None

    def balance(text: str) -> str:
        open_curly = text.count("{")
        close_curly = text.count("}")
        open_brack = text.count("[")
        close_brack = text.count("]")
        fix = text
        fix += "}" * max(0, open_curly - close_curly)
        fix += "]" * max(0, open_brack - close_brack)
        return fix

    def clean_trailing(val: str) -> str:
        # Replace trailing colon with empty array to close fields
        val = re.sub(r":\s*$", ": []", val.strip())
        # Drop trailing commas before closing braces/brackets
        val = re.sub(r",\s*([}\]])", r"\1", val)
        return val

    def truncate_to_last_block(val: str) -> str:
        # Cut off trailing incomplete field by trimming after last closing brace/bracket
        last_brace = val.rfind("}")
        last_brack = val.rfind("]")
        end = max(last_brace, last_brack)
        if end != -1:
            return val[: end + 1]
        return val

    parsed = try_parse(cleaned)
    if parsed is None:
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = cleaned[start : end + 1]
            parsed = try_parse(candidate)
    if parsed is None:
        fixed = truncate_to_last_block(cleaned)
        fixed = clean_trailing(fixed)
        fixed = balance(fixed)
        parsed = try_parse(fixed)
    if parsed is None:
        # Iteratively drop the last line and retry a few times to remove truncated tails
        tmp = cleaned
        for _ in range(3):
            if "\n" in tmp:
                tmp = "\n".join(tmp.split("\n")[:-1])
            else:
                tmp = tmp[:-1]
            fixed = truncate_to_last_block(tmp)
            fixed = clean_trailing(fixed)
            fixed = balance(fixed)
            parsed = try_parse(fixed)
            if parsed is not None:
                break
    if parsed is None:
        excerpt = cleaned[:400]
        print(f"‚ö†Ô∏è Normalization ({provider}) parse failed for {debug_tag or 'entity'}; excerpt:\n{excerpt}\n", file=sys.stderr)
        try:
            out_dir = Path("tests")
            out_dir.mkdir(parents=True, exist_ok=True)
            with (out_dir / "normalize_failures.log").open("a") as f:
                f.write(f"{datetime.utcnow().isoformat()} | {debug_tag or entity_type} | provider={provider} | model={model}\n")
                f.write(excerpt + "\n\n")
        except Exception:
            pass
    return parsed


def build_brand_text_block(row: sqlite3.Row) -> str:
    def clip(val: Optional[str], max_len: int = 800) -> str:
        if not val:
            return ""
        s = str(val)
        if len(s) > max_len:
            return s[:max_len]
        return s

    parts = []
    parts.append(f"NAME: {clip(row['brand_name'])}")
    parts.append(f"SUMMARY: {clip(row['brand_summary'] or row['extracted_description'])}")
    vertical = row["brand_vertical_override"] or row["recommended_vertical"] or row["brand_vertical"] or row["extracted_product_category"]
    parts.append(f"VERTICAL: {clip(vertical)}")
    parts.append(f"AUDIENCE: {clip(row['brand_audience_profile'] or row['extracted_target_audience'])}")
    parts.append(f"FUNCTIONS: ")
    parts.append(f"USE_CASES: {clip(row['brand_use_cases'])}")
    parts.append(f"PAIN_POINTS: {clip(row['brand_pain_points'] or row['extracted_goals'])}")
    parts.append(f"KEYWORDS: {clip(row['brand_keywords'] or row['extracted_key_themes'])}")
    parts.append(f"GEO: {clip(row['brand_geo_focus'])}")
    return "\n".join(parts)


def embed_text_block(text: str, provider: str = "local", model: str = "text-embedding-3-large") -> Optional[List[float]]:
    if not text:
        return None
    if provider == "openai":
        if not ensure_openai_confirmation("embeddings", allow_fallback=False):
            return None
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            print("‚ùå OPENAI_API_KEY not set; cannot embed.", file=sys.stderr)
            return None
        client = OpenAI(api_key=key)
        try:
            resp = client.embeddings.create(
                model=model,
                input=text,
            )
            return resp.data[0].embedding
        except Exception as exc:
            print(f"‚ö†Ô∏è Embedding failed: {exc}", file=sys.stderr)
            return None
    # Local embedding via Ollama
    if not ensure_ollama_running():
        return None
    try:
        res = requests.post(
            f"{OLLAMA_HOST}/api/embed",
            json={"model": model, "input": text},
            timeout=120,
        )
        if res.status_code == 404:
            print("‚ùå Ollama embeddings endpoint not available (404). Ensure your Ollama version supports /api/embed or use --openai.", file=sys.stderr)
            return None
        res.raise_for_status()
        data = res.json()
        vec = data.get("embedding") or data.get("embeddings")
        if not vec:
            print("‚ö†Ô∏è Ollama embeddings response missing 'embedding' field.", file=sys.stderr)
        return vec
    except Exception as exc:
        print(f"‚ö†Ô∏è Local embedding failed: {exc}", file=sys.stderr)
        return None


def parse_embedding(text: str) -> Optional[List[float]]:
    try:
        return json.loads(text) if text else None
    except Exception:
        return None


# ---- Lexicon / taxonomy helpers ----
def map_label_from_lexicon(conn: sqlite3.Connection, raw_label: Optional[str]) -> Optional[str]:
    if raw_label is None:
        return None
    if isinstance(raw_label, list):
        raw = ", ".join(str(x) for x in raw_label if x)
    else:
        raw = str(raw_label)
    raw = raw.strip()
    if not raw:
        return raw
    row = conn.execute(
        """
        SELECT normalized_label
        FROM normalization_lexicon
        WHERE lower(raw_label) = lower(?)
        LIMIT 1
        """,
        (raw,),
    ).fetchone()
    if row and row["normalized_label"]:
        return row["normalized_label"]
    return raw


def ollama_generate(prompt: str, model: str) -> Optional[str]:
    if not ensure_ollama_running():
        return None
    try:
        res = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "format": "json",
                "options": {"num_predict": 1024, "stop": ["\n\n", "\n###"]},
            },
            stream=True,
            timeout=120,
        )
        res.raise_for_status()
        chunks = []
        for line in res.iter_lines():
            if not line:
                continue
            try:
                obj = json.loads(line.decode("utf-8"))
                if "response" in obj:
                    chunks.append(obj["response"])
            except Exception:
                continue
        return "".join(chunks).strip()
    except Exception as exc:
        print(f"‚ö†Ô∏è Ollama request failed: {exc}", file=sys.stderr)
        return None


def simplify_entity_text(entity_type: str, name: str, raw_text: str, provider: str, model: str) -> Optional[str]:
    if not raw_text:
        return raw_text
    prompt = (
        "Rewrite the following noisy text into a short, plain-language description.\n"
        "Keep only high-signal details: what it is, who it's for, core topics/offerings.\n"
        "Remove navigation/UI/boilerplate, testimonials, pricing, CTA fluff.\n"
        "Stay factual; do not invent missing details.\n\n"
        f"ENTITY_TYPE: {entity_type}\n"
        f"NAME: {name}\n"
        f"RAW TEXT:\n{raw_text}\n"
    )
    if provider == "openai":
        if not ensure_openai_confirmation("entity simplification", allow_fallback=False):
            return raw_text
        key = os.getenv("OPENAI_API_KEY")
        if not key:
            return raw_text
        client = OpenAI(api_key=key)
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
            )
            return resp.choices[0].message.content.strip()
        except Exception:
            return raw_text
    # local / ollama
    text = ollama_generate(prompt, model)
    return text or raw_text


def map_topics_to_apple(conn: sqlite3.Connection, topics_val) -> str:
    if not topics_val:
        return ""
    # If apple categories table empty, just serialize strings
    has_cats = conn.execute("SELECT 1 FROM apple_podcast_categories LIMIT 1").fetchone()
    if not has_cats:
        if isinstance(topics_val, list):
            return ",".join([t if isinstance(t, str) else t.get("apple_subcategory_name") or t.get("apple_category_name") or "" for t in topics_val])
        return str(topics_val)
    rows = conn.execute("SELECT name, full_path FROM apple_podcast_categories").fetchall()
    names = {r["name"].lower(): r["full_path"] for r in rows}
    collected: List[str] = []
    def handle_item(item):
        if isinstance(item, list):
            for sub in item:
                handle_item(sub)
            return
        if isinstance(item, str):
            key = item.lower()
        elif isinstance(item, dict):
            key = (
                item.get("apple_subcategory_name")
                or item.get("apple_category_name")
                or item.get("apple_category_id")
                or ""
            ).lower()
        else:
            key = str(item).lower()
        if key in names:
            collected.append(names[key])
        elif key:
            collected.append(key)

    if isinstance(topics_val, list):
        handle_item(topics_val)
    else:
        handle_item(topics_val)
    return ",".join(collected)


def ingest_brand(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_brands_table(conn)

    website_url = args.url
    website_text, links = scrape_website_text(website_url)
    about = fetch_website_about(website_url)
    if about and about.get("about"):
        website_text = about["about"]
    socials = find_social_links(links)

    linkedin_text = ""
    if socials["linkedin"]:
        linkedin_text = scrape_linkedin_about(socials["linkedin"][0])

    social_raw = []
    twitter_bio = None
    instagram_bio = None
    if socials["twitter"]:
        twitter_bio = fetch_twitter_bio(socials["twitter"][0])
        if twitter_bio:
            social_raw.append({"twitter": twitter_bio})
    if socials["instagram"]:
        instagram_bio = fetch_instagram_bio(socials["instagram"][0])
        if instagram_bio:
            social_raw.append({"instagram": instagram_bio})
    if socials["website"]:
        about = fetch_website_about(website_url)
        if about:
            social_raw.append({"website": about})

    extracted = extract_brand_fields(website_text, linkedin_text)
    if not extracted:
        print("‚ùå GPT extraction failed or no OPENAI_API_KEY; brand not inserted.")
        return 1

    brand_name = args.brand_name or extracted.get("brand_name")
    def as_str(val):
        if val is None:
            return None
        if isinstance(val, (list, dict)):
            return json.dumps(val)
        return str(val)

    # Map extracted fields into both legacy extracted_* and new brand_* columns
    ex_description = as_str(extracted.get("summary"))
    ex_product_category = as_str(extracted.get("brand_vertical"))
    ex_target = as_str(extracted.get("brand_audience_profile"))
    ex_tone = None
    ex_key_themes = as_str(extracted.get("keywords"))
    ex_goals = as_str(extracted.get("brand_pain_points"))

    conn.execute(
        """
        INSERT INTO brands (
            brand_name, website_url, source_website_raw, source_linkedin_raw, source_social_raw,
            extracted_description, extracted_product_category, extracted_target_audience,
            extracted_tone, extracted_key_themes, extracted_goals,
            brand_vertical, brand_audience_profile, brand_use_cases, brand_pain_points, brand_geo_focus, brand_keywords, brand_summary,
            brand_vertical_override, flag_inconsistent_positioning,
            recommended_vertical, recommended_confidence, candidate_verticals,
            created_at, updated_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        ON CONFLICT(website_url) DO UPDATE SET
            brand_name=excluded.brand_name,
            source_website_raw=excluded.source_website_raw,
            source_linkedin_raw=excluded.source_linkedin_raw,
            source_social_raw=excluded.source_social_raw,
            extracted_description=excluded.extracted_description,
            extracted_product_category=excluded.extracted_product_category,
            extracted_target_audience=excluded.extracted_target_audience,
            extracted_tone=excluded.extracted_tone,
            extracted_key_themes=excluded.extracted_key_themes,
            extracted_goals=excluded.extracted_goals,
            brand_vertical=excluded.brand_vertical,
            brand_audience_profile=excluded.brand_audience_profile,
            brand_use_cases=excluded.brand_use_cases,
            brand_pain_points=excluded.brand_pain_points,
            brand_geo_focus=excluded.brand_geo_focus,
            brand_keywords=excluded.brand_keywords,
            brand_summary=excluded.brand_summary,
            brand_vertical_override=excluded.brand_vertical_override,
            flag_inconsistent_positioning=excluded.flag_inconsistent_positioning,
            recommended_vertical=excluded.recommended_vertical,
            recommended_confidence=excluded.recommended_confidence,
            candidate_verticals=excluded.candidate_verticals,
            updated_at=CURRENT_TIMESTAMP
        """,
        (
            as_str(brand_name),          # brand_name
            website_url,                 # website_url
            website_text,                # source_website_raw
            linkedin_text,               # source_linkedin_raw
            json.dumps(social_raw),      # source_social_raw
            ex_description,              # extracted_description
            ex_product_category,         # extracted_product_category
            ex_target,                   # extracted_target_audience
            ex_tone,                     # extracted_tone
            ex_key_themes,               # extracted_key_themes
            ex_goals,                    # extracted_goals
            as_str(extracted.get("brand_vertical")),          # brand_vertical
            as_str(extracted.get("brand_audience_profile")),  # brand_audience_profile
            as_str(extracted.get("brand_use_cases")),         # brand_use_cases
            as_str(extracted.get("brand_pain_points")),       # brand_pain_points
            as_str(extracted.get("brand_geo_focus")),         # brand_geo_focus
            as_str(extracted.get("keywords")),                # brand_keywords
            as_str(extracted.get("summary")),                 # brand_summary
            None,                                             # brand_vertical_override (manual)
            1 if extracted.get("flag_inconsistent_positioning") else 0,  # flag
            as_str(extracted.get("recommended_vertical")),
            float(extracted.get("recommended_confidence")) if extracted.get("recommended_confidence") is not None else None,
            json.dumps(extracted.get("candidate_verticals")) if extracted.get("candidate_verticals") is not None else None,
        ),
    )
    conn.commit()
    # Fetch inserted/updated row for optional test output
    row = conn.execute("SELECT * FROM brands WHERE website_url = ?", (website_url,)).fetchone()
    adlid = row["adlid"] if row else None
    print(f"‚úÖ Stored brand: {brand_name or website_url}")

    if args.write_test and row and adlid:
        out_dir = Path("tests")
        out_dir.mkdir(parents=True, exist_ok=True)
        txt_path = out_dir / f"test-enrich-brand-adlid-{adlid}_cleaned.txt"
        txt_path.write_text(build_brand_text_block(row))
        print(f"üìù Wrote {txt_path}")
    conn.close()
    return 0



def test_compare(args: argparse.Namespace) -> int:
    def load_text(txt: Optional[str], path: Optional[Path]) -> Optional[str]:
        if txt:
            return txt
        if path and path.exists():
            return path.read_text()
        return None

    if hasattr(args, "brand_id") and getattr(args, "brand_id", None) and hasattr(args, "creator_id") and getattr(args, "creator_id", None):
        db_path = Path(getattr(args, "adl_db", "adelined_matching.db"))
        if not db_path.exists():
            print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
            return 1
        conn = sqlite3.connect(db_path, timeout=10)
        conn.row_factory = sqlite3.Row
        configure_sqlite(conn)
        bmap = load_embeddings_map(conn, "brand", args.brand_id)
        cmap = load_embeddings_map(conn, "creator", args.creator_id)
        if not bmap or not cmap:
            print("‚ùå Missing embeddings for provided ids.", file=sys.stderr)
            return 1
        weights = {
            "semantic_summary": 0.25,
            "audience": 0.25,
            "pain_points": 0.15,
            "vertical": 0.15,
            "topics": 0.10,
            "tone": 0.05,
            "geo": 0.05,
        }
        sims = {}
        for vt, w in weights.items():
            if vt in bmap and vt in cmap:
                sims[vt] = cosine_similarity(bmap[vt], cmap[vt])
        num = sum(sims.get(vt, 0) * w for vt, w in weights.items() if vt in sims)
        denom = sum(w for vt, w in weights.items() if vt in sims)
        final_score = num / denom if denom else 0.0
        print("TEST COMPARISON RESULT")
        print("======================")
        print(f"Final score: {final_score:.3f}")
        print(f"Interpretation: {relevance_label(final_score)}")
        print("\nPer-dimension:")
        for vt, val in sims.items():
            print(f"  {vt:16s}: {val:.3f}")
        conn.close()
        return 0

    t1 = load_text(args.text1, args.file1)
    t2 = load_text(args.text2, args.file2)
    if not t1 or not t2:
        print("‚ùå Provide both inputs via --text1/--file1 and --text2/--file2", file=sys.stderr)
        return 1

    v1 = embed_text_block(t1)
    v2 = embed_text_block(t2)
    if not v1 or not v2:
        print("‚ùå Failed to embed one or both inputs.", file=sys.stderr)
        return 1

    score = cosine_similarity(v1, v2)
    print("TEST COMPARISON RESULT")
    print("======================")
    print(f"Similarity score: {score:.3f}")
    print(f"Interpretation: {relevance_label(score)}")
    return 0


def export_entity(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    target = args.target
    entity_id = args.id
    out_path = args.out or Path(f"tests/export-{target}-{entity_id}.txt")
    out_path.parent.mkdir(parents=True, exist_ok=True)

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        use_norm = getattr(args, "usenorm", False)
        if target == "creator":
            row = conn.execute(
                "SELECT cleaned_text_block, normalized_text FROM creator_enrichment WHERE podcastindex_id = ?",
                (entity_id,),
            ).fetchone()
            if not row:
                print(f"‚ùå No enrichment found for creator pid={entity_id}", file=sys.stderr)
                return 1
            text = None
            if use_norm and row["normalized_text"]:
                text = row["normalized_text"]
            elif row["cleaned_text_block"]:
                text = row["cleaned_text_block"]
            if not text:
                print(f"‚ùå No text found for creator pid={entity_id}", file=sys.stderr)
                return 1
            out_path.write_text(text)
            print(f"üìù Exported creator pid={entity_id} {'normalized' if use_norm and row['normalized_text'] else 'cleaned'} block to {out_path}")
            return 0
        if target == "brand":
            row = conn.execute(
                "SELECT * FROM brands WHERE adlid = ?",
                (entity_id,),
            ).fetchone()
            if not row:
                print(f"‚ùå No brand found for adlid={entity_id}", file=sys.stderr)
                return 1
            text_block = None
            if use_norm and row["normalized_summary"]:
                parts = [
                    f"NAME: {row['brand_name'] or ''}",
                    f"SUMMARY:\n{row['normalized_summary'] or ''}",
                    f"VERTICAL:\n{row['normalized_vertical'] or ''}",
                    f"AUDIENCE:\n{row['normalized_audience'] or ''}",
                    f"USE_CASES:\n{row['normalized_use_cases'] or ''}",
                    f"PAIN_POINTS:\n{row['normalized_pain_points'] or ''}",
                    f"THEMES:\n{row['normalized_themes'] or ''}",
                    f"TONE:\n{row['normalized_tone'] or ''}",
                    f"GEO:\n{row['normalized_geo'] or ''}",
                    f"TOPICS:\n{row['normalized_topics'] or ''}",
                ]
                text_block = "\n".join(parts)
            else:
                text_block = build_brand_text_block(row)
            out_path.write_text(text_block)
            print(f"üìù Exported brand adlid={entity_id} {'normalized' if use_norm and row['normalized_summary'] else 'cleaned'} block to {out_path}")
            return 0
        print(f"‚ùå Unknown target: {target}", file=sys.stderr)
        return 1
    finally:
        conn.close()


def get_normalized_entity(conn: sqlite3.Connection, entity_type: str, entity_id: int) -> Optional[sqlite3.Row]:
    return conn.execute(
        """
        SELECT *
        FROM entity_normalized
        WHERE entity_type=? AND entity_id=? AND version='v1'
        """,
        (entity_type, entity_id),
    ).fetchone()


def normalize_entities(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_creators_table(conn)
    ensure_creator_enrichment_table(conn)
    ensure_brands_table(conn)
    ensure_entity_normalized_table(conn)
    ensure_normalization_lexicon_table(conn)
    ensure_apple_categories_table(conn)

    entity_type = args.entity_type
    provider = getattr(args, "llm_provider", "local")
    ollama_model = getattr(args, "ollama_model", LOCAL_OLLAMA_MODEL)
    ids: List[int] = []
    if getattr(args, "id", None):
        ids = [args.id]
    elif getattr(args, "id_range", None):
        start_id, end_id = args.id_range
        if entity_type == "brand":
            rows = conn.execute(
                "SELECT adlid FROM brands WHERE adlid BETWEEN ? AND ? ORDER BY adlid",
                (start_id, end_id),
            ).fetchall()
            ids = [r[0] for r in rows]
        else:
            rows = conn.execute(
                "SELECT podcastindex_id FROM creators WHERE podcastindex_id BETWEEN ? AND ? ORDER BY podcastindex_id",
                (start_id, end_id),
            ).fetchall()
            ids = [r[0] for r in rows]
    else:
        if entity_type == "brand" and getattr(args, "all_brands", False):
            rows = conn.execute(
                "SELECT adlid FROM brands LIMIT ? OFFSET ?",
                (args.limit, args.offset),
            ).fetchall()
            ids = [r[0] for r in rows]
        if entity_type == "creator" and getattr(args, "all_creators", False):
            rows = conn.execute(
                "SELECT podcastindex_id FROM creators LIMIT ? OFFSET ?",
                (args.limit, args.offset),
            ).fetchall()
            ids = [r[0] for r in rows]
    if not ids:
        print("‚ùå Provide --id or --all-brands/--all-creators", file=sys.stderr)
        conn.close()
        return 1

    # Provide a short hint list of Apple categories if present
    cat_rows = conn.execute(
        "SELECT full_path FROM apple_podcast_categories LIMIT 30"
    ).fetchall()
    categories_hint = "\n".join([r["full_path"] for r in cat_rows]) if cat_rows else ""

    # If provider is openai, confirm once; if declined and fallback allowed, switch to local.
    if provider == "openai":
        if not ensure_openai_confirmation("normalization", allow_fallback=True):
            provider = "local"

    normalized = 0
    attempted = 0
    for eid in ids:
        exists = conn.execute(
            "SELECT 1 FROM entity_normalized WHERE entity_type=? AND entity_id=? AND version='v1'",
            (entity_type, eid),
        ).fetchone()
        if exists and not getattr(args, "force", False):
            continue
        name = ""
        fields_parts: List[str] = []
        if entity_type == "brand":
            row = conn.execute("SELECT * FROM brands WHERE adlid=?", (eid,)).fetchone()
            if not row:
                continue
            name = row["brand_name"] or f"brand {eid}"
            for key in ["brand_summary", "brand_vertical", "brand_audience_profile", "brand_use_cases", "brand_pain_points", "brand_keywords", "source_website_raw", "source_linkedin_raw"]:
                if row[key]:
                    fields_parts.append(f"{key.upper()}: {row[key]}")
        else:
            row = conn.execute(
                """
                SELECT c.*, ce.cleaned_text_block, ce.raw_text_sources
                FROM creators c
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                WHERE c.podcastindex_id=?
                """,
                (eid,),
            ).fetchone()
            if not row:
                continue
            name = row["title"] or f"podcast {eid}"
            for key in ["title", "description", "categories", "cleaned_text_block"]:
                if row.get(key) if isinstance(row, dict) else row[key]:
                    fields_parts.append(f"{key.upper()}: {row[key]}")
            try:
                raw_sources = json.loads(row["raw_text_sources"]) if row["raw_text_sources"] else {}
            except Exception:
                raw_sources = {}
            for k, v in (raw_sources or {}).items():
                if v:
                    fields_parts.append(f"{k.upper()}: {v}")
        raw_blob = "\n".join(fields_parts)
        if len(raw_blob) > 12000:
            raw_blob = raw_blob[:12000]
        simplified = simplify_entity_text(entity_type, name, raw_blob, provider, ollama_model) or raw_blob
        fields_text = f"SIMPLIFIED:\n{simplified}\n\nRAW:\n{raw_blob}"
        debug_tag = f"{entity_type}:{eid}"
        attempted += 1
        norm = normalize_entity_to_json(entity_type, name, fields_text, categories_hint, provider=provider, model=ollama_model, debug_tag=debug_tag)
        if not norm or not norm.get("summary"):
            print(f"‚ö†Ô∏è Normalization failed for {entity_type} {eid}")
            continue
        # Coerce and map labels through lexicon where possible
        for key in ["vertical", "audience", "themes", "use_cases", "pain_points", "tone", "geo"]:
            norm[key] = map_label_from_lexicon(conn, norm.get(key))
        topics_val = norm.get("topics")
        topics_str = map_topics_to_apple(conn, topics_val)
        # Build a canonical normalized block for reference/embedding
        block_parts = [
            f"SUMMARY:\n{norm.get('summary','')}",
            f"VERTICAL:\n{norm.get('vertical','')}",
            f"AUDIENCE:\n{norm.get('audience','')}",
            f"PAIN_POINTS:\n{norm.get('pain_points','')}",
            f"USE_CASES:\n{norm.get('use_cases','')}",
            f"THEMES:\n{norm.get('themes','')}",
            f"TONE:\n{norm.get('tone','')}",
            f"GEO:\n{norm.get('geo','')}",
            f"TOPICS:\n{topics_str}",
        ]
        normalized_block = "\n".join(block_parts)
        conn.execute(
            """
            INSERT OR REPLACE INTO entity_normalized (entity_type, entity_id, version, normalized_block, norm_fields)
            VALUES (?, ?, 'v1', ?, ?)
            """,
            (entity_type, eid, normalized_block, json.dumps(norm)),
        )
        # persist normalized_* columns
        def to_str(val):
            if val is None:
                return None
            if isinstance(val, list):
                return ", ".join(str(x) for x in val if x)
            return str(val)

        if entity_type == "brand":
            conn.execute(
                """
                UPDATE brands SET
                    normalized_summary=?, normalized_vertical=?, normalized_audience=?, normalized_pain_points=?,
                    normalized_use_cases=?, normalized_themes=?, normalized_tone=?, normalized_geo=?, normalized_topics=?
                WHERE adlid=?
                """,
                (
                    to_str(norm.get("summary")),
                    to_str(norm.get("vertical")),
                    to_str(norm.get("audience")),
                    to_str(norm.get("pain_points")),
                    to_str(norm.get("use_cases")),
                    to_str(norm.get("themes")),
                    to_str(norm.get("tone")),
                    to_str(norm.get("geo")),
                    to_str(topics_str),
                    eid,
                ),
            )
        else:
            conn.execute(
                """
                UPDATE creators SET
                    normalized_summary=?, normalized_vertical=?, normalized_audience=?, normalized_pain_points=?,
                    normalized_use_cases=?, normalized_themes=?, normalized_tone=?, normalized_geo=?, normalized_topics=?
                WHERE podcastindex_id=?
                """,
                (
                    to_str(norm.get("summary")),
                    to_str(norm.get("vertical")),
                    to_str(norm.get("audience")),
                    to_str(norm.get("pain_points")),
                    to_str(norm.get("use_cases")),
                    to_str(norm.get("themes")),
                    to_str(norm.get("tone")),
                    to_str(norm.get("geo")),
                    to_str(topics_str),
                    eid,
                ),
            )
        normalized += 1
        if normalized % 10 == 0:
            conn.commit()
            print(f"Normalized {normalized}/{len(ids)}...", end="\r")
    conn.commit()
    print(f"\n‚úÖ Normalized {normalized} entities of {attempted} attempted.")
    conn.close()
    return 0


def build_embedding_text(entity: sqlite3.Row, entity_type: str, vector_type: str) -> Optional[str]:
    summary = entity["normalized_summary"] or ""
    vertical = entity["normalized_vertical"] or ""
    audience = entity["normalized_audience"] or ""
    pains = entity["normalized_pain_points"] or ""
    use_cases = entity["normalized_use_cases"] or ""
    themes = entity["normalized_themes"] or ""
    tone = entity["normalized_tone"] or ""
    geo = entity["normalized_geo"] or ""
    topics = entity["normalized_topics"] or ""
    if vector_type == "semantic_summary":
        return f"SUMMARY:\n{summary}\nVERTICAL:\n{vertical}\nAUDIENCE:\n{audience}\nTHEMES:\n{themes}"
    if vector_type == "topics":
        return f"TOPICS (APPLE CATEGORIES):\n{topics}"
    if vector_type == "vertical":
        return f"INDUSTRY / VERTICAL:\n{vertical}"
    if vector_type == "audience":
        return f"AUDIENCE:\n{audience}\nGEO:\n{geo}"
    if vector_type == "pain_points":
        return f"PAIN POINTS:\n{pains}"
    if vector_type == "use_cases":
        return f"USE CASES:\n{use_cases}"
    if vector_type == "themes":
        return f"THEMES:\n{themes}"
    if vector_type == "tone":
        return f"TONE:\n{tone}"
    if vector_type == "geo":
        return f"GEO:\n{geo}"
    return None


def compute_embedding_confidence(entity: sqlite3.Row) -> float:
    score = 1.0
    def missing(val: Optional[str]) -> bool:
        return not val or not str(val).strip()
    if missing(entity["normalized_vertical"]):
        score -= 0.25
    if missing(entity["normalized_audience"]):
        score -= 0.2
    if missing(entity["normalized_summary"]):
        score -= 0.15
    if missing(entity["normalized_topics"]):
        score -= 0.1
    if missing(entity["normalized_pain_points"]):
        score -= 0.05
    if score < 0:
        score = 0.0
    if score > 1:
        score = 1.0
    return score


def embed_entity(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_entity_normalized_table(conn)
    ensure_entity_embeddings_table(conn)
    ensure_creators_table(conn)
    ensure_brands_table(conn)

    vector_types = [
        "semantic_summary",
        "topics",
        "vertical",
        "audience",
        "pain_points",
        "use_cases",
        "themes",
        "tone",
        "geo",
    ]
    entity_type = args.entity_type
    embedding_provider = "openai" if getattr(args, "openai", False) else getattr(args, "embedding_provider", "local")
    embedding_model = getattr(args, "embedding_model", LOCAL_OLLAMA_MODEL)
    if embedding_provider == "openai":
        if not ensure_openai_confirmation("embeddings", allow_fallback=False):
            conn.close()
            return 1
        if not embedding_model or embedding_model == LOCAL_OLLAMA_MODEL:
            embedding_model = "text-embedding-3-large"
    # Optional bulk delete for this entity type
    if getattr(args, "refreshall", False):
        confirm = input(f"Type YES to delete all embeddings for {entity_type}s: ").strip()
        if confirm != "YES":
            print("Aborted.")
            conn.close()
            return 1
        conn.execute("DELETE FROM entity_embeddings WHERE entity_type=?", (entity_type,))
        conn.commit()
        print(f"üóëÔ∏è Deleted all {entity_type} embeddings.")

    base_query_creator = """
        SELECT en.id as norm_id, en.normalized_block, c.*
        FROM entity_normalized en
        JOIN creators c ON c.podcastindex_id = en.entity_id
        WHERE en.entity_type='creator' AND en.version='v1'
    """
    base_query_brand = """
        SELECT en.id as norm_id, en.normalized_block, b.*
        FROM entity_normalized en
        JOIN brands b ON b.adlid = en.entity_id
        WHERE en.entity_type='brand' AND en.version='v1'
    """

    rows: List[sqlite3.Row]
    if args.id:
        rows = conn.execute(
            base_query_creator + " AND en.entity_id=?" if entity_type == "creator" else base_query_brand + " AND en.entity_id=?",
            (args.id,),
        ).fetchall()
    else:
        rows = conn.execute(
            (base_query_creator if entity_type == "creator" else base_query_brand) + " LIMIT ? OFFSET ?",
            (args.limit, args.offset),
        ).fetchall()
    if not rows:
        print("No normalized records to embed.")
        conn.close()
        return 0
    model_name = "text-embedding-3-large"
    inserted = 0
    for r in rows:
        ent_id = r["podcastindex_id"] if entity_type == "creator" else r["adlid"]
        conn.execute(
            "DELETE FROM entity_embeddings WHERE entity_type=? AND entity_id=?",
            (entity_type, ent_id),
        )
        confidence = compute_embedding_confidence(r)
        for vt in vector_types:
            text = build_embedding_text(r, entity_type, vt)
            if not text or not text.strip():
                continue
            vec = embed_text_block(text, provider=embedding_provider, model=embedding_model)
            if not vec:
                continue
            conn.execute(
                """
                INSERT INTO entity_embeddings (entity_type, entity_id, vector_type, embedding_vector, confidence, source_text, source_normalized_id, model_name)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (entity_type, ent_id, vt, json.dumps(vec), confidence, text[:4000], r["norm_id"], model_name),
            )
            inserted += 1
    conn.commit()
    print(f"‚úÖ Embedded {inserted} vectors for {len(rows)} {entity_type}(s).")
    conn.close()
    return 0


def load_embeddings_map(conn: sqlite3.Connection, entity_type: str, entity_id: int) -> Dict[str, List[float]]:
    rows = conn.execute(
        """
        SELECT vector_type, embedding_vector
        FROM entity_embeddings
        WHERE entity_type=? AND entity_id=?
        """,
        (entity_type, entity_id),
    ).fetchall()
    result = {}
    for r in rows:
        vec = parse_embedding(r["embedding_vector"])
        if vec:
            result[r["vector_type"]] = vec
    return result

def test_enrich(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    pid = args.pid
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    base_row = conn.execute(
        """
        SELECT p.id AS podcastindex_id,
               p.title AS pi_title,
               p.description AS pi_description,
               p.url AS feed_url,
               p.link AS web_link,
               p.category1, p.category2, p.category3, p.category4, p.category5,
               p.category6, p.category7, p.category8, p.category9, p.category10,
               c.itunesid
        FROM creators c
        LEFT JOIN podcastindex_feeds p ON p.id = c.podcastindex_id
        WHERE c.podcastindex_id = ?
        """,
        (pid,),
    ).fetchone()
    if not base_row:
        print(f"‚ùå No creator/podcastindex row found for podcastindex_id={pid}", file=sys.stderr)
        conn.close()
        return 1

    base = dict(base_row)
    print(f"üîé Test-enrich for podcastindex_id={pid} title='{base.get('pi_title') or ''}'")

    title_for_lookup = base.get("pi_title") or ""
    spotify_url = base.get("feed_url") if (base.get("feed_url") or "").startswith("https://open.spotify.com/show/") else None

    print("‚Üí Fetching Apple metadata...")
    apple_meta = fetch_apple_metadata(base.get("itunesid"), title_for_lookup)
    print("‚Üí Fetching Spotify metadata...")
    spotify_meta = fetch_spotify_metadata(spotify_url, title_for_lookup)
    print("‚Üí Fetching YouTube metadata...")
    youtube_meta = fetch_youtube_metadata(title_for_lookup, mode="lean")
    print("‚Üí Fetching social bios...")
    social_meta = fetch_social_bios_from_sources(base, apple_meta, spotify_meta, youtube_meta)

    print("‚Üí Building text block...")
    raw_pieces = []
    for val in [
        base.get("pi_description"),
        apple_meta.get("description"),
        (spotify_meta.get("show", {}) if spotify_meta else {}).get("description"),
        (youtube_meta.get("channel", {}) if youtube_meta else {}).get("about"),
        (social_meta.get("website") or {}).get("about") if social_meta else None,
    ]:
        if val:
            raw_pieces.append(val)
    if spotify_meta.get("episodes_sample"):
        ep_titles = [ep.get("name") or "" for ep in spotify_meta["episodes_sample"] if ep]
        raw_pieces.append("; ".join(ep_titles[:30]))
    combined_raw = "\n\n".join(raw_pieces)
    summary = extract_creator_metadata(combined_raw) if combined_raw else {}
    cleaned_text, raw_sources = build_text_block(
        base=base,
        apple=apple_meta,
        spotify=spotify_meta,
        youtube=youtube_meta,
        social=social_meta,
        extracted=summary,
    )

    out_dir = Path("tests")
    out_dir.mkdir(parents=True, exist_ok=True)
    base_name = f"test-enrich-pid-{pid}"
    json_path = out_dir / f"{base_name}.json"
    txt_path = out_dir / f"{base_name}_cleaned.txt"

    payload = {
        "podcastindex_id": pid,
        "podcast_title": base.get("pi_title"),
        "base": base,
        "apple": apple_meta,
        "spotify": spotify_meta,
        "youtube": youtube_meta,
        "social": social_meta,
        "raw_text_sources": raw_sources,
        "cleaned_text_block": cleaned_text,
        "summary": summary,
    }
    json_path.write_text(json.dumps(payload, indent=2))
    txt_path.write_text(cleaned_text or "")
    print(f"‚úÖ Wrote {json_path} and {txt_path}")
    conn.close()
    return 0


def cosine_similarity(a: List[float], b: List[float]) -> float:
    va = np.array(a, dtype=float).ravel()
    vb = np.array(b, dtype=float).ravel()
    if va.shape != vb.shape or va.size == 0:
        return 0.0
    denom = (np.linalg.norm(va) * np.linalg.norm(vb))
    if denom == 0:
        return 0.0
    return float(np.dot(va, vb) / denom)


def format_table(rows: List[Tuple[str, float, str, str]], header: Tuple[str, str, str, str]) -> str:
    cols = list(zip(*([header] + rows)))
    widths = [max(len(str(cell)) for cell in col) for col in cols]
    def fmt_row(row):
        return " | ".join(str(cell).ljust(widths[i]) for i, cell in enumerate(row))
    sep = "-+-".join("-" * w for w in widths)
    lines = [fmt_row(header), sep]
    for r in rows:
        lines.append(fmt_row(r))
    return "\n".join(lines)


def relevance_label(score: float) -> str:
    if score >= 0.8:
        return "strong"
    if score >= 0.7:
        return "medium"
    return "weak"


# Override with multi-dimensional ranking using entity_embeddings
def rank_entities(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_entity_embeddings_table(conn)
    limit = args.limit
    adlid = args.adlid
    weights = {
        "semantic_summary": 0.25,
        "audience": 0.25,
        "pain_points": 0.15,
        "vertical": 0.15,
        "topics": 0.10,
        "tone": 0.05,
        "geo": 0.05,
    }

    def score_pair(bmap: Dict[str, List[float]], cmap: Dict[str, List[float]]) -> Tuple[float, Dict[str, float]]:
        sims = {}
        for vt, w in weights.items():
            if vt in bmap and vt in cmap:
                sims[vt] = cosine_similarity(bmap[vt], cmap[vt])
        num = sum(sims.get(vt, 0) * w for vt, w in weights.items() if vt in sims)
        denom = sum(w for vt, w in weights.items() if vt in sims)
        return (num / denom if denom else 0.0), sims

    if args.bybrand:
        brand = conn.execute("SELECT adlid, brand_name FROM brands WHERE adlid = ?", (adlid,)).fetchone()
        if not brand:
            print(f"‚ùå Brand adlid {adlid} not found.", file=sys.stderr)
            return 1
        bmap = load_embeddings_map(conn, "brand", adlid)
        if not bmap:
            print(f"‚ùå No embeddings for brand adlid {adlid}. Run normalize-entity then embed-entity.", file=sys.stderr)
            return 1
        creator_rows = conn.execute(
            """
            SELECT c.podcastindex_id, COALESCE(c.title, pf.title, '') AS name
            FROM creators c
            LEFT JOIN podcastindex_feeds pf ON pf.id = c.podcastindex_id
            WHERE EXISTS (SELECT 1 FROM entity_embeddings ee WHERE ee.entity_type='creator' AND ee.entity_id=c.podcastindex_id)
            """
        ).fetchall()
        scored = []
        for r in creator_rows:
            cmap = load_embeddings_map(conn, "creator", r["podcastindex_id"])
            if not cmap:
                continue
            final_score, sims = score_pair(bmap, cmap)
            top_dims = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:2]
            notes = "; ".join([f"{d}:{v:.2f}" for d, v in top_dims])
            scored.append((r["podcastindex_id"], r["name"] or "", final_score, relevance_label(final_score), notes))
        scored.sort(key=lambda x: x[2], reverse=True)
        rows = scored[:limit]
        print(f"Ranking by Brand: adlid={adlid}, brand=\"{brand['brand_name'] or ''}\"")
        table = format_table(rows, ("id", "podcast_title", "score", "relevance", "top_dims"))
        print(table)
        if getattr(args, "csv", False):
            out_name = f"{datetime.utcnow():%Y%m%d}-brand{adlid}.csv"
            with open(out_name, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["id", "podcast_title", "score", "relevance", "top_dims"])
                for r in rows:
                    writer.writerow(r)
            print(f"üíæ Wrote {out_name}")
        conn.close()
        return 0

    if args.bycreator:
        creator = conn.execute(
            "SELECT c.podcastindex_id, c.title AS name FROM creators c WHERE c.podcastindex_id = ?",
            (adlid,),
        ).fetchone()
        if not creator:
            print(f"‚ùå Creator adlid {adlid} not found.", file=sys.stderr)
            return 1
        cmap = load_embeddings_map(conn, "creator", adlid)
        if not cmap:
            print(f"‚ùå No embeddings for creator adlid {adlid}. Run normalize-entity then embed-entity.", file=sys.stderr)
            return 1
        brand_rows = conn.execute(
            """
            SELECT b.adlid, COALESCE(b.brand_name, '') AS brand_name
            FROM brands b
            WHERE EXISTS (SELECT 1 FROM entity_embeddings ee WHERE ee.entity_type='brand' AND ee.entity_id=b.adlid)
            """
        ).fetchall()
        scored = []
        for r in brand_rows:
            bmap = load_embeddings_map(conn, "brand", r["adlid"])
            if not bmap:
                continue
            final_score, sims = score_pair(bmap, cmap)
            top_dims = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:2]
            notes = "; ".join([f"{d}:{v:.2f}" for d, v in top_dims])
            scored.append((r["adlid"], r["brand_name"] or "", final_score, relevance_label(final_score), notes))
        scored.sort(key=lambda x: x[2], reverse=True)
        rows = scored[:limit]
        print(f"Ranking by Creator: adlid={adlid}, podcast=\"{creator['name'] or ''}\"")
        table = format_table(rows, ("id", "brand_name", "score", "relevance", "notes"))
        print(table)
        if getattr(args, "csv", False):
            out_name = f"{datetime.utcnow():%Y%m%d}-creator{adlid}.csv"
            with open(out_name, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["id", "brand_name", "score", "relevance", "notes"])
                for r in rows:
                    writer.writerow(r)
            print(f"üíæ Wrote {out_name}")
        conn.close()
        return 0

    print("‚ùå Must specify --bybrand or --bycreator", file=sys.stderr)
    conn.close()
    return 1


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Adelined data CLI")
    subparsers = parser.add_subparsers(dest="action", required=True)

    fetch_parser = subparsers.add_parser("fetch", help="Fetch creator data from a source")
    fetch_parser.add_argument(
        "source",
        choices=["youtube", "spotify"],
        help="Data source to pull from",
    )
    fetch_parser.add_argument(
        "--output",
        type=Path,
        help="Optional JSON output file to write results",
    )

    # YouTube options
    fetch_parser.add_argument(
        "--channels",
        help="Comma-separated YouTube channel handles/IDs/URLs to fetch (for source youtube)",
    )
    fetch_parser.add_argument(
        "--channels-file",
        type=Path,
        help="Path to newline-delimited list of YouTube channel handles/IDs/URLs",
    )

    # Spotify options
    fetch_parser.add_argument(
        "--shows",
        help="Comma-separated Spotify show IDs or URLs to fetch (for source spotify)",
    )
    fetch_parser.add_argument(
        "--shows-file",
        type=Path,
        help="Path to newline-delimited list of Spotify show IDs or URLs",
    )

    ingest_parser = subparsers.add_parser("ingest", help="Ingest data into the Adelined DB")
    ingest_parser.add_argument(
        "source",
        choices=["podcastindex", "apple"],
        help="Dataset to ingest",
    )
    ingest_parser.add_argument(
        "--source-db",
        type=Path,
        default=Path("data/podcastindex_feeds.db"),
        help="Path to source SQLite database (default: data/podcastindex_feeds.db)",
    )
    ingest_parser.add_argument(
        "--target-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to target Adelined SQLite database (default: adelined_matching.db)",
    )
    ingest_parser.add_argument(
        "--refresh-feeds",
        action="store_true",
        help="Refresh podcastindex_feeds from the source DB before filtering into creators (default: use existing feeds only)",
    )
    ingest_parser.add_argument(
        "--batch-size",
        type=int,
        default=5000,
        help="Batch size for streaming copy (default: 5000)",
    )

    enrich_parser = subparsers.add_parser("enrich", help="Enrich creators with external signals")
    enrich_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    enrich_parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="Max creators to process in this run (default: 100)",
    )
    enrich_parser.add_argument(
        "--offset",
        type=int,
        default=0,
        help="Offset into creators table (default: 0)",
    )
    enrich_parser.add_argument(
        "--only-id",
        type=int,
        help="Optional specific podcastindex_id to process",
    )
    enrich_parser.add_argument(
        "--pidrange",
        nargs=2,
        type=int,
        metavar=("START_PID", "END_PID"),
        help="Optional inclusive podcastindex_id range to process (START_PID END_PID)",
    )
    enrich_parser.add_argument(
        "--sources",
        help="Comma-separated sources to fetch (default: all). Options: spotify,apple,youtube,social",
    )

    discover_parser = subparsers.add_parser("discover", help="Discover new creators from external sources")
    discover_parser.add_argument(
        "source",
        choices=["apple"],
        help="Discovery source",
    )
    discover_parser.add_argument(
        "--terms",
        help="Comma-separated search terms",
    )
    discover_parser.add_argument(
        "--terms-file",
        type=Path,
        help="File with search terms (one per line)",
    )
    discover_parser.add_argument(
        "--limit-per-term",
        type=int,
        default=200,
        help="Results per page (max 200)",
    )
    discover_parser.add_argument(
        "--max-pages",
        type=int,
        default=5,
        help="Max pages per term",
    )
    discover_parser.add_argument(
        "--country",
        default="GB",
        help="iTunes country code (default: GB)",
    )
    discover_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    report_parser = subparsers.add_parser("enrichment", help="Manage or inspect enrichment data")
    report_sub = report_parser.add_subparsers(dest="enrichment_action", required=True)

    clear_parser = report_sub.add_parser("clear", help="Clear creator_enrichment table (requires confirmation)")
    clear_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    show_parser = report_sub.add_parser("show", help="Show enrichment summaries or source data")
    show_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    show_parser.add_argument(
        "--source",
        choices=["spotify", "apple", "youtube", "social", "twitter", "instagram", "website", "patreon"],
        help="Specific source to list; omit for summary counts",
    )
    show_parser.add_argument(
        "--limit",
        type=int,
        default=5,
        help="Max rows to display (default: 5)",
    )

    summary_parser = report_sub.add_parser("summary", help="Report overall enrichment coverage")
    summary_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    brand_parser = subparsers.add_parser("brand", help="Ingest a brand by URL")
    brand_parser.add_argument(
        "--url",
        required=True,
        help="Brand website URL",
    )
    brand_parser.add_argument(
        "--brand-name",
        help="Optional brand name override",
    )
    brand_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    brand_parser.add_argument(
        "--write-test",
        action="store_true",
        help="Write test brand embedding text to tests/test-enrich-brand-adlid-<id>_cleaned.txt",
    )

    norm_entity_parser = subparsers.add_parser("normalize-entity", help="Normalize a brand or creator into structured fields")
    norm_entity_parser.add_argument(
        "--entity-type",
        required=True,
        choices=["brand", "creator"],
        help="Which entity type to normalize",
    )
    norm_entity_parser.add_argument("--id", type=int, help="Specific id (adlid for brand, podcastindex_id for creator)")
    norm_entity_parser.add_argument("--all-brands", action="store_true", help="Normalize all brands (paged with limit/offset)")
    norm_entity_parser.add_argument("--all-creators", action="store_true", help="Normalize all creators (paged with limit/offset)")
    norm_entity_parser.add_argument(
        "--id-range",
        nargs=2,
        type=int,
        metavar=("START_ID", "END_ID"),
        help="Normalize a contiguous id range (inclusive). Uses adlid for brands, podcastindex_id for creators.",
    )
    norm_entity_parser.add_argument("--limit", type=int, default=100, help="Max rows to process (default 100)")
    norm_entity_parser.add_argument("--offset", type=int, default=0, help="Offset for bulk mode")
    norm_entity_parser.add_argument(
        "--llm-provider",
        choices=["local", "openai"],
        default="local",
        help="Provider for normalization LLM (default: local via Ollama)",
    )
    norm_entity_parser.add_argument(
        "--ollama-model",
        default=LOCAL_OLLAMA_MODEL,
        help="Ollama model name for local normalization (default from OLLAMA_MODEL or phi3.5)",
    )
    norm_entity_parser.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing normalized rows for these entities",
    )
    norm_entity_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    embed_entity_parser = subparsers.add_parser("embed-entity", help="Embed normalized brands or creators into per-dimension vectors")
    embed_entity_parser.add_argument(
        "--entity-type",
        required=True,
        choices=["brand", "creator"],
        help="Which entity type to embed",
    )
    embed_entity_parser.add_argument("--id", type=int, help="Specific id (adlid for brand, podcastindex_id for creator)")
    embed_entity_parser.add_argument("--limit", type=int, default=100, help="Max rows to embed (default 100)")
    embed_entity_parser.add_argument("--offset", type=int, default=0, help="Offset for bulk mode")
    embed_entity_parser.add_argument(
        "--refreshall",
        action="store_true",
        help="Delete existing embeddings for this entity type before embedding (requires typing YES)",
    )
    embed_entity_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    embed_entity_parser.add_argument(
        "--embedding-provider",
        choices=["openai", "local"],
        default="local",
        help="Provider for embeddings (default: local via Ollama; use openai to call OpenAI embeddings)",
    )
    embed_entity_parser.add_argument(
        "--openai",
        action="store_true",
        help="Shortcut to set --embedding-provider openai",
    )
    embed_entity_parser.add_argument(
        "--embedding-model",
        default=LOCAL_OLLAMA_MODEL,
        help="Embedding model name (for local default uses OLLAMA_MODEL or phi3.5; for openai defaults to text-embedding-3-large)",
    )

    export_parser = subparsers.add_parser("export", help="Export a stored embedding text block to a file")
    export_parser.add_argument(
        "--target",
        required=True,
        choices=["creator", "brand"],
        help="Which entity to export",
    )
    export_parser.add_argument(
        "--id",
        required=True,
        type=int,
        help="podcastindex_id for creator or adlid for brand",
    )
    export_parser.add_argument(
        "--out",
        type=Path,
        help="Output file path (default: tests/export-<target>-<id>.txt)",
    )
    export_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    export_parser.add_argument(
        "--usenorm",
        action="store_true",
        help="If set, export normalized text when available",
    )

    rank_parser = subparsers.add_parser("rank", help="Rank creators vs brands via embeddings")
    rank_mode = rank_parser.add_mutually_exclusive_group(required=True)
    rank_mode.add_argument("--bybrand", action="store_true", help="Rank creators for a brand")
    rank_mode.add_argument("--bycreator", action="store_true", help="Rank brands for a creator")
    rank_parser.add_argument(
        "--adlid",
        type=int,
        required=True,
        help="adlid of the brand (when --bybrand) or creator (when --bycreator)",
    )
    rank_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    rank_parser.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Number of top results to show (default: 20)",
    )
    rank_parser.add_argument(
        "--csv",
        action="store_true",
        help="If set, also write results to CSV (yyyyMMdd-brand{n}.csv or creator{n}.csv)",
    )

    test_compare = subparsers.add_parser("test-compare", help="Compare two text inputs via embeddings")
    test_compare.add_argument("--text1", help="First text input")
    test_compare.add_argument("--text2", help="Second text input")
    test_compare.add_argument("--file1", type=Path, help="Path to first text file")
    test_compare.add_argument("--file2", type=Path, help="Path to second text file")
    test_compare.add_argument("--brand-id", type=int, help="Compare using brand embeddings (adlid)")
    test_compare.add_argument("--creator-id", type=int, help="Compare using creator embeddings (podcastindex_id)")
    test_compare.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    test_enrich = subparsers.add_parser("test-enrich", help="Dump enrichment for a creator")
    test_enrich.add_argument(
        "--pid",
        type=int,
        required=True,
        help="podcastindex_id (from creators) to inspect",
    )
    test_enrich.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    return parser.parse_args(argv)


def handle_fetch_youtube(args: argparse.Namespace) -> int:
    channel_inputs = load_list_inputs(args.channels, args.channels_file)
    if not channel_inputs:
        print("‚ùå Please provide YouTube channels via --channels or --channels-file", file=sys.stderr)
        return 1

    fetcher = YouTubeChannelFetcher(channels=channel_inputs, delay=args.delay)
    print(f"Fetching YouTube channels: {channel_inputs}")
    records = fetcher.fetch()
    print(f"‚úÖ Retrieved {len(records)} YouTube channels.")

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        payload = [record.__dict__ for record in records]
        args.output.write_text(json.dumps(payload, indent=2))
        print(f"üíæ Wrote JSON: {args.output}")

    preview = records[:5]
    if preview:
        print("Sample rows:")
        for rec in preview:
            print(f" ‚Ä¢ {rec.title} ‚Äî {rec.channel_url}")
    return 0


def handle_fetch_spotify(args: argparse.Namespace) -> int:
    show_inputs = load_list_inputs(args.shows, args.shows_file)
    if not show_inputs:
        print("‚ùå Please provide Spotify shows via --shows or --shows-file", file=sys.stderr)
        return 1

    fetcher = SpotifyShowFetcher(shows=show_inputs, delay=args.delay)
    print(f"Fetching Spotify shows: {show_inputs}")
    records = fetcher.fetch()
    print(f"‚úÖ Retrieved {len(records)} Spotify shows.")

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        payload = [record.__dict__ for record in records]
        args.output.write_text(json.dumps(payload, indent=2))
        print(f"üíæ Wrote JSON: {args.output}")

    preview = records[:5]
    if preview:
        print("Sample rows:")
        for rec in preview:
            print(f" ‚Ä¢ {rec.title} ‚Äî {rec.show_url}")
    return 0


def main(argv: Optional[Sequence[str]] = None) -> int:
    if argv is None:
        argv = sys.argv[1:]

    if len(argv) == 1 and argv[0].lower() == "help":
        print_help()
        return 0

    args = parse_args(argv)
    if args.action == "fetch":
        if args.source == "youtube":
            return handle_fetch_youtube(args)
        if args.source == "spotify":
            return handle_fetch_spotify(args)
    if args.action == "ingest":
        if args.source == "podcastindex":
            return ingest_podcastindex_feeds(args)
        if args.source == "apple":
            return ingest_apple_discovery(args)
    if args.action == "enrich":
        return enrich_creators(args)
    if args.action == "enrichment":
        if args.enrichment_action == "clear":
            return clear_enrichment(args)
        if args.enrichment_action == "show":
            return show_enrichment(args)
        if args.enrichment_action == "summary":
            return summary_enrichment(args)
    if args.action == "discover":
        if args.source == "apple":
            return discover_apple(args)
    if args.action == "brand":
        return ingest_brand(args)
    if args.action == "normalize-entity":
        return normalize_entities(args)
    if args.action == "embed-entity":
        return embed_entity(args)
    if args.action == "rank":
        return rank_entities(args)
    if args.action == "test-compare":
        return test_compare(args)
    if args.action == "test-enrich":
        return test_enrich(args)
    if args.action == "export":
        return export_entity(args)
    source = getattr(args, "source", "")
    print(f"Unknown action/source combination: {args.action} {source}")
    return 1


def print_help() -> None:
    help_text = """
Adelined CLI

Commands:
  python3 adl fetch youtube [options]
  python3 adl fetch spotify [options]
  python3 adl ingest podcastindex [options]
  python3 adl discover apple [options]
  python3 adl enrich [options]
  python3 adl enrichment clear [options]
  python3 adl enrichment show [options]
  python3 adl enrichment summary [options]
  python3 adl brand --url <website>
  python3 adl normalize-entity --entity-type brand|creator [--id <id> | --all-brands/--all-creators]
  python3 adl embed-entity --entity-type brand|creator [--id <id> | --limit N --offset M]
  python3 adl rank --bybrand/--bycreator --adlid <id> [options]
  python3 adl test-compare --text1/--file1 --text2/--file2
  python3 adl test-enrich --pid <podcastindex_id>
  python3 adl export --target creator|brand --id <id> [--out path]

Examples:
  # YouTube: fetch channel metadata by handles
  python3 adl fetch youtube --channels @lexfridman,@hubermanlab

  # YouTube: from file (one handle/URL/channel_id per line)
  python3 adl fetch youtube --channels-file channels.txt --output cache/youtube.json

  # Spotify: fetch show metadata by show IDs
  python3 adl fetch spotify --shows 5AvwZVawapvyhJUIx71pdJ,4rOoJ6Egrf8K2IrywzwOMk

  # Spotify: from URLs file
  python3 adl fetch spotify --shows-file spotify_urls.txt --output cache/spotify.json

  # Ingest PodcastIndex snapshot into Adelined DB (itunesId dedupe)
  python3 adl ingest podcastindex --source-db data/podcastindex_feeds.db --target-db adelined_matching.db

  # Discover Apple shows via iTunes Search API (GB)
  python3 adl discover apple --terms "comedy,news" --limit-per-term 200 --max-pages 5 --country GB --adl-db adelined_matching.db

  # Enrich a slice of creators
  python3 adl enrich --adl-db adelined_matching.db --limit 50 --offset 0
  python3 adl enrich --adl-db adelined_matching.db --only-id 12345

  # Normalize and embed (multi-vector)
  python3 adl normalize-entity --entity-type brand --all-brands --limit 50
  python3 adl embed-entity --entity-type brand --limit 50
  # Ingest a brand (scrape + GPT extract)
  python3 adl brand --url https://brandsite.com
  # Embed creators or brands (OpenAI embedding)
  python3 adl embed creators --adl-db adelined_matching.db --limit 100
  python3 adl embed brands --adl-db adelined_matching.db --limit 50
Options:
  --output    Optional JSON file to write scraped records
  --source-db Source SQLite path for ingest (default: data/podcastindex_feeds.db)
  --target-db Target Adelined SQLite path (default: adelined_matching.db)
  --batch-size Batch size for ingest (default: 5000)
  --limit     Max creators to enrich per run (default: 100)
  --offset    Offset into creators table (default: 0)
  --only-id   Enrich only this podcastindex_id
  --limit-per-term Apple discovery results per page (default: 200)
  --max-pages     Apple discovery max pages per term (default: 5)
  --limit     Max rows to embed (default: 100)
  --offset    Offset rows to embed (default: 0)
"""
    print(help_text.strip())


if __name__ == "__main__":
    sys.exit(main())
