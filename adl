#!/usr/bin/env python3
"""
Adelined data CLI.

Usage example:
    python3 adl fetch youtube --channels @lexfridman,@hubermanlab --output cache/youtube.json
"""

import argparse
import json
import os
import re
import sqlite3
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import requests
from bs4 import BeautifulSoup


PODCASTINDEX_COLUMNS: Sequence[str] = [
    "id",
    "url",
    "title",
    "lastUpdate",
    "link",
    "lastHttpStatus",
    "dead",
    "contentType",
    "itunesId",
    "originalUrl",
    "itunesAuthor",
    "itunesOwnerName",
    "explicit",
    "imageUrl",
    "itunesType",
    "generator",
    "newestItemPubdate",
    "language",
    "oldestItemPubdate",
    "episodeCount",
    "popularityScore",
    "priority",
    "createdOn",
    "updateFrequency",
    "chash",
    "host",
    "newestEnclosureUrl",
    "podcastGuid",
    "description",
    "category1",
    "category2",
    "category3",
    "category4",
    "category5",
    "category6",
    "category7",
    "category8",
    "category9",
    "category10",
    "newestEnclosureDuration",
]


@dataclass
class YouTubeChannelRecord:
    channel_url: str
    title: str
    description: str
    about: str
    subscribers: Optional[str]
    scraped_at: str


@dataclass
class SpotifyShowRecord:
    show_url: str
    title: str
    author: str
    description: str
    thumbnail: Optional[str]
    scraped_at: str


class YouTubeChannelFetcher:
    def __init__(
        self,
        channels: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.channels = channels
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})


class YouTubeChannelFetcher:
    def __init__(
        self,
        channels: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.channels = channels
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch(self) -> List[YouTubeChannelRecord]:
        results: List[YouTubeChannelRecord] = []
        for raw in self.channels:
            url = self._normalise_channel_url(raw)
            about_url = url.rstrip("/") + "/about"
            html = self._get(about_url)
            if not html:
                continue
            record = self._parse_about_page(url, html)
            if record:
                results.append(record)
            if self.delay:
                time.sleep(self.delay)
        return results

    def _normalise_channel_url(self, identifier: str) -> str:
        ident = identifier.strip()
        if ident.startswith("http://") or ident.startswith("https://"):
            return ident
        if ident.startswith("@"):
            return f"https://www.youtube.com/{ident}"
        if ident.startswith("UC"):
            return f"https://www.youtube.com/channel/{ident}"
        return f"https://www.youtube.com/@{ident}"

    def _get(self, url: str) -> Optional[str]:
        try:
            res = self.session.get(url, timeout=20)
            res.raise_for_status()
            return res.text
        except Exception as exc:
            print(f"‚ùå Failed to fetch {url}: {exc}", file=sys.stderr)
            return None

    def _parse_about_page(self, channel_url: str, html: str) -> Optional[YouTubeChannelRecord]:
        soup = BeautifulSoup(html, "html.parser")
        title = (soup.find("meta", {"property": "og:title"}) or {}).get("content", "")
        description = (soup.find("meta", {"property": "og:description"}) or {}).get(
            "content", ""
        )

        about_text = ""
        about_el = soup.select_one("#description-container") or soup.select_one(
            "#description"
        )
        if about_el:
            about_text = about_el.get_text(separator=" ", strip=True)

        subscriber_text = None
        sub_el = soup.select_one("#subscriber-count")
        if sub_el:
            subscriber_text = sub_el.get_text(strip=True)

        scraped_at = datetime.utcnow().isoformat()
        if not title and not description and not about_text:
            return None
        return YouTubeChannelRecord(
            channel_url=channel_url,
            title=title,
            description=description,
            about=about_text,
            subscribers=subscriber_text,
            scraped_at=scraped_at,
        )


class SpotifyShowFetcher:
    def __init__(
        self,
        shows: Sequence[str],
        delay: float = 1.0,
        user_agent: str = "Mozilla/5.0 AdelinedBot/0.1",
    ):
        self.shows = shows
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})

    def fetch(self) -> List[SpotifyShowRecord]:
        results: List[SpotifyShowRecord] = []
        for raw in self.shows:
            url = self._normalise_show_url(raw)
            html = self._get(url)
            if not html:
                continue
            oembed = self._get_oembed(url)
            record = self._parse_show_page(url, html, oembed)
            if record:
                results.append(record)
            if self.delay:
                time.sleep(self.delay)
        return results

    def _normalise_show_url(self, identifier: str) -> str:
        ident = identifier.strip()
        if ident.startswith("http://") or ident.startswith("https://"):
            return ident
        return f"https://open.spotify.com/show/{ident}"

    def _get(self, url: str) -> Optional[str]:
        try:
            res = self.session.get(url, timeout=20)
            res.raise_for_status()
            return res.text
        except Exception as exc:
            print(f"‚ùå Failed to fetch {url}: {exc}", file=sys.stderr)
            return None

    def _get_oembed(self, url: str) -> Dict:
        try:
            res = self.session.get(
                "https://open.spotify.com/oembed",
                params={"url": url},
                timeout=15,
            )
            res.raise_for_status()
            return res.json()
        except Exception:
            return {}

    def _parse_show_page(self, show_url: str, html: str, oembed: Dict) -> Optional[SpotifyShowRecord]:
        soup = BeautifulSoup(html, "html.parser")
        title = oembed.get("title") or (soup.find("meta", {"property": "og:title"}) or {}).get(
            "content", ""
        )
        description = (
            (soup.find("meta", {"property": "og:description"}) or {}).get("content", "")
        )
        author = oembed.get("author_name", "")
        thumbnail = oembed.get("thumbnail_url") or (soup.find("meta", {"property": "og:image"}) or {}).get(
            "content"
        )

        scraped_at = datetime.utcnow().isoformat()
        if not title and not description:
            return None
        return SpotifyShowRecord(
            show_url=show_url,
            title=title or "",
            author=author or "",
            description=description or "",
            thumbnail=thumbnail,
            scraped_at=scraped_at,
        )


def ensure_podcastindex_tables(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS podcastindex_feeds (
            id INTEGER PRIMARY KEY,
            url TEXT,
            title TEXT,
            lastUpdate INTEGER,
            link TEXT,
            lastHttpStatus INTEGER,
            dead INTEGER,
            contentType TEXT,
            itunesId INTEGER,
            originalUrl TEXT,
            itunesAuthor TEXT,
            itunesOwnerName TEXT,
            explicit INTEGER,
            imageUrl TEXT,
            itunesType TEXT,
            generator TEXT,
            newestItemPubdate INTEGER,
            language TEXT,
            oldestItemPubdate INTEGER,
            episodeCount INTEGER,
            popularityScore INTEGER,
            priority INTEGER,
            createdOn INTEGER,
            updateFrequency INTEGER,
            chash TEXT,
            host TEXT,
            newestEnclosureUrl TEXT,
            podcastGuid TEXT,
            description TEXT,
            category1 TEXT,
            category2 TEXT,
            category3 TEXT,
            category4 TEXT,
            category5 TEXT,
            category6 TEXT,
            category7 TEXT,
            category8 TEXT,
            category9 TEXT,
            category10 TEXT,
            newestEnclosureDuration INTEGER
        )
        """
    )
    conn.execute("CREATE INDEX IF NOT EXISTS idx_podcastindex_feeds_itunesId ON podcastindex_feeds(itunesId)")


def ensure_creators_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creators (
            adlid INTEGER PRIMARY KEY AUTOINCREMENT,
            podcastindex_id INTEGER,
            podcastguid TEXT,
            itunesid INTEGER,
            language TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )
    conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_creators_pid ON creators(podcastindex_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_creators_itunesid ON creators(itunesid)")


def ensure_creator_enrichment_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS creator_enrichment (
            podcastindex_id INTEGER PRIMARY KEY,
            uk_score REAL,
            source_spotify TEXT,
            source_apple TEXT,
            source_youtube TEXT,
            source_social TEXT,
            cleaned_text_block TEXT,
            raw_text_sources TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def configure_sqlite(conn: sqlite3.Connection) -> None:
    # Reduce lock contention and waits.
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA busy_timeout=5000;")


def ensure_apple_discovery_table(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS apple_discovery (
            trackId INTEGER PRIMARY KEY,
            collectionName TEXT,
            artistName TEXT,
            feedUrl TEXT,
            artworkUrl TEXT,
            primaryGenreName TEXT,
            genres TEXT,
            country TEXT,
            trackCount INTEGER,
            releaseDate TEXT,
            description TEXT,
            raw_json TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
        """
    )


def clear_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    response = input(
        f"This will DELETE all rows from creator_enrichment in {db_path}. Type YES to confirm: "
    ).strip()
    if response != "YES":
        print("Aborted; no changes made.")
        return 0
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        conn.execute("DELETE FROM creator_enrichment")
        conn.commit()
        print("‚úÖ Cleared creator_enrichment.")
        return 0
    finally:
        conn.close()


def show_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        def load_json(val):
            try:
                return json.loads(val) if val else {}
            except Exception:
                return {}

        def count_nonempty(d: Dict) -> int:
            if not isinstance(d, dict):
                return 0
            return sum(1 for v in d.values() if v not in (None, "", [], {}))

        if not args.source:
            cur = conn.execute(
                """
                SELECT ce.podcastindex_id, ce.source_spotify, ce.source_apple, ce.source_youtube, ce.source_social,
                       pf.title as name
                FROM creator_enrichment ce
                LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
                LIMIT ?
                """,
                (args.limit,),
            )
            rows = cur.fetchall()
            if not rows:
                print("No enrichment rows found.")
                return 0
            for row in rows:
                spotify_j = load_json(row["source_spotify"])
                apple_j = load_json(row["source_apple"])
                youtube_j = load_json(row["source_youtube"])
                social_j = load_json(row["source_social"])
                print(f"name: {row['name'] or 'N/A'}")
                print(f"spotify: {count_nonempty(spotify_j)} data")
                print(f"apple: {count_nonempty(apple_j)} data")
                print(f"youtube: {count_nonempty(youtube_j)} data")
                print(f"social: {count_nonempty(social_j)} data")
                print("")
            return 0

        col = {
            "spotify": "source_spotify",
            "apple": "source_apple",
            "youtube": "source_youtube",
            "social": "source_social",
        }[args.source]
        cur = conn.execute(
            f"""
            SELECT ce.podcastindex_id, ce.{col}, pf.title as name
            FROM creator_enrichment ce
            LEFT JOIN podcastindex_feeds pf ON pf.id = ce.podcastindex_id
            WHERE ce.{col} IS NOT NULL
            LIMIT ?
            """,
            (args.limit,),
        )
        rows = cur.fetchall()
        if not rows:
            print(f"No rows with {args.source} data.")
            return 0
        for row in rows:
            data = load_json(row[col])
            if not isinstance(data, dict):
                print(f"{row['name'] or row['podcastindex_id']}: (invalid JSON)")
                continue
            print(f"{row['name'] or row['podcastindex_id']}:")
            for k, v in data.items():
                if v in (None, "", [], {}):
                    continue
                print(f"  {k}: {v}")
            print("")
        return 0
    finally:
        conn.close()


def summary_enrichment(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        total_creators = conn.execute("SELECT COUNT(*) FROM creators").fetchone()[0]
        total_enriched = conn.execute("SELECT COUNT(*) FROM creator_enrichment").fetchone()[0]

        def has_payload(val) -> bool:
            if isinstance(val, dict):
                if not val:
                    return False
                status = val.get("status")
                if status in ("error", "not_found", "missing_credentials"):
                    return False
                for v in val.values():
                    if has_payload(v):
                        return True
                return False
            if isinstance(val, list):
                return any(has_payload(v) for v in val)
            return val not in (None, "", [], {})

        def count_valid(col: str) -> int:
            rows = conn.execute(
                f"SELECT {col} FROM creator_enrichment WHERE {col} IS NOT NULL"
            ).fetchall()
            count = 0
            for r in rows:
                try:
                    d = json.loads(r[col]) if r[col] else {}
                except Exception:
                    d = {}
                if has_payload(d):
                    count += 1
            return count

        spotify_ok = count_valid("source_spotify")
        apple_ok = count_valid("source_apple")
        youtube_ok = count_valid("source_youtube")
        social_ok = count_valid("source_social")

        print(f"Total creators: {total_creators}")
        print(f"Enriched: {total_enriched} / {total_creators}")
        print(f"    spotify: {spotify_ok} / {total_enriched}")
        print(f"    apple: {apple_ok} / {total_enriched}")
        print(f"    youtube: {youtube_ok} / {total_enriched}")
        print(f"    socials: {social_ok} / {total_enriched}")
        return 0
    finally:
        conn.close()


def ingest_podcastindex_feeds(args: argparse.Namespace) -> int:
    source_db = Path(args.source_db)
    target_db = Path(args.target_db)

    if not source_db.exists():
        print(f"‚ùå Source DB not found: {source_db}", file=sys.stderr)
        return 1

    batch_size = args.batch_size
    print(f"üîÑ Ingesting from {source_db} -> {target_db} (batch size {batch_size})")

    src = sqlite3.connect(source_db, timeout=10)
    src.row_factory = sqlite3.Row
    tgt = sqlite3.connect(target_db, timeout=10)
    tgt.row_factory = sqlite3.Row
    configure_sqlite(src)
    configure_sqlite(tgt)

    try:
        ensure_podcastindex_tables(tgt)
        ensure_creators_table(tgt)
        ensure_creator_enrichment_table(tgt)

        existing_podcastindex_ids = {
            row[0] for row in tgt.execute("SELECT id FROM podcastindex_feeds")
        }
        existing_creators_by_pid = {
            row[0] for row in tgt.execute("SELECT podcastindex_id FROM creators")
        }

        print(
            f"Existing podcastindex ids: {len(existing_podcastindex_ids)} | "
            f"creators by podcastindex_id: {len(existing_creators_by_pid)}"
        )

        src_cursor = src.execute("SELECT * FROM podcasts")

        placeholders = ",".join("?" for _ in PODCASTINDEX_COLUMNS)
        feed_sql = f"""
            INSERT OR IGNORE INTO podcastindex_feeds ({','.join(PODCASTINDEX_COLUMNS)})
            VALUES ({placeholders})
        """
        creator_sql = """
            INSERT OR IGNORE INTO creators (
                podcastindex_id, podcastguid, itunesid, language
            ) VALUES (?, ?, ?, ?)
        """

        total_processed = 0
        inserted_feeds = 0
        inserted_creators = 0
        skipped_existing = 0
        skipped_non_en = 0

        while True:
            rows = src_cursor.fetchmany(batch_size)
            if not rows:
                break

            feed_rows = []
            creator_rows = []

            for row in rows:
                podcastindex_id = row["id"]
                podcastguid = row["podcastGuid"]
                itunes_id = row["itunesId"]
                lang = (row["language"] or "").lower()
                if lang and "en" not in lang:
                    skipped_non_en += 1
                    continue

                # Conflict check: if this podcastindex_id already exists with a different itunesId/podcastGuid, stop
                if podcastindex_id in existing_podcastindex_ids:
                    existing_row = tgt.execute(
                        "SELECT itunesId, podcastGuid FROM podcastindex_feeds WHERE id = ?",
                        (podcastindex_id,),
                    ).fetchone()
                    if existing_row:
                        existing_itunes_val = existing_row[0]
                        existing_guid_val = existing_row[1]
                        if (
                            existing_itunes_val
                            and itunes_id
                            and existing_itunes_val != itunes_id
                        ) or (
                            existing_guid_val
                            and podcastguid
                            and existing_guid_val != podcastguid
                        ):
                            raise RuntimeError(
                                f"Conflicting data for podcastindex_id {podcastindex_id}: "
                                f"existing itunes {existing_itunes_val} vs {itunes_id}, "
                                f"guid {existing_guid_val} vs {podcastguid}"
                            )
                    skipped_existing += 1
                    continue

                feed_rows.append(tuple(row[col] for col in PODCASTINDEX_COLUMNS))
                existing_podcastindex_ids.add(podcastindex_id)
                inserted_feeds += 1

                if podcastindex_id not in existing_creators_by_pid:
                    creator_rows.append(
                        (
                            podcastindex_id,
                            podcastguid,
                            itunes_id,
                            row["language"],
                        )
                    )
                    existing_creators_by_pid.add(podcastindex_id)
                    inserted_creators += 1

            if feed_rows:
                tgt.executemany(feed_sql, feed_rows)
            if creator_rows:
                tgt.executemany(creator_sql, creator_rows)
            tgt.commit()

            total_processed += len(rows)
            print(
                f"Processed {total_processed:,} rows | inserted feeds: {inserted_feeds:,} | "
                f"inserted creators: {inserted_creators:,} | skipped existing: {skipped_existing:,}"
            )

        print(
            f"‚úÖ Done. Inserted feeds: {inserted_feeds:,}; creators: {inserted_creators:,}; "
            f"skipped existing podcastindex_id: {skipped_existing:,}; skipped non-en: {skipped_non_en:,}"
        )
        return 0
    finally:
        src.close()
        tgt.close()


def parse_comma_list(raw: Optional[str], *, default: Sequence[str]) -> List[str]:
    if not raw:
        return list(default)
    if raw.lower() == "all":
        return list(default)
    return [chunk.strip() for chunk in raw.split(",") if chunk.strip()]


def load_list_inputs(raw: Optional[str], path: Optional[Path]) -> List[str]:
    items: List[str] = []
    if raw:
        items.extend(parse_comma_list(raw, default=[]))
    if path and path.exists():
        items.extend([line.strip() for line in path.read_text().splitlines() if line.strip()])
    # Deduplicate while preserving order
    seen = set()
    deduped = []
    for item in items:
        if item not in seen:
            seen.add(item)
            deduped.append(item)
    return deduped


def discover_apple(args: argparse.Namespace) -> int:
    terms = load_list_inputs(args.terms, args.terms_file)
    if not terms:
        print("‚ùå No search terms provided.", file=sys.stderr)
        return 1

    db_path = Path(args.adl_db)
    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    ensure_apple_discovery_table(conn)

    seen_ids = {
        row[0] for row in conn.execute("SELECT trackId FROM apple_discovery")
    }

    inserted = 0
    total_terms = len(terms)
    try:
        for idx, term in enumerate(terms, start=1):
            print(f"üîé Term {idx}/{total_terms}: '{term}'")
            offset = 0
            pages = 0
            while pages < args.max_pages:
                backoff = 5
                params = {
                    "term": term,
                    "country": args.country,
                    "media": "podcast",
                    "limit": args.limit_per_term,
                    "offset": offset,
                }
                while True:
                    res = requests.get("https://itunes.apple.com/search", params=params, timeout=15)
                    if res.status_code == 429:
                        print(f"  429 rate limit hit; sleeping {backoff}s...")
                        time.sleep(backoff)
                        backoff = min(backoff * 2, 60)
                        continue
                    res.raise_for_status()
                    break
                payload = res.json()
                results = payload.get("results", [])
                if not results:
                    print(f"  No more results at page {pages}.")
                    break
                rows = []
                for r in results:
                    tid = r.get("trackId")
                    if not tid or tid in seen_ids:
                        continue
                    seen_ids.add(tid)
                    rows.append(
                        (
                            tid,
                            r.get("collectionName"),
                            r.get("artistName"),
                            r.get("feedUrl"),
                            r.get("artworkUrl100"),
                            r.get("primaryGenreName"),
                            json.dumps(r.get("genres")),
                            r.get("country"),
                            r.get("trackCount"),
                            r.get("releaseDate"),
                            r.get("description") or r.get("longDescription"),
                            json.dumps(r),
                        )
                    )
                if rows:
                    conn.executemany(
                        """
                        INSERT INTO apple_discovery (
                            trackId, collectionName, artistName, feedUrl, artworkUrl,
                            primaryGenreName, genres, country, trackCount, releaseDate,
                            description, raw_json
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        rows,
                    )
                    conn.commit()
                    inserted += len(rows)
                    print(f"  Page {pages+1}: inserted {len(rows)} new (total {inserted})")
                offset += args.limit_per_term
                pages += 1
                time.sleep(0.5)
        print(f"‚úÖ Apple discovery inserted {inserted} new rows into apple_discovery.")
        return 0
    finally:
        conn.close()


def ingest_apple_discovery(args: argparse.Namespace) -> int:
    source_db = Path(args.source_db)
    target_db = Path(args.target_db)

    if not target_db.exists():
        print(f"‚ùå Target DB not found: {target_db}", file=sys.stderr)
        return 1

    tgt = sqlite3.connect(target_db, timeout=10)
    tgt.row_factory = sqlite3.Row
    configure_sqlite(tgt)
    try:
        ensure_creators_table(tgt)
        ensure_apple_discovery_table(tgt)

        existing_itunes = {
            row[0] for row in tgt.execute("SELECT itunesid FROM creators WHERE itunesid IS NOT NULL")
        }

        rows = tgt.execute(
            "SELECT trackId, collectionName, country FROM apple_discovery"
        ).fetchall()
        to_insert = []
        for r in rows:
            tid = r["trackId"]
            if tid in existing_itunes:
                continue
            # Use negative trackId to avoid clashing with PodcastIndex IDs
            pid = -int(tid)
            language = "en-gb" if (r["country"] or "").lower() == "gb" else "en"
            to_insert.append((pid, None, tid, language))

        if not to_insert:
            print("No new Apple discovery rows to merge.")
            return 0

        tgt.executemany(
            """
            INSERT OR IGNORE INTO creators (podcastindex_id, podcastguid, itunesid, language)
            VALUES (?, ?, ?, ?)
            """,
            to_insert,
        )
        tgt.commit()
        print(f"‚úÖ Merged {len(to_insert)} Apple discovery rows into creators (pid = -trackId).")
        return 0
    finally:
        tgt.close()


def fetch_apple_metadata(itunes_id: Optional[int], title: str) -> Dict:
    if not title and not itunes_id:
        return {}
    try:
        params = {"entity": "podcast"}
        if itunes_id:
            params["id"] = itunes_id
            url = "https://itunes.apple.com/lookup"
        else:
            params["term"] = title
            params["limit"] = 3
            url = "https://itunes.apple.com/search"
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        results = data.get("results", [])
        if not results:
            return {}
        best = results[0]
        return {
            "title": best.get("collectionName") or best.get("trackName"),
            "description": best.get("description") or best.get("collectionExplicitness"),
            "genres": best.get("genres"),
            "publisher": best.get("artistName"),
        }
    except Exception as exc:
        print(f"‚ö†Ô∏è Apple lookup failed: {exc}", file=sys.stderr)
        return {}


def fetch_spotify_metadata(show_url: Optional[str], title: str) -> Dict:
    client_id = os.getenv("SPOTIFY_CLIENT_ID")
    client_secret = os.getenv("SPOTIFY_CLIENT_SECRET")
    if not client_id or not client_secret:
        return {"status": "missing_credentials"}

    try:
        token_res = requests.post(
            "https://accounts.spotify.com/api/token",
            data={"grant_type": "client_credentials"},
            auth=(client_id, client_secret),
            timeout=15,
        )
        token_res.raise_for_status()
        access_token = token_res.json().get("access_token")
        headers = {"Authorization": f"Bearer {access_token}"}

        show_id = None
        if show_url and "open.spotify.com/show/" in show_url:
            show_id = show_url.rstrip("/").split("/")[-1]
        else:
            if not title:
                return {"status": "not_found"}
            search_res = requests.get(
                "https://api.spotify.com/v1/search",
                headers=headers,
                params={"q": title, "type": "show", "market": "GB", "limit": 3},
                timeout=15,
            )
            search_res.raise_for_status()
            items = search_res.json().get("shows", {}).get("items", [])
            if items:
                show_id = items[0]["id"]

        if not show_id:
            return {"status": "not_found"}

        show_res = requests.get(
            f"https://api.spotify.com/v1/shows/{show_id}",
            headers=headers,
            params={"market": "GB"},
            timeout=15,
        )
        show_res.raise_for_status()
        show_data = show_res.json()

        ep_res = requests.get(
            f"https://api.spotify.com/v1/shows/{show_id}/episodes",
            headers=headers,
            params={"market": "GB", "limit": 10},
            timeout=15,
        )
        ep_res.raise_for_status()
        eps = ep_res.json().get("items", []) or []
        ep_samples = [
            {"name": ep.get("name"), "description": ep.get("description")}
            for ep in eps[:10]
        ]

        return {
            "show": {
                "id": show_id,
                "name": show_data.get("name"),
                "description": show_data.get("description"),
                "publisher": show_data.get("publisher"),
                "languages": show_data.get("languages"),
                "total_episodes": show_data.get("total_episodes"),
            },
            "episodes_sample": ep_samples,
            "status": "ok",
        }
    except requests.HTTPError as http_err:
        status_code = http_err.response.status_code if http_err.response else None
        detail = ""
        try:
            detail = http_err.response.json()
        except Exception:
            detail = http_err.response.text if hasattr(http_err, "response") else ""
        print(f"‚ö†Ô∏è Spotify fetch failed: {http_err} detail={detail}", file=sys.stderr)
        return {"status": "error", "error": str(http_err), "detail": detail, "http_status": status_code}
    except Exception as exc:
        print(f"‚ö†Ô∏è Spotify fetch failed: {exc}", file=sys.stderr)
        return {"status": "error", "error": str(exc)}


def fetch_youtube_metadata(title: str, *, mode: str = "lean") -> Dict:
    api_key = os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        return {"status": "missing_credentials"}

    try:
        # Search channel
        search_res = requests.get(
            "https://www.googleapis.com/youtube/v3/search",
            params={
                "part": "snippet",
                "q": title,
                "type": "channel",
                "regionCode": "GB",
                "maxResults": 8,
                "key": api_key,
            },
            timeout=15,
        )
        search_res.raise_for_status()
        items = search_res.json().get("items", [])
        picked_channel = None

        for ch in items:
            channel_id = ch["snippet"]["channelId"]
            # Fetch channel details to get full description
            chan_res = requests.get(
                "https://www.googleapis.com/youtube/v3/channels",
                params={
                    "part": "snippet",
                    "id": channel_id,
                    "key": api_key,
                },
                timeout=15,
            )
            if chan_res.status_code != 200:
                continue
            chan_items = chan_res.json().get("items", [])
            if not chan_items:
                continue
            chan_snip = chan_items[0].get("snippet", {}) or {}
            channel_title = chan_snip.get("title", "")
            channel_description = chan_snip.get("description", "")

            lower_title = channel_title.lower()
            if "topic" in lower_title or lower_title.endswith(" - topic"):
                # Reject auto-generated topic channels
                continue
            if not channel_description or len(channel_description.strip()) < 20:
                # Reject channels with no meaningful about text
                continue

            picked_channel = {
                "id": channel_id,
                "title": channel_title,
                "about": channel_description,
            }
            break

        if not picked_channel:
            return {"status": "not_found"}

        channel_id = picked_channel["id"]
        channel_title = picked_channel["title"]
        channel_description = picked_channel["about"]

        # Fetch top videos by viewCount
        video_limit = 1 if mode == "lean" else 3
        videos_res = requests.get(
            "https://www.googleapis.com/youtube/v3/search",
            params={
                "part": "snippet",
                "channelId": channel_id,
                "order": "viewCount",
                "maxResults": video_limit,
                "type": "video",
                "key": api_key,
            },
            timeout=15,
        )
        videos_res.raise_for_status()
        video_items = videos_res.json().get("items", []) or []
        videos_sample = []
        comments_sample: List[str] = []

        for vid in video_items:
            vid_id = vid["id"]["videoId"]
            vid_title = vid["snippet"]["title"]
            vid_desc = vid["snippet"].get("description", "")
            videos_sample.append({"id": vid_id, "title": vid_title, "description": vid_desc})

            # Comments
            if mode != "lean":
                comments_res = requests.get(
                    "https://www.googleapis.com/youtube/v3/commentThreads",
                    params={
                        "part": "snippet",
                        "videoId": vid_id,
                        "maxResults": 50,
                        "order": "relevance",
                        "textFormat": "plainText",
                        "key": api_key,
                    },
                    timeout=15,
                )
                if comments_res.status_code == 200:
                    threads = comments_res.json().get("items", []) or []
                    for th in threads:
                        top = th.get("snippet", {}).get("topLevelComment", {}).get("snippet", {})
                        txt = top.get("textDisplay")
                        if txt:
                            comments_sample.append(txt)
                        if len(comments_sample) >= 50:
                            break
                if len(comments_sample) >= 50:
                    break

        return {
            "channel": picked_channel,
            "videos_sample": videos_sample,
            "comments_sample": comments_sample[:50],
            "status": "ok",
        }
    except requests.HTTPError as http_err:
        detail = ""
        status_code = http_err.response.status_code if http_err.response else None
        try:
            detail = http_err.response.json()
        except Exception:
            detail = http_err.response.text if hasattr(http_err, "response") else ""
        print(f"‚ö†Ô∏è YouTube fetch failed: {http_err} detail={detail}", file=sys.stderr)
        return {"status": "error", "error": str(http_err), "detail": detail, "http_status": status_code}
    except Exception as exc:
        print(f"‚ö†Ô∏è YouTube fetch failed: {exc}", file=sys.stderr)
        return {"status": "error", "error": str(exc)}


def fetch_social_bios(title: str) -> Dict:
    return {}


def build_text_block(
    base: Dict,
    apple: Dict,
    spotify: Dict,
    youtube: Dict,
    social: Dict,
) -> Tuple[str, Dict]:
    # Compose raw sources
    raw_sources = {
        "podcastindex_description": base.get("pi_description"),
        "spotify_description": (spotify.get("show", {}) if spotify else {}).get("description"),
        "apple_description": apple.get("description"),
        "youtube_about": (youtube.get("channel", {}) if youtube else {}).get("about"),
        "youtube_descriptions": [v.get("description") for v in (youtube.get("videos_sample") or [])] if youtube else [],
        "youtube_comments": youtube.get("comments_sample") if youtube else [],
        "social_bios": {
            "twitter": (social.get("twitter") or {}) if social else {},
            "instagram": (social.get("instagram") or {}) if social else {},
            "website_about": (social.get("website") or {}).get("about") if social else None,
        },
    }

    categories = []
    for key in ("category1", "category2", "category3", "category4", "category5", "category6", "category7", "category8", "category9", "category10"):
        val = base.get(key)
        if val:
            categories.append(val)
    categories_text = ", ".join(categories)

    episode_summary = ""
    if spotify.get("episodes_sample"):
        titles = [ep.get("name") or "" for ep in spotify["episodes_sample"] if ep]
        episode_summary = "; ".join(titles[:20])

    text_block = f"""TITLE:
{apple.get('title') or spotify.get('title') or base.get('pi_title') or ''}

DESCRIPTION:
{apple.get('description') or spotify.get('description') or base.get('pi_description') or ''}

CATEGORIES:
{categories_text}

EPISODE_SUMMARY:
{episode_summary}

YOUTUBE_ABOUT:
{youtube.get('about') or ''}

YOUTUBE_COMMENTS:
{youtube.get('comments') or ''}

HOST_BIO:
{social.get('bio') or ''}
"""
    cleaned = clean_text_block(text_block)
    # Cap length to avoid runaway blobs
    if len(cleaned) > 4000:
        cleaned = cleaned[:4000]
    return cleaned, raw_sources


def enrich_creators(args: argparse.Namespace) -> int:
    db_path = Path(args.adl_db)
    if not db_path.exists():
        print(f"‚ùå DB not found: {db_path}", file=sys.stderr)
        return 1

    have_spotify = bool(os.getenv("SPOTIFY_CLIENT_ID") and os.getenv("SPOTIFY_CLIENT_SECRET"))
    have_youtube = bool(os.getenv("YOUTUBE_API_KEY"))
    if not have_spotify:
        print("‚ùå SPOTIFY_CLIENT_ID/SECRET not set; enrichment requires Spotify API.", file=sys.stderr)
        return 1
    if not have_youtube:
        print("‚ùå YOUTUBE_API_KEY not set; enrichment requires YouTube API.", file=sys.stderr)
        return 1

    conn = sqlite3.connect(db_path, timeout=10)
    conn.row_factory = sqlite3.Row
    configure_sqlite(conn)
    try:
        ensure_podcastindex_tables(conn)
        ensure_creators_table(conn)
        ensure_creator_enrichment_table(conn)

        requested_sources = None
        if args.sources:
            requested_sources = {s.strip().lower() for s in args.sources.split(",") if s.strip()}
            print(f"üîé Enriching only requested sources: {', '.join(sorted(requested_sources))}")
            print("   Will prioritize rows with missing/error/not_found for those sources.")
        disabled_sources = set()

        cursor = conn.cursor()
        if args.only_id:
            cursor.execute(
                """
                SELECT c.podcastindex_id, c.itunesid, c.language,
                       p.url as feed_url, p.link as web_link, p.title as pi_title, p.description as pi_description,
                       p.category1, p.category2, p.category3, p.category4, p.category5,
                       p.category6, p.category7, p.category8, p.category9, p.category10,
                       p.podcastGuid,
                       ce.source_spotify, ce.source_apple, ce.source_youtube, ce.source_social
                FROM creators c
                LEFT JOIN podcastindex_feeds p ON c.podcastindex_id = p.id
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                WHERE c.podcastindex_id = ?
                """,
                (args.only_id,),
            )
        else:
            cursor.execute(
                """
                SELECT c.podcastindex_id, c.itunesid, c.language,
                       p.url as feed_url, p.link as web_link, p.title as pi_title, p.description as pi_description,
                       p.category1, p.category2, p.category3, p.category4, p.category5,
                       p.category6, p.category7, p.category8, p.category9, p.category10,
                       p.podcastGuid,
                       ce.source_spotify, ce.source_apple, ce.source_youtube, ce.source_social
                FROM creators c
                LEFT JOIN podcastindex_feeds p ON c.podcastindex_id = p.id
                LEFT JOIN creator_enrichment ce ON ce.podcastindex_id = c.podcastindex_id
                LIMIT ? OFFSET ?
                """,
                (args.limit * 5, args.offset),
            )

        raw_rows = cursor.fetchall()
        if not raw_rows:
            print("No creators to enrich.")
            return 0

        def load_json(val):
            try:
                return json.loads(val) if val else {}
            except Exception:
                return {}

        def needs_source_refresh(source_name: str, existing: Dict) -> bool:
            if requested_sources and source_name not in requested_sources:
                return False
            status = existing.get("status")
            if not existing or status in (None, "", "not_found", "error", "missing_credentials"):
                return True
            return False

        rows = []
        for r in raw_rows:
            existing_spotify = load_json(r["source_spotify"])
            existing_apple = load_json(r["source_apple"])
            existing_youtube = load_json(r["source_youtube"])
            existing_social = load_json(r["source_social"])
            ce_missing = r["source_spotify"] is None and r["source_apple"] is None and r["source_youtube"] is None and r["source_social"] is None
            if not requested_sources:
                # When no sources specified, refresh rows with any missing/error/empty source OR entirely missing enrichment
                refresh_needed = ce_missing or any(
                    needs_source_refresh(src_name, existing)
                    for src_name, existing in [
                        ("spotify", existing_spotify),
                        ("apple", existing_apple),
                        ("youtube", existing_youtube),
                        ("social", existing_social),
                    ]
                )
                if refresh_needed:
                    rows.append(r)
            else:
                refresh_needed = False
                for src_name, existing in [
                    ("spotify", existing_spotify),
                    ("apple", existing_apple),
                    ("youtube", existing_youtube),
                    ("social", existing_social),
                ]:
                    if src_name in requested_sources and needs_source_refresh(src_name, existing):
                        refresh_needed = True
                        break
                if refresh_needed or ce_missing:
                    rows.append(r)

        if not rows:
            print("No creators to enrich for the requested sources.")
            return 0
        print(f"üîÅ Will process {len(rows)} creators (including missing/error/not_found for requested sources).")

        enriched_rows = 0
        stats = {
            "spotify_ok": 0,
            "spotify_missing": 0,
            "spotify_not_found": 0,
            "youtube_ok": 0,
            "youtube_missing": 0,
            "youtube_not_found": 0,
            "apple_ok": 0,
            "apple_not_found": 0,
        }
        for idx, row in enumerate(rows, start=1):
            podcastindex_id = row["podcastindex_id"]
            itunes_id = row["itunesid"]
            language = (row["language"] or "").lower()
            if language and "en" not in language:
                continue
            base = dict(row)
            # Use feed URL to guess spotify URL if it matches open.spotify.com/show/...
            spotify_url = base.get("feed_url") if base.get("feed_url", "").startswith("https://open.spotify.com/show/") else None

            title_for_lookup = base.get("pi_title") or ""

            existing_spotify = load_json(row["source_spotify"])
            existing_apple = load_json(row["source_apple"])
            existing_youtube = load_json(row["source_youtube"])
            existing_social = load_json(row["source_social"])

            fetch_spotify_flag = ((not requested_sources) or ("spotify" in requested_sources)) and ("spotify" not in disabled_sources)
            fetch_apple_flag = ((not requested_sources) or ("apple" in requested_sources)) and ("apple" not in disabled_sources)
            fetch_youtube_flag = ((not requested_sources) or ("youtube" in requested_sources)) and ("youtube" not in disabled_sources)
            fetch_social_flag = ((not requested_sources) or ("social" in requested_sources)) and ("social" not in disabled_sources)

            spotify_fetched = fetch_spotify_flag
            youtube_fetched = fetch_youtube_flag
            spotify_meta = existing_spotify if not fetch_spotify_flag else fetch_spotify_metadata(spotify_url, title_for_lookup)
            apple_meta = existing_apple if not fetch_apple_flag else fetch_apple_metadata(itunes_id, title_for_lookup)
            youtube_meta = existing_youtube if not fetch_youtube_flag else fetch_youtube_metadata(title_for_lookup, mode="lean")
            social_meta = existing_social if not fetch_social_flag else fetch_social_bios_from_sources(base, apple_meta, spotify_meta, youtube_meta)

            # Stop on 403 errors for any source
            def is_403(meta: Dict) -> bool:
                if not isinstance(meta, dict):
                    return False
                if meta.get("http_status") == 403:
                    return True
                detail = meta.get("detail")
                if isinstance(detail, str) and "quotaExceeded" in detail:
                    return True
                if isinstance(detail, dict) and meta.get("detail", {}).get("error", {}).get("errors"):
                    # Inspect nested error reasons for quotaExceeded
                    for err in meta["detail"]["error"]["errors"]:
                        if err.get("reason") == "quotaExceeded":
                            return True
                return False

            if spotify_fetched and is_403(spotify_meta):
                if requested_sources and "spotify" in requested_sources:
                    print(f"STOP: Spotify returned 403 for podcastindex_id={podcastindex_id}. Detail: {spotify_meta.get('detail')}")
                    print("\033[31mEnrichment halted due to Spotify 403 (likely quota or key restriction).\033[0m")
                    conn.rollback()
                    return 1
                else:
                    print(f"‚ö†Ô∏è Spotify 403 encountered for podcastindex_id={podcastindex_id}. Skipping further Spotify calls this run.")
                    print("\033[31mSkipping Spotify due to 403 (likely quota or key restriction).\033[0m")
                    disabled_sources.add("spotify")
                    spotify_meta = {"status": "error", "http_status": 403, "detail": spotify_meta.get("detail")}
            if youtube_fetched and is_403(youtube_meta):
                if requested_sources and "youtube" in requested_sources:
                    print(f"STOP: YouTube returned 403 for podcastindex_id={podcastindex_id}. Detail: {youtube_meta.get('detail')}")
                    print("\033[31mEnrichment halted due to YouTube 403 (likely quota or API key restriction).\033[0m")
                    conn.rollback()
                    return 1
                else:
                    print(f"‚ö†Ô∏è YouTube 403 encountered for podcastindex_id={podcastindex_id}. Skipping further YouTube calls this run.")
                    print("\033[31mSkipping YouTube due to 403 (likely quota or API key restriction).\033[0m")
                    disabled_sources.add("youtube")
                    youtube_meta = {"status": "error", "http_status": 403, "detail": youtube_meta.get("detail")}

            # Stats
            apple_status = apple_meta.get("status") or ("ok" if apple_meta else "not_found")
            spotify_status = spotify_meta.get("status") or ("ok" if spotify_meta else "not_found")
            youtube_status = youtube_meta.get("status") or ("ok" if youtube_meta else "not_found")
            social_status = social_meta.get("status") if isinstance(social_meta, dict) else None
            if apple_status == "ok":
                stats["apple_ok"] += 1
            else:
                stats["apple_not_found"] += 1
            if spotify_status == "ok":
                stats["spotify_ok"] += 1
            elif spotify_status == "missing_credentials":
                stats["spotify_missing"] += 1
            else:
                stats["spotify_not_found"] += 1
            if youtube_status == "ok":
                stats["youtube_ok"] += 1
            elif youtube_status == "missing_credentials":
                stats["youtube_missing"] += 1
            else:
                stats["youtube_not_found"] += 1

            cleaned_text, raw_sources = build_text_block(
                base=base,
                apple=apple_meta,
                spotify=spotify_meta,
                youtube=youtube_meta,
                social=social_meta,
            )

            uk_score = compute_uk_score(base, apple_meta, spotify_meta, youtube_meta, social_meta)

            conn.execute(
                """
                INSERT OR REPLACE INTO creator_enrichment (
                    podcastindex_id, uk_score, source_spotify, source_apple, source_youtube, source_social,
                    cleaned_text_block, raw_text_sources, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """,
                (
                    podcastindex_id,
                    uk_score,
                    json.dumps(spotify_meta),
                    json.dumps(apple_meta),
                    json.dumps(youtube_meta),
                    json.dumps(social_meta),
                    cleaned_text,
                    json.dumps(raw_sources),
                ),
            )
            enriched_rows += 1
            if enriched_rows % 10 == 0:
                conn.commit()
                print(
                    f"Enriched {enriched_rows}/{len(rows)} (last podcastindex_id={podcastindex_id})"
                )

        conn.commit()
        print(
            f"‚úÖ Enriched {enriched_rows} creators. "
            f"Apple ok: {stats['apple_ok']}, not_found/missing: {stats['apple_not_found']}; "
            f"Spotify ok: {stats['spotify_ok']}, missing creds: {stats['spotify_missing']}, other: {stats['spotify_not_found']}; "
            f"YouTube ok: {stats['youtube_ok']}, missing creds: {stats['youtube_missing']}, other: {stats['youtube_not_found']}"
        )
        return 0
    finally:
        conn.close()


def clean_text_block(text: str) -> str:
    """
    Apply basic cleaning rules:
    - strip URLs
    - remove hashtags/handles
    - remove boilerplate like 'like and subscribe'
    - remove timestamps like [00:00], 00:00:
    - collapse repeated whitespace
    """
    if not text:
        return ""
    # Remove URLs
    text = re.sub(r"https?://\\S+|www\\.\\S+", " ", text)
    # Remove hashtags/handles
    text = re.sub(r"[@#][A-Za-z0-9_]+", " ", text)
    # Remove timestamps like [00:00] or 00:00:
    text = re.sub(r"\\[?\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\]?\\:?", " ", text)
    # Remove common boilerplate phrases
    boilerplate_patterns = [
        r"like and subscribe",
        r"don‚Äôt forget to subscribe",
        r"don't forget to subscribe",
        r"check out our sponsor",
        r"leave a comment",
        r"support us on patreon",
    ]
    for pat in boilerplate_patterns:
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)
    # Collapse repeated text: naive approach by splitting sentences and deduping consecutive repeats
    parts = text.split(".")
    deduped_parts = []
    prev = None
    for part in parts:
        chunk = part.strip()
        if not chunk:
            continue
        if chunk == prev:
            continue
        deduped_parts.append(chunk)
        prev = chunk
    text = ". ".join(deduped_parts)
    # Normalize whitespace
    text = re.sub(r"\\s+", " ", text).strip()
    return text


UK_KEYWORDS = [
    "united kingdom",
    "uk",
    "british",
    "england",
    "scotland",
    "wales",
    "northern ireland",
    "london",
    "manchester",
    "birmingham",
    "leeds",
    "glasgow",
    "edinburgh",
    "bristol",
    "liverpool",
    "cardiff",
    "belfast",
]


def compute_uk_score(base: Dict, apple: Dict, spotify: Dict, youtube: Dict, social: Dict) -> float:
    score = 0.0
    haystack = " ".join(
        [
            str(base.get("pi_description") or ""),
            str(base.get("pi_title") or ""),
            str(apple.get("description") or ""),
            str(spotify.get("description") or ""),
            str(youtube.get("channel", {}).get("about", "") if youtube else ""),
            str(social.get("website", {}).get("about", "") if social else ""),
        ]
    ).lower()

    # PodcastIndex location not present in schema, so skip; use text heuristics.
    if any(word in haystack for word in ("uk podcast", "british", "london", "uk-based", "britain")):
        score += 0.4

    # Cities / keywords
    for kw in UK_KEYWORDS:
        if kw in haystack:
            score += 0.1
            break

    # Domain heuristic
    feed_url = base.get("feed_url") or ""
    if ".co.uk" in feed_url or feed_url.endswith(".uk"):
        score += 0.2

    # Cap 0..1
    score = max(0.0, min(1.0, score))
    return score


def clean_bio_text(text: str, max_len: int = 500) -> Optional[str]:
    if not text:
        return None
    # Remove URLs, hashtags, handles
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)
    text = re.sub(r"[@#][A-Za-z0-9_]+", " ", text)
    # Remove boilerplate phrases
    boiler = [
        "follow me",
        "link in bio",
        "business enquiries",
        "business inquiries",
        "subscribe",
        "shop",
    ]
    for b in boiler:
        text = re.sub(b, " ", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text).strip()
    if len(text) < 10:
        return None
    if len(text) > max_len:
        text = text[:max_len]
    return text


def extract_social_candidates(base: Dict, apple: Dict, spotify: Dict, youtube: Dict) -> Dict[str, List[str]]:
    texts = [
        base.get("pi_description") or "",
        apple.get("description") or "",
        spotify.get("show", {}).get("description") or "",
        (youtube.get("channel", {}) if youtube else {}).get("about") or "",
    ]
    urls = []
    for t in texts:
        urls.extend(re.findall(r"https?://\S+", t))
    # Base links
    if base.get("feed_url"):
        urls.append(base["feed_url"])
    if base.get("web_link"):
        urls.append(base["web_link"])

    candidates = {"twitter": [], "instagram": [], "website": [], "patreon": []}
    for u in urls:
        lu = u.lower()
        if "twitter.com" in lu or "x.com" in lu:
            candidates["twitter"].append(u)
        elif "instagram.com" in lu:
            candidates["instagram"].append(u)
        elif "patreon.com" in lu:
            candidates["patreon"].append(u)
        else:
            candidates["website"].append(u)
    return candidates


def fetch_twitter_bio(url: str) -> Optional[Dict]:
    try:
        res = requests.get(url, timeout=10)
        if res.status_code != 200:
            return None
        soup = BeautifulSoup(res.text, "html.parser")
        meta = soup.find("meta", attrs={"name": "description"})
        if meta and meta.get("content"):
            bio = clean_bio_text(meta["content"])
            if bio:
                handle = url.split("/")[-1]
                return {"handle": f"@{handle}", "bio": bio}
    except Exception:
        return None
    return None


def fetch_instagram_bio(url: str) -> Optional[Dict]:
    try:
        res = requests.get(url, timeout=10)
        if res.status_code != 200:
            return None
        soup = BeautifulSoup(res.text, "html.parser")
        meta = soup.find("meta", property="og:description")
        if meta and meta.get("content"):
            desc = meta["content"]
            # og:description has follower counts; take before "Followers" if present
            desc = desc.split("Followers")[0]
            bio = clean_bio_text(desc)
            if bio:
                handle = url.rstrip("/").split("/")[-1]
                return {"handle": f"@{handle}", "bio": bio}
    except Exception:
        return None
    return None


def fetch_website_about(url: str) -> Optional[Dict]:
    try:
        targets = [url]
        if url.endswith("/"):
            base = url.rstrip("/")
        else:
            base = url
        targets.append(base + "/about")
        targets.append(base + "/about-us")
        for t in targets:
            try:
                res = requests.get(t, timeout=10)
                if res.status_code != 200:
                    continue
                soup = BeautifulSoup(res.text, "html.parser")
                paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
                text = " ".join(paragraphs)
                bio = clean_bio_text(text, max_len=500)
                if bio:
                    return {"url": t, "about": bio}
            except Exception:
                continue
    except Exception:
        return None
    return None


def fetch_social_bios_from_sources(base: Dict, apple: Dict, spotify: Dict, youtube: Dict) -> Dict:
    cands = extract_social_candidates(base, apple, spotify, youtube)
    result = {}
    if cands["twitter"]:
        tw = fetch_twitter_bio(cands["twitter"][0])
        if tw:
            result["twitter"] = tw
    if cands["instagram"]:
        ig = fetch_instagram_bio(cands["instagram"][0])
        if ig:
            result["instagram"] = ig
    if cands["patreon"]:
        pt = fetch_website_about(cands["patreon"][0])
        if pt:
            result["patreon"] = pt
    if cands["website"]:
        ws = fetch_website_about(cands["website"][0])
        if ws:
            result["website"] = ws
    return result


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Adelined data CLI")
    subparsers = parser.add_subparsers(dest="action", required=True)

    fetch_parser = subparsers.add_parser("fetch", help="Fetch creator data from a source")
    fetch_parser.add_argument(
        "source",
        choices=["youtube", "spotify"],
        help="Data source to pull from",
    )
    fetch_parser.add_argument(
        "--output",
        type=Path,
        help="Optional JSON output file to write results",
    )

    # YouTube options
    fetch_parser.add_argument(
        "--channels",
        help="Comma-separated YouTube channel handles/IDs/URLs to fetch (for source youtube)",
    )
    fetch_parser.add_argument(
        "--channels-file",
        type=Path,
        help="Path to newline-delimited list of YouTube channel handles/IDs/URLs",
    )

    # Spotify options
    fetch_parser.add_argument(
        "--shows",
        help="Comma-separated Spotify show IDs or URLs to fetch (for source spotify)",
    )
    fetch_parser.add_argument(
        "--shows-file",
        type=Path,
        help="Path to newline-delimited list of Spotify show IDs or URLs",
    )

    ingest_parser = subparsers.add_parser("ingest", help="Ingest data into the Adelined DB")
    ingest_parser.add_argument(
        "source",
        choices=["podcastindex", "apple"],
        help="Dataset to ingest",
    )
    ingest_parser.add_argument(
        "--source-db",
        type=Path,
        default=Path("data/podcastindex_feeds.db"),
        help="Path to source SQLite database (default: data/podcastindex_feeds.db)",
    )
    ingest_parser.add_argument(
        "--target-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to target Adelined SQLite database (default: adelined_matching.db)",
    )
    ingest_parser.add_argument(
        "--batch-size",
        type=int,
        default=5000,
        help="Batch size for streaming copy (default: 5000)",
    )

    enrich_parser = subparsers.add_parser("enrich", help="Enrich creators with external signals")
    enrich_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    enrich_parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="Max creators to process in this run (default: 100)",
    )
    enrich_parser.add_argument(
        "--offset",
        type=int,
        default=0,
        help="Offset into creators table (default: 0)",
    )
    enrich_parser.add_argument(
        "--only-id",
        type=int,
        help="Optional specific podcastindex_id to process",
    )
    enrich_parser.add_argument(
        "--sources",
        help="Comma-separated sources to fetch (default: all). Options: spotify,apple,youtube,social",
    )

    discover_parser = subparsers.add_parser("discover", help="Discover new creators from external sources")
    discover_parser.add_argument(
        "source",
        choices=["apple"],
        help="Discovery source",
    )
    discover_parser.add_argument(
        "--terms",
        help="Comma-separated search terms",
    )
    discover_parser.add_argument(
        "--terms-file",
        type=Path,
        help="File with search terms (one per line)",
    )
    discover_parser.add_argument(
        "--limit-per-term",
        type=int,
        default=200,
        help="Results per page (max 200)",
    )
    discover_parser.add_argument(
        "--max-pages",
        type=int,
        default=5,
        help="Max pages per term",
    )
    discover_parser.add_argument(
        "--country",
        default="GB",
        help="iTunes country code (default: GB)",
    )
    discover_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    report_parser = subparsers.add_parser("enrichment", help="Manage or inspect enrichment data")
    report_sub = report_parser.add_subparsers(dest="enrichment_action", required=True)

    clear_parser = report_sub.add_parser("clear", help="Clear creator_enrichment table (requires confirmation)")
    clear_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    show_parser = report_sub.add_parser("show", help="Show enrichment summaries or source data")
    show_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )
    show_parser.add_argument(
        "--source",
        choices=["spotify", "apple", "youtube", "social"],
        help="Specific source to list; omit for summary counts",
    )
    show_parser.add_argument(
        "--limit",
        type=int,
        default=5,
        help="Max rows to display (default: 5)",
    )

    summary_parser = report_sub.add_parser("summary", help="Report overall enrichment coverage")
    summary_parser.add_argument(
        "--adl-db",
        type=Path,
        default=Path("adelined_matching.db"),
        help="Path to Adelined SQLite database (default: adelined_matching.db)",
    )

    return parser.parse_args(argv)


def handle_fetch_youtube(args: argparse.Namespace) -> int:
    channel_inputs = load_list_inputs(args.channels, args.channels_file)
    if not channel_inputs:
        print("‚ùå Please provide YouTube channels via --channels or --channels-file", file=sys.stderr)
        return 1

    fetcher = YouTubeChannelFetcher(channels=channel_inputs, delay=args.delay)
    print(f"Fetching YouTube channels: {channel_inputs}")
    records = fetcher.fetch()
    print(f"‚úÖ Retrieved {len(records)} YouTube channels.")

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        payload = [record.__dict__ for record in records]
        args.output.write_text(json.dumps(payload, indent=2))
        print(f"üíæ Wrote JSON: {args.output}")

    preview = records[:5]
    if preview:
        print("Sample rows:")
        for rec in preview:
            print(f" ‚Ä¢ {rec.title} ‚Äî {rec.channel_url}")
    return 0


def handle_fetch_spotify(args: argparse.Namespace) -> int:
    show_inputs = load_list_inputs(args.shows, args.shows_file)
    if not show_inputs:
        print("‚ùå Please provide Spotify shows via --shows or --shows-file", file=sys.stderr)
        return 1

    fetcher = SpotifyShowFetcher(shows=show_inputs, delay=args.delay)
    print(f"Fetching Spotify shows: {show_inputs}")
    records = fetcher.fetch()
    print(f"‚úÖ Retrieved {len(records)} Spotify shows.")

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        payload = [record.__dict__ for record in records]
        args.output.write_text(json.dumps(payload, indent=2))
        print(f"üíæ Wrote JSON: {args.output}")

    preview = records[:5]
    if preview:
        print("Sample rows:")
        for rec in preview:
            print(f" ‚Ä¢ {rec.title} ‚Äî {rec.show_url}")
    return 0


def main(argv: Optional[Sequence[str]] = None) -> int:
    if argv is None:
        argv = sys.argv[1:]

    if len(argv) == 1 and argv[0].lower() == "help":
        print_help()
        return 0

    args = parse_args(argv)
    if args.action == "fetch":
        if args.source == "youtube":
            return handle_fetch_youtube(args)
        if args.source == "spotify":
            return handle_fetch_spotify(args)
    if args.action == "ingest":
        if args.source == "podcastindex":
            return ingest_podcastindex_feeds(args)
        if args.source == "apple":
            return ingest_apple_discovery(args)
    if args.action == "enrich":
        return enrich_creators(args)
    if args.action == "enrichment":
        if args.enrichment_action == "clear":
            return clear_enrichment(args)
        if args.enrichment_action == "show":
            return show_enrichment(args)
        if args.enrichment_action == "summary":
            return summary_enrichment(args)
    if args.action == "discover":
        if args.source == "apple":
            return discover_apple(args)
    print(f"Unknown action/source combination: {args.action} {args.source}")
    return 1


def print_help() -> None:
    help_text = """
Adelined CLI

Commands:
  python3 adl fetch youtube [options]
  python3 adl fetch spotify [options]
  python3 adl ingest podcastindex [options]
  python3 adl discover apple [options]
  python3 adl enrich [options]
  python3 adl enrichment clear [options]
  python3 adl enrichment show [options]
  python3 adl enrichment summary [options]

Examples:
  # YouTube: fetch channel metadata by handles
  python3 adl fetch youtube --channels @lexfridman,@hubermanlab

  # YouTube: from file (one handle/URL/channel_id per line)
  python3 adl fetch youtube --channels-file channels.txt --output cache/youtube.json

  # Spotify: fetch show metadata by show IDs
  python3 adl fetch spotify --shows 5AvwZVawapvyhJUIx71pdJ,4rOoJ6Egrf8K2IrywzwOMk

  # Spotify: from URLs file
  python3 adl fetch spotify --shows-file spotify_urls.txt --output cache/spotify.json

  # Ingest PodcastIndex snapshot into Adelined DB (itunesId dedupe)
  python3 adl ingest podcastindex --source-db data/podcastindex_feeds.db --target-db adelined_matching.db

  # Discover Apple shows via iTunes Search API (GB)
  python3 adl discover apple --terms "comedy,news" --limit-per-term 200 --max-pages 5 --country GB --adl-db adelined_matching.db

  # Enrich a slice of creators
  python3 adl enrich --adl-db adelined_matching.db --limit 50 --offset 0
  python3 adl enrich --adl-db adelined_matching.db --only-id 12345
Options:
  --output    Optional JSON file to write scraped records
  --source-db Source SQLite path for ingest (default: data/podcastindex_feeds.db)
  --target-db Target Adelined SQLite path (default: adelined_matching.db)
  --batch-size Batch size for ingest (default: 5000)
  --limit     Max creators to enrich per run (default: 100)
  --offset    Offset into creators table (default: 0)
  --only-id   Enrich only this podcastindex_id
  --limit-per-term Apple discovery results per page (default: 200)
  --max-pages     Apple discovery max pages per term (default: 5)
"""
    print(help_text.strip())


if __name__ == "__main__":
    sys.exit(main())
